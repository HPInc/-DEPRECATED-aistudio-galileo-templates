{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "Most of the libraries that are necessary for the development of this example are built-in on the GenAI workspace, available in AI Studio. More specific libraries to handle the type of input will be added here. In this case, we are giving support to transcripts in the webvtt format, used to store transcripts, which require the webvtt-py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db657805",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# Summarization of transcripts with Langchain\n",
    "\n",
    "In this example, we intend to create a summarizer for long transcripts. The main goal is to break the original transcript into different chunks based on context - i.e. using an unsupervised approach to identify the different topics throughout the transcript (somehow similarly to Topic Modelling) - and summarize each of these chunks. in the end, the different summaries are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff65e3-dbdf-457a-8746-66c453182f26",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "from src.utils import configure_hf_cache\n",
    "\n",
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ad0fd",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a24f4-01f4-4a33-8124-7d7601ced6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_config_and_secrets\n",
    "\n",
    "config_path = \"../../configs/config.yaml\"\n",
    "secrets_path = \"../../configs/secrets.yaml\"\n",
    "\n",
    "config, secrets = load_config_and_secrets(config_path, secrets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95de597-657b-496e-a776-8d6e0f6be190",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab31867-255e-489c-810d-42786bde5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import configure_proxy\n",
    "\n",
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from the transcript\n",
    "\n",
    "At first, we need to read the data from the transcript. As our transcript is in the .vtt format, we use a library called webvtt-py to read the content. As the text is a trancript of audio/video, it is organized in small chunks of conversation, each containing a sequential id, the time of the start and end of the chunk, and the text content (often in the form speaker:content).\n",
    "\n",
    "From this data, we expect to extract the actual content,  while keeping reference to the other metadata - for this reason, we are loading all the data into a Pandas dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16a213-9f92-4c75-93ff-66adc3133cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"../data\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"'data' folder not found in path: {os.path.abspath(data_path)}\")\n",
    "\n",
    "file_path = os.path.join(data_path, \"I_have_a_dream.vtt\")\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for caption in webvtt.read(file_path):\n",
    "    line = caption.text.split(\":\")\n",
    "    while len(line) < 2:\n",
    "        line = [''] + line\n",
    "    data[\"id\"].append(caption.identifier)\n",
    "    data[\"speaker\"].append(line[0].strip())\n",
    "    data[\"content\"].append(line[1].strip())\n",
    "    data[\"start\"].append(caption.start)\n",
    "    data[\"end\"].append(caption.end)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6763-3eb0-41fb-900f-67778c3d5caf",
   "metadata": {},
   "source": [
    "As a second option, we provide here a code to load the same structure from a plain text document, which only contains the actual content of the speech/conversation, without extra metadata. For the sake of simplicity and reuse of code, we keep the same Data Frame structure as the previous version, by filling the remaining fields with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33cbb-1c2b-404e-ad65-b243c6702308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(file_path) as file:\n",
    "    lines = file.read()\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for line in lines.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        data[\"id\"].append(\"\")\n",
    "        data[\"speaker\"].append(\"\")\n",
    "        data[\"content\"].append(line.strip())\n",
    "        data[\"start\"].append(\"\")\n",
    "        data[\"end\"].append(\"\")        \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e122c-5a54-4285-8788-be12fc86e278",
   "metadata": {},
   "source": [
    "## Step 2: Semantic chunking of the transcript\n",
    "Having the information content loaded according to the transcription format - with the text split into audio blocks, or into paragraphs, we now want to group these small blocks into relevant topics - so we can summarize each topic individually. Here, we are using a very simple approach for that, by using a semantic embedding of each sentence (using an embedding model from Hugging Face Sentence Transformers), and identifying the \"breaks\" among chunks as the ones with higher semantic distance. Notice that this method can be parameterized, to inform the number of topics or the best method to identify the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c67feb-11f7-47ad-bdec-3ec252e51797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5538aee-0233-4b58-a574-879dfa64a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter():\n",
    "    def __init__ (self, content, embedding_model, method=\"number\", partition_count = 10, quantile = 0.9):\n",
    "        self.content = content\n",
    "        self.embedding_model = embedding_model\n",
    "        self.partition_count = partition_count\n",
    "        self.quantile = quantile\n",
    "        self.embeddings = embedding_model.encode(content)\n",
    "        self.distances = [cosine(embeddings[i - 1], embeddings[i]) for i in range(1, len(embeddings))]\n",
    "        self.breaks = []\n",
    "        self.centroids = []\n",
    "        self.load_breaks(method=method)\n",
    "\n",
    "    def centroid_distance(self, embedding_id, centroid_id):\n",
    "        return cosine(self.embeddings[embedding], self.centroid[centroid])\n",
    "\n",
    "    def adjust_neighbors(self):\n",
    "        self.breaks = []\n",
    "\n",
    "    def load_breaks(self, method = 'number'):\n",
    "        if method == 'number':\n",
    "            if self.partition_count > len(self.distances):\n",
    "                self.partition_count = len(self.distances)\n",
    "            self.breaks = np.sort(np.argpartition(self.distances, self.partition_count - 1)[0:self.partition_count - 1])\n",
    "        elif method == 'quantiles':\n",
    "            threshold = np.quantile(self.distances, self.quantile)\n",
    "            self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "        else:\n",
    "            self.breaks = []\n",
    "\n",
    "    def get_centroid(self, beginning, end):\n",
    "        return embedding_model.encode('\\n'.join(self.content[beginning : end]))\n",
    "    \n",
    "    def load_centroids(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            self.centroids = [self.get_centroid(0, len(self.content))]\n",
    "        else:\n",
    "            self.centroids = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                self.centroids += [self.get_centroid(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            self.centroids += [self.get_centroid(beginning, len(self.content))]\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        return '\\n'.join(self.content[beginning : end])\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            return [self.get_chunk(0, len(self.content))]\n",
    "        else:\n",
    "            chunks = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                chunks += [self.get_chunk(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            chunks += [self.get_chunk(beginning, len(self.content))]\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f13a0c-9704-462c-ba20-6b267367648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_separator = \"\\n *-* \\n\"\n",
    "\n",
    "splitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\n",
    "chunks = chunk_separator.join(splitter.get_chunks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 3: Using a LLM model to Summarize each chunk\n",
    "In our example, we are going to summarize each individual chunk separately. This solution might be advantageous in different situations:\n",
    " * When the original text is too big , or the loaded model works with a context that is too small. In this scenario, breaking information into chunks are necessary to allow the model to be applied\n",
    " * When the user wants to make sure that all the separate topics of a conversation are covered into the summarized version. An extra step could be added to allow some verification or manual configuration of the chunks to allow the user to customize the output\n",
    "\n",
    "To achieve this goal, we load a LLM model and use a summarization prompt. For the model, we provide two different options for loading the model:\n",
    " * **local**: by loading the llama2-7b model from the asset downloaded on the project\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    " \n",
    "This choice can be set in the variable model_source below or as an entry in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file. The option hugging-face-local, while available in other models, is not available for this example yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fba44-6a64-40a1-88e6-d5cf1f5cc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import initialize_llm\n",
    "\n",
    "model_source = \"local\"\n",
    "if \"model_source\" in config:\n",
    "    model_source = config[\"model_source\"]\n",
    "\n",
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdb8f-a70a-499a-a2d6-56c14d965169",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "The following text is an excerpt of a transcription:\n",
    "\n",
    "### \n",
    "{context} \n",
    "###\n",
    "\n",
    "Please, produce a single paragraph summarizing the given excerpt.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a586d-fbf5-4551-b022-d50da386e74c",
   "metadata": {},
   "source": [
    "## Step 4: Create parallel chain to summarize the transcript\n",
    "\n",
    "In the following cell, we create a chain that will receive a single string with multiple chunks (separated by the declared separator), than:\n",
    "  * Break the input into separated chains - using the break_chunks function embedded in a RunnableLambda to be used in LangChain\n",
    "  * Run a Parallel Chain with the following elements for each chunk:\n",
    "    * Get an individual element\n",
    "    * Personalize the prompt template to create an individual prompt for each chunk\n",
    "    * Use the LLM inference to summarize the chunk\n",
    "  * Merge the individual summaries into a single one\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5cde3-b064-4280-8ada-8df68820a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Converts prompt_template to LangChain object\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def break_chunks(text):\n",
    "    \"\"\"\n",
    "    Split text into chunks using the predefined separator.\n",
    "    \"\"\"\n",
    "    return text.split(chunk_separator)\n",
    "\n",
    "def process_chunk(chunk_text):\n",
    "    # Create a proper runnable chain for each chunk\n",
    "    chunk_chain = (\n",
    "        RunnablePassthrough.assign(context=lambda _: chunk_text)\n",
    "        | prompt \n",
    "        | llm\n",
    "    )\n",
    "    return chunk_chain.invoke({})\n",
    "\n",
    "def process_chunks(text):\n",
    "    chunks_list = break_chunks(text)\n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks_list):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks_list)}...\")\n",
    "        try:\n",
    "            result = process_chunk(chunk)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {e}\")\n",
    "            results.append(f\"Error: {str(e)}\")\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "lambda_break = RunnableLambda(break_chunks)\n",
    "\n",
    "def join_summaries(summaries_dict):\n",
    "    # Extract values from the dictionary and join them\n",
    "    return \"\\n\\n\".join([str(v) for v in summaries_dict.values()])\n",
    "\n",
    "lambda_join = RunnableLambda(join_summaries)\n",
    "\n",
    "chain = RunnableLambda(process_chunks) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 5: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptquality as pq\n",
    "from src.utils import setup_galileo_environment\n",
    "\n",
    "#########################################\n",
    "# In order to connect to Galileo, create a secrets.yaml file in the same folder as this notebook\n",
    "# This file should be an entry called Galileo, with the your personal Galileo API Key\n",
    "# Galileo API keys can be created on https://console.hp.galileocloud.io/settings/api-keys\n",
    "#########################################\n",
    "\n",
    "setup_galileo_environment(secrets)\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d47fe-41b9-4f19-9b11-4ef2d3f2740f",
   "metadata": {},
   "source": [
    "## Step 6: Run the chain and connect the metrics to Galileo\n",
    "\n",
    "In this session, we call the created chain and create the mechanisms to ingest the quality metrics into Galileo. For this example, we create a personalized metric (scorer), that will be running locally to measure the quality of the summarization. For this reason, we use HuggingFace implementation of ROUGE (using evaluate library), and implement into a CustomScorer from Galileo (next cell).\n",
    "\n",
    "Below, we illustrate two alternative ways to connect to Galileo:\n",
    "  * Using a customized run, which calculates the scores and logs into Galileo\n",
    "  * Using the langchain callback (currently unavailable due to compatibility issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bb40e-7823-490a-af94-0d8aae5e5886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import time\n",
    "import json\n",
    "import promptquality as pq\n",
    "\n",
    "def rouge_executor(row) -> float:\n",
    "    try:\n",
    "        print(f\"node_input: {row.node_input}\")\n",
    "        print(f\"node_output: {row.node_output}\")\n",
    "\n",
    "        # Try to decode node_input as JSON\n",
    "        try:\n",
    "            node_input = json.loads(row.node_input)\n",
    "            reference = node_input.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_input: {row.node_input}\")\n",
    "            return 0.0\n",
    "\n",
    "        # Try to decode node_output as JSON\n",
    "        try:\n",
    "            node_output = json.loads(row.node_output)\n",
    "            prediction = node_output.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_output: {row.node_output}\")\n",
    "            return 0.0\n",
    "\n",
    "        if not reference or not prediction:\n",
    "            print(\"'content' fields are empty in node_input or node_output\")\n",
    "            return 0.0\n",
    "\n",
    "        # Calculates ROUGE-L\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_values = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "        return rouge_values.get(\"rougeL\", 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in rouge_executor: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def rouge_aggregator(scores, indices) -> dict:\n",
    "    if len(scores) == 0:\n",
    "        return {'Average RougeL': 0.0}\n",
    "    else:\n",
    "        return {'Average RougeL': sum(scores) / len(scores)}\n",
    "\n",
    "# Define CustomScorer with corrected functions\n",
    "rouge_scorer = pq.CustomScorer(name='RougeL', executor=rouge_executor, aggregator=rouge_aggregator)\n",
    "\n",
    "# Configures the assessment execution\n",
    "partitioned_run = pq.EvaluateRun(\n",
    "    project_name=\"AIStudio_template_code_summarization\",\n",
    "    run_name=\"Test4 partitioned script\",\n",
    "    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    ")\n",
    "\n",
    "# Measures execution time\n",
    "start_time = time.time()\n",
    "response = chain.invoke(chunks)\n",
    "total_time = int((time.time() - start_time) * 1000000)\n",
    "\n",
    "partitioned_run.add_workflow(input=chunks, output=response, duration_ns=total_time) \n",
    "partitioned_run.add_llm_step(input=chunks, output=response, duration_ns=total_time, model='local')\n",
    "\n",
    "# Finalizes the execution of the assessment\n",
    "partitioned_run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5bff8-fb62-433e-b1ff-985e610fe4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import initialize_galileo_evaluator\n",
    "\n",
    "# Create a custom ROUGE scorer for summarization evaluation\n",
    "def rouge_executor(row) -> float:\n",
    "    try:\n",
    "        print(f\"node_input: {row.node_input}\")\n",
    "        print(f\"node_output: {row.node_output}\")\n",
    "\n",
    "        # Try to decode node_input as JSON\n",
    "        try:\n",
    "            node_input = json.loads(row.node_input)\n",
    "            reference = node_input.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_input: {row.node_input}\")\n",
    "            return 0.0\n",
    "\n",
    "        # Try to decode node_output as JSON\n",
    "        try:\n",
    "            node_output = json.loads(row.node_output)\n",
    "            prediction = node_output.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_output: {row.node_output}\")\n",
    "            return 0.0\n",
    "\n",
    "        if not reference or not prediction:\n",
    "            print(\"'content' fields are empty in node_input or node_output\")\n",
    "            return 0.0\n",
    "\n",
    "        # Calculates ROUGE-L\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_values = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "        return rouge_values.get(\"rougeL\", 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in rouge_executor: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def rouge_aggregator(scores, indices) -> dict:\n",
    "    if len(scores) == 0:\n",
    "        return {'Average RougeL': 0.0}\n",
    "    else:\n",
    "        return {'Average RougeL': sum(scores) / len(scores)}\n",
    "\n",
    "# Define CustomScorer with corrected functions\n",
    "rouge_scorer = pq.CustomScorer(name='RougeL', executor=rouge_executor, aggregator=rouge_aggregator)\n",
    "\n",
    "# Create and configure the Galileo evaluator with our custom scorer\n",
    "summarization_callback = initialize_galileo_evaluator(\n",
    "    project_name=\"AIStudio_template_code_summarization\",\n",
    "    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    ")\n",
    "\n",
    "### THIS CODE IS NOT WORKING YET, AS GALILEO DOES NOT SUPPORT LISTS AS THE OUTPUT OF CHAIN NODES \n",
    "\n",
    "#summaries = chain.invoke(chunks, config={\"callbacks\": [summarization_callback]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e043a-4d8a-47ba-83b2-d99c0d803575",
   "metadata": {},
   "source": [
    "## Galileo Protect\n",
    "\n",
    "Galileo Protect serves as a powerful tool for safeguarding AI model outputs by detecting and preventing the release of sensitive information like personal addresses or other PII. By integrating Galileo Protect into your AI pipelines, you can ensure that model responses comply with privacy and security guidelines in real-time.\n",
    "\n",
    "Galileo functions as an API that provides support for protection verification of your chain/LLM. To log into the Galileo console, it is necessary to integrate it with another service, such as Galileo Evaluate or Galileo Observe.\n",
    "\n",
    "**Attention**: an integrated API within the Galileo console is required to perform this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a39a4c-6a04-4510-9c10-4b281566d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import galileo_protect as gp\n",
    "from src.utils import initialize_galileo_protect\n",
    "\n",
    "# Create a project and stage for Galileo Protect\n",
    "project, project_id, stage_id = initialize_galileo_protect('validate_protect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23d0eb-806a-4a47-b051-f45ba2466801",
   "metadata": {},
   "source": [
    "Galileo Protect works by creating rules that identify conditions such as Personally Identifiable Information (PII) and toxicity. It ensures that the prompt will not receive or respond to sensitive questions. In this example, we create a set of rules (ruleset) and a set of actions that return a pre-programmed response if a rule is triggered. Galileo Protect also offers a variety of other metrics to suit different protection needs. You can learn more about the available metrics here: [Supported Metrics and Operators](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/supported-metrics-and-operators).\n",
    "\n",
    "Additionally, it is possible to import rulesets directly from Galileo through stages. Learn more about this feature here: [Invoking Rulesets](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/invoking-rulesets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23894954-93b0-4c06-9a2c-e3323fcaa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "\n",
    "# Define a ruleset for PII detection (specifically SSN)\n",
    "pii_ruleset = Ruleset(\n",
    "    # Define the rules to check for potential issues\n",
    "    rules=[\n",
    "        {\n",
    "            \"metric\": \"pii\",  # Using Personal Identifiable Information metric\n",
    "            \"operator\": \"contains\",  # Check if PII contains specific type\n",
    "            \"target_value\": \"ssn\",  # Looking for Social Security Numbers\n",
    "        },\n",
    "    ],\n",
    "    # Define the action to take when rules are triggered\n",
    "    action={\n",
    "        \"type\": \"OVERRIDE\",  # Override the model response\n",
    "        \"choices\": [\n",
    "            \"Personal Identifiable Information detected. Sorry, I cannot provide the response.\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize ProtectTool with the configured stage_id and ruleset\n",
    "protect_tool = ProtectTool(stage_id=stage_id, prioritized_rulesets=[pii_ruleset], timeout=10)\n",
    "\n",
    "# Use existing chain and combine with ProtectTool\n",
    "protect_parser = ProtectParser(chain=chain)  # 'chain' has already been defined previously\n",
    "protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "# Example of using the protected chain with input and output\n",
    "input_data = {\n",
    "    \"input\": \"John Doe's social security number is 123-45-6789.\",\n",
    "    \"output\": \"John Doe's social security number is 123-45-6789.\"\n",
    "}\n",
    "\n",
    "# Invoke the protected chain\n",
    "print(\"Invoking the chain with PII protection...\")\n",
    "response = protected_chain.invoke(input_data)\n",
    "print(\"Protected chain response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563e41c-c1a9-423e-a11f-5e7c6ac53770",
   "metadata": {},
   "source": [
    "## Galileo Observe\n",
    "\n",
    "Galileo Observe helps you monitor your generative AI applications in production. With Observe you will understand how your users are using your application and identify where things are going wrong. Keep tabs on your production system, instantly receive alerts when bad things happen, and perform deep root cause analysis though the Observe dashboard.\n",
    "\n",
    "You can connect Galileo Observe to your Langchain chain to monitor metrics such as cost and guardrail indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cbf0e-6c9d-4120-b9e3-17b376b3b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from src.utils import initialize_galileo_observer\n",
    "\n",
    "example_query = \"\"\"Tell me a story about technology and innovation. \n",
    "Explain how artificial intelligence is shaping the future. \n",
    "Summarize the impact of renewable energy on society.\"\"\"\n",
    "\n",
    "result_break = lambda_break.invoke(example_query)\n",
    "\n",
    "chain = lambda_break | {\n",
    "    f\"summary_{i}\": itemgetter(i) | prompt | llm\n",
    "    for i in range(len(result_break))\n",
    "} | lambda_join | StrOutputParser()\n",
    "\n",
    "monitor_handler = initialize_galileo_observer(\"validate_galileo_observe\")\n",
    "\n",
    "print(\"Invoking the chain with Galileo Observe...\")\n",
    "try:\n",
    "    output = chain.invoke(\n",
    "        example_query,\n",
    "        config={\"callbacks\": [monitor_handler]}\n",
    "    )\n",
    "    print(\"Observed chain output:\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error during chain execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910bddc-a194-4011-b82d-3a8cd6ea762d",
   "metadata": {},
   "source": [
    "## Model service Galileo Protect + Observe\n",
    "\n",
    "In this example, we illustrate a different approach to create a text summarizer. Instead of splitting the text into topics and summarize the topics individually, this model service provides a REST API endpoint to allow summarization of an entire text, in a single call to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1ce88",
   "metadata": {},
   "source": [
    "## Text Summarization Service\n",
    "\n",
    "This section demonstrates how to use our TextSummarizationService from the src/service directory. This approach improves code organization by separating the service implementation from the notebook, making it easier to maintain and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "# Import the TextSummarizationService class\n",
    "from src.service import TextSummarizationService\n",
    "\n",
    "# Set up the MLflow experiment\n",
    "mlflow.set_experiment(\"Summarization_Service\")\n",
    "\n",
    "# Define paths\n",
    "model_path = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Warning: Model file not found at {model_path}. You may need to update the path.\")\n",
    "\n",
    "# Define the input and output schema\n",
    "input_schema = Schema([ColSpec(\"string\", \"text\")])\n",
    "output_schema = Schema([ColSpec(\"string\", \"summary\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Log and register the model using MLflow\n",
    "with mlflow.start_run(run_name=\"Text_Summarization_Service\") as run:\n",
    "    # Log the model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"text_summarization_service\",\n",
    "        python_model=TextSummarizationService(),\n",
    "        artifacts={\"secrets\": secrets_path, \"config\": config_path, \"model\": model_path},\n",
    "        signature=signature,\n",
    "        pip_requirements=[\n",
    "            \"galileo-protect==0.15.1\",\n",
    "            \"galileo-observe==1.13.2\",\n",
    "            \"pyyaml\",\n",
    "            \"pandas\",\n",
    "            \"sentence-transformers\",\n",
    "            \"langchain_core\",\n",
    "            \"langchain_huggingface\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Register the model in MLflow\n",
    "    model_uri = f\"runs:/{run.info.run_id}/text_summarization_service\"\n",
    "    mlflow.register_model(model_uri=model_uri, name=\"Text_Summarization_Service\")\n",
    "    print(f\"Model registered successfully with run ID: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
