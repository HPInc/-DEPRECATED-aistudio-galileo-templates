{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa0e7e3-1428-47aa-b02d-9e058386faf6",
   "metadata": {},
   "source": [
    "# Scientific Presentation Script Generator\n",
    "\n",
    "This notebook uses the RAG (Retrieval-Augmented Generation) architecture to generate a scientific presentation script. The process involves searching for a paper related to a specific topic using the arXiv API. After the search, the necessary pipelines are applied to format and generate a cohesive and informative presentation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39edbbff-1aae-43a9-82e8-950bdc1fd592",
   "metadata": {},
   "source": [
    "## RAG with Galileo, Langchain and GPT\n",
    "\n",
    "- What is RAG?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6d677-8e73-411d-828e-a63f70edb461",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "This step install the necessary libraries for connecting with Galileo and the models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6521a055-e62f-45bd-ba11-ff3104cdbafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  feedparser newspaper3k listparser\n",
    "!pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c24c88-59aa-4d2b-a7fa-674e9130e2c7",
   "metadata": {},
   "source": [
    "## Chapter 1: Building the Paper Research and Processing Pipeline\n",
    "\n",
    "<img src=\"img/text-generation1.png\" alt=\"image.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we will build a complete pipeline for researching and processing scientific papers. The process will be detailed in several stages:\n",
    "\n",
    "1. **Paper Research**: We will start by selecting a specific topic to search for relevant papers. The research will be conducted using specialized APIs, such as the arXiv API.\n",
    "2. **Paper Download**: After identifying the paper of interest, we will download the file to store it locally.\n",
    "3. **Text Extraction**: Once the paper is saved, we will use the PyMuPDF library to extract the text from the PDF document.\n",
    "4. **Chunk Creation**: The extracted text will be segmented into small blocks, called \"chunks,\" using the LangChain framework.\n",
    "5. **Embedding Generation**: Each chunk will be converted into a vector representation using Hugging Face's embedding models. This representation allows for semantic analysis of the content.\n",
    "6. **Storage in the Vector Database**: Finally, the generated embeddings will be stored in a vector database, which in our case will be ChromaDB. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034bdfe-71a7-47fa-85d0-dae9ad26f53f",
   "metadata": {},
   "source": [
    "## Step 1: Search Papers\n",
    "\n",
    "In this step, the pipeline performs a search for relevant scientific papers using the arXiv API.\n",
    "\n",
    "1. **Query Definition**: The user provides a term or topic of interest (query), which is used to search for papers on arXiv. The search can be refined by specifying the maximum number of results returned        (max_results), with the default value being 1.\n",
    "\n",
    "2. **Request Submission**: The search_arxiv_and_extract_text function sends an HTTP request to the arXiv API with the specified query. The API returns information about the papers that match the search term.\n",
    "\n",
    "3. **Response Processing**: If the request is successful, the response content (in XML format) is processed to extract information about each paper found. Specifically, the function retrieves the paper's        title and the corresponding PDF link.\n",
    "\n",
    "4. **PDF Download**: Using the PDF link, the pipeline downloads the document locally using the download_pdf function, which saves the file in a temporary directory.\n",
    "\n",
    "5. **Text Extraction**: After the download, the text from the PDF is extracted using the PyMuPDF library. The extracted text is then consolidated and presented in a string format, which can be used in later stages for embedding generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "a678462e-7d6b-4c77-9b69-b6029239522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "3ec03004-8040-4424-851d-b130a29a3f62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def search_arxiv_and_extract_text(query, max_results=1):\n",
    "    \"\"\"\n",
    "    Searches arXiv for articles based on a query and extracts the text from the associated PDF.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search term to use in the arXiv search.\n",
    "    - max_results (int): The maximum number of results to return from the search. Default is 1.\n",
    "    \"\"\"\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "            pdf_url = entry.find('{http://www.w3.org/2005/Atom}link[@title=\"pdf\"]').attrib['href']\n",
    "            \n",
    "            pdf_path = f\"temp_{title[:50]}.pdf\"  \n",
    "            pdf_downloaded = download_pdf(pdf_url, pdf_path)\n",
    "            \n",
    "            if pdf_downloaded:\n",
    "                loader = PyMuPDFLoader(pdf_path)\n",
    "                docs = loader.load()\n",
    "                text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "                print(f\"Text extracted from the article '{title.strip()}':\\n{text[:500]}...\")  \n",
    "            else:\n",
    "                print(f\"Error downloading article PDF '{title.strip()}'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error accessing arXiv.\")\n",
    "\n",
    "def download_pdf(pdf_url, output_path):\n",
    "    \"\"\"\n",
    "     Downloads a PDF file from a URL and saves it locally.\n",
    "     Parameters:\n",
    "    - pdf_url (str): The URL of the PDF to be downloaded.\n",
    "    - output_path (str): The path where the PDF will be saved locally.\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error downloading PDF: {response.status_code}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "850df95e-efc5-47ed-bf0f-1c4f6ceca2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from the article 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n",
      "  Frameworks':\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy...\n"
     ]
    }
   ],
   "source": [
    "query = \"RAG\" # change as you prefer\n",
    "search_arxiv_and_extract_text(query, max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "eed0a0a5-0999-42d8-8afb-472dea4d7214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Lecture Notes: Optimization for Machine Learning',\n",
       "  'summary': 'Lecture notes on optimization for machine learning, derived from a course at\\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\\nSimons Foundation, Berkeley.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1909.03550v1',\n",
       "  'authors': ['Elad Hazan'],\n",
       "  'text': 'lecture notes:\\nOptimization for Machine Learning\\nversion 0.57\\nAll rights reserved.\\nElad Hazan 1\\n1www.cs.princeton.edu/~ehazan\\narXiv:1909.03550v1  [cs.LG]  8 Sep 2019\\n\\nii\\n\\nPreface\\nThis text was written to accompany a series of lectures given at the Machine\\nLearning Summer School Buenos Aires, following a lecture series at the\\nSimons Center for Theoretical Computer Science, Berkeley. It was extended\\nfor the course COS 598D - Optimization for Machine Learning, Princeton\\nUniversity, Spring 2019.\\nI am grateful to Paula Gradu for proofreading parts of this manuscript.\\nI’m also thankful for the help of the following students and colleagues for\\ncorrections and suggestions to this text: Udaya Ghai, John Hallman, No´\\ne\\nPion, Xinyi Chen.\\niii\\n\\niv\\nPreface\\nFigure 1:\\nProfessor Arkadi Nemirovski, Pioneer of mathematical optimiza-\\ntion\\n\\nContents\\nPreface\\niii\\n1\\nIntroduction\\n3\\n1.1\\nExamples of optimization problems in machine learning\\n. . .\\n4\\n1.1.1\\nEmpirical Risk Minimization\\n. . . . . . . . . . . . . .\\n4\\n1.1.2\\nMatrix completion and recommender systems . . . . .\\n6\\n1.1.3\\nLearning in Linear Dynamical Systems\\n. . . . . . . .\\n7\\n1.2\\nWhy is mathematical programming hard? . . . . . . . . . . .\\n8\\n1.2.1\\nThe computational model . . . . . . . . . . . . . . . .\\n8\\n1.2.2\\nHardness of constrained mathematical programming .\\n9\\n2\\nBasic concepts in optimization and analysis\\n11\\n2.1\\nBasic deﬁnitions and the notion of convexity . . . . . . . . . .\\n11\\n2.1.1\\nProjections onto convex sets . . . . . . . . . . . . . . .\\n13\\n2.1.2\\nIntroduction to optimality conditions . . . . . . . . . .\\n14\\n2.1.3\\nSolution concepts for non-convex optimization\\n. . . .\\n15\\n2.2\\nPotentials for distance to optimality\\n. . . . . . . . . . . . . .\\n16\\n2.3\\nGradient descent and the Polyak stepsize\\n. . . . . . . . . . .\\n18\\n2.4\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.5\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n23\\n3\\nStochastic Gradient Descent\\n25\\n3.1\\nTraining feedforward neural networks\\n. . . . . . . . . . . . .\\n25\\n3.2\\nGradient descent for smooth optimization . . . . . . . . . . .\\n27\\n3.3\\nStochastic gradient descent\\n. . . . . . . . . . . . . . . . . . .\\n29\\n3.4\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n31\\n4\\nGeneralization and Non-Smooth Optimization\\n33\\n4.1\\nA note on non-smooth optimization\\n. . . . . . . . . . . . . .\\n34\\n4.2\\nMinimizing Regret . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\nv\\n\\nvi\\nCONTENTS\\n4.3\\nRegret implies generalization\\n. . . . . . . . . . . . . . . . . .\\n35\\n4.4\\nOnline gradient descent\\n. . . . . . . . . . . . . . . . . . . . .\\n36\\n4.5\\nLower bounds . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.6\\nOnline gradient descent for strongly convex functions . . . . .\\n39\\n4.7\\nOnline Gradient Descent implies SGD . . . . . . . . . . . . .\\n41\\n4.8\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n4.9\\nBibliographic remarks\\n. . . . . . . . . . . . . . . . . . . . . .\\n45\\n5\\nRegularization\\n47\\n5.1\\nMotivation: prediction from expert advice . . . . . . . . . . .\\n47\\n5.1.1\\nThe weighted majority algorithm . . . . . . . . . . . .\\n49\\n5.1.2\\nRandomized weighted majority . . . . . . . . . . . . .\\n51\\n5.1.3\\nHedge . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n5.2\\nThe Regularization framework\\n. . . . . . . . . . . . . . . . .\\n53\\n5.2.1\\nThe RFTL algorithm\\n. . . . . . . . . . . . . . . . . .\\n54\\n5.2.2\\nMirrored Descent . . . . . . . . . . . . . . . . . . . . .\\n55\\n5.2.3\\nDeriving online gradient descent\\n. . . . . . . . . . . .\\n56\\n5.2.4\\nDeriving multiplicative updates . . . . . . . . . . . . .\\n57\\n5.3\\nTechnical background: regularization functions . . . . . . . .\\n57\\n5.4\\nRegret bounds for Mirrored Descent\\n. . . . . . . . . . . . . .\\n59\\n5.5\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n5.6\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n63\\n6\\nAdaptive Regularization\\n65\\n6.1\\nAdaptive Learning Rates: Intuition . . . . . . . . . . . . . . .\\n65\\n6.2\\nA Regularization Viewpoint\\n. . . . . . . . . . . . . . . . . .\\n66\\n6.3\\nTools from Matrix Calculus . . . . . . . . . . . . . . . . . . .\\n66\\n6.4\\nThe AdaGrad Algorithm and Its Analysis . . . . . . . . . . .\\n67\\n6.5\\nDiagonal AdaGrad . . . . . . . . . . . . . . . . . . . . . . . .\\n71\\n6.6\\nState-of-the-art: from Adam to Shampoo and beyond\\n. . . .\\n72\\n6.7\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n73\\n6.8\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n74\\n7\\nVariance Reduction\\n75\\n7.1\\nVariance reduction: Intuition . . . . . . . . . . . . . . . . . .\\n75\\n7.2\\nSetting and deﬁnitions . . . . . . . . . . . . . . . . . . . . . .\\n76\\n7.3\\nThe variance reduction advantage . . . . . . . . . . . . . . . .\\n77\\n7.4\\nA simple variance-reduced algorithm . . . . . . . . . . . . . .\\n78\\n7.5\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n80\\n\\nCONTENTS\\nvii\\n8\\nNesterov Acceleration\\n81\\n8.1\\nAlgorithm and implementation . . . . . . . . . . . . . . . . .\\n81\\n8.2\\nAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.3\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n84\\n9\\nThe conditional gradient method\\n85\\n9.1\\nReview: relevant concepts from linear algebra . . . . . . . . .\\n85\\n9.2\\nMotivation: matrix completion and recommendation systems\\n86\\n9.3\\nThe Frank-Wolfe method\\n. . . . . . . . . . . . . . . . . . . .\\n88\\n9.4\\nProjections vs. linear optimization . . . . . . . . . . . . . . .\\n90\\n9.5\\nExercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n9.6\\nBibliographic Remarks . . . . . . . . . . . . . . . . . . . . . .\\n94\\n10 Second order methods for machine learning\\n95\\n10.1 Motivating example: linear regression\\n. . . . . . . . . . . . .\\n95\\n10.2 Self-Concordant Functions . . . . . . . . . . . . . . . . . . . .\\n96\\n10.3 Newton’s method for self-concordant functions\\n. . . . . . . .\\n97\\n10.4 Linear-time second-order methods\\n. . . . . . . . . . . . . . . 100\\n10.4.1 Estimators for the Hessian Inverse . . . . . . . . . . . 100\\n10.4.2 Incorporating the estimator . . . . . . . . . . . . . . . 101\\n10.5 Exercises\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n10.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 104\\n11 Hyperparameter Optimization\\n105\\n11.1 Formalizing the problem . . . . . . . . . . . . . . . . . . . . . 105\\n11.2 Hyperparameter optimization algorithms . . . . . . . . . . . . 106\\n11.3 A Spectral Method . . . . . . . . . . . . . . . . . . . . . . . . 107\\n11.3.1 Background: Compressed Sensing\\n. . . . . . . . . . . 108\\n11.3.2 The Spectral Algorithm . . . . . . . . . . . . . . . . . 110\\n11.4 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . 111\\n\\nviii\\nCONTENTS\\n\\nNotation\\nWe use the following mathematical notation in this writeup:\\n• d-dimensional Euclidean space is denoted Rd.\\n• Vectors are denoted by boldface lower-case letters such as x ∈Rd. Co-\\nordinates of vectors are denoted by underscore notation xi or regular\\nbrackets x(i).\\n• Matrices are denoted by boldface upper-case letters such as X ∈Rm×n.\\nTheir coordinates by X(i, j), or Xij.\\n• Functions are denoted by lower case letters f : Rd 7→R.\\n• The k-th diﬀerential of function f is denoted by ∇kf ∈Rdk.\\nThe\\ngradient is denoted without the superscript, as ∇f.\\n• We use the mathcal macro for sets, such as K ⊆Rd.\\n• We denote the gradient at point xt as ∇xt, or simply ∇t.\\n• We denote the global or local optima of functions by x⋆.\\n• We denote distance to optimality for iterative algorithms by ht =\\nf(xt) −f(x⋆).\\n• Euclidean distance to optimality is denoted dt = ∥xt −x⋆∥.\\n1\\n\\n2\\nCONTENTS\\n\\nChapter 1\\nIntroduction\\nThe topic of this lecture series is the mathematical optimization approach\\nto machine learning.\\nIn standard algorithmic theory, the burden of designing an eﬃcient al-\\ngorithm for solving a problem at hand is on the algorithm designer. In the\\ndecades since in the introduction of computer science, elegant algorithms\\nhave been designed for tasks ranging from ﬁnding the shortest path in a\\ngraph, computing the optimal ﬂow in a network, compressing a computer\\nﬁle containing an image captured by digital camera, and replacing a string\\nin a text document.\\nThe design approach, while useful to many tasks, falls short of more\\ncomplicated problems, such as identifying a particular person in an image\\nin bitmap format, or translating text from English to Hebrew. There may\\nvery well be an elegant algorithm for the above tasks, but the algorithmic\\ndesign scheme does not scale.\\nAs Turing promotes in his paper [83], it is potentially easier to teach a\\ncomputer to learn how to solve a task, rather than teaching it the solution\\nfor the particular tasks. In eﬀect, that’s what we do at school, or in this\\nlecture series...\\nThe machine learning approach to solving problems is to have an au-\\ntomated mechanism for learning an algorithm.\\nConsider the problem of\\nclassifying images into two categories: those containing cars and those con-\\ntaining chairs (assuming there are only two types of images in the world).\\nIn ML we train (teach) a machine to achieve the desired functionality. The\\nsame machine can potentially solve any algorithmic task, and diﬀers from\\ntask to task only by a set of parameters that determine the functionality of\\nthe machine. This is much like the wires in a computer chip determine its\\n3\\n\\n4\\nCHAPTER 1. INTRODUCTION\\nfunctionality. Indeed, one of the most popular machines are artiﬁcial neural\\nnetworks.\\nThe mathematical optimization approach to machine learning is to view\\nthe process of machine training as an optimization problem. If we let w ∈Rd\\nbe the parameters of our machine (a.k.a. model), that are constrained to\\nbe in some set K ⊆Rd, and f the function measuring success in mapping\\nexamples to their correct label, then the problem we are interested in is\\ndescribed by the mathematical optimization problem of\\nmin\\nw∈K f(w)\\n(1.1)\\nThis is the problem that the lecture series focuses on, with particular em-\\nphasis on functions that arise in machine learning and have special structure\\nthat allows for eﬃcient algorithms.\\n1.1\\nExamples of optimization problems in machine\\nlearning\\n1.1.1\\nEmpirical Risk Minimization\\nMachine learning problems exhibit special structure. For example, one of\\nthe most basic optimization problems in supervised learning is that of ﬁtting\\na model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems\\narising in such formulations is separability across diﬀerent examples into\\nindividual losses.\\nAn example of such formulation is the supervised learning paradigm of\\nlinear classiﬁcation. In this model, the learner is presented with positive and\\nnegative examples of a concept. Each example, denoted by ai, is represented\\nin Euclidean space by a d dimensional feature vector. For example, a com-\\nmon representation for emails in the spam-classiﬁcation problem are binary\\nvectors in Euclidean space, where the dimension of the space is the number of\\nwords in the language. The i’th email is a vector ai whose entries are given\\nas ones for coordinates corresponding to words that appear in the email,\\nand zero otherwise1. In addition, each example has a label bi ∈{−1, +1},\\ncorresponding to whether the email has been labeled spam/not spam. The\\n1Such a representation may seem na¨\\nıve at ﬁrst as it completely ignores the words’ order\\nof appearance and their context. Extensions to capture these features are indeed studied\\nin the Natural Language Processing literature.\\n\\n1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING5\\ngoal is to ﬁnd a hyperplane separating the two classes of vectors: those with\\npositive labels and those with negative labels. If such a hyperplane, which\\ncompletely separates the training set according to the labels, does not ex-\\nist, then the goal is to ﬁnd a hyperplane that achieves a separation of the\\ntraining set with the smallest number of mistakes.\\nMathematically speaking, given a set of m examples to train on, we seek\\nx ∈Rd that minimizes the number of incorrectly classiﬁed examples, i.e.\\nmin\\nx∈Rd\\n1\\nm\\nX\\ni∈[m]\\nδ(sign(x⊤ai) ̸= bi)\\n(1.2)\\nwhere sign(x) ∈{−1, +1} is the sign function, and δ(z) ∈{0, 1} is the\\nindicator function that takes the value 1 if the condition z is satisﬁed and\\nzero otherwise.\\nThe mathematical formulation of the linear classiﬁcation above is a spe-\\ncial case of mathematical programming (1.1), in which\\nf(x) = 1\\nm\\nX\\ni∈[m]\\nδ(sign(x⊤ai) ̸= bi) =\\nE\\ni∼[m][ℓi(x)],\\nwhere we make use of the expectation operator for simplicity, and denote\\nℓi(x) = δ(sign(x⊤ai) ̸= bi) for brevity. Since the program above is non-\\nconvex and non-smooth, it is common to take a convex relaxation and replace\\nℓi with convex loss functions. Typical choices include the means square error\\nfunction and the hinge loss, given by\\nℓai,bi(x) = max{0, 1 −bi · x⊤ai}.\\nThis latter loss function in the context of binary classiﬁcation gives rise\\nto the popular soft-margin SVM problem.\\nAnother important optimization problem is that of training a deep neural\\nnetwork for binary classiﬁcation. For example, consider a dataset of images,\\nrepresented in bitmap format and denoted by {ai ∈Rd|i ∈[m]}, i.e. m\\nimages over n pixels. We would like to ﬁnd a mapping from images to the\\ntwo categories, {bi ∈{0, 1}} of cars and chairs. The mapping is given by a\\nset of parameters of a machine class, such as weights in a neural network,\\nor values of a support vector machine.\\nWe thus try to ﬁnd the optimal\\nparameters that match ai to b, i..e\\nmin\\nw∈Rd f(w) = E\\nai,bi [ℓ(fw(ai), bi)] .\\n\\n6\\nCHAPTER 1. INTRODUCTION\\n1.1.2\\nMatrix completion and recommender systems\\nMedia recommendations have changed signiﬁcantly with the advent of the\\nInternet and rise of online media stores. The large amounts of data collected\\nallow for eﬃcient clustering and accurate prediction of users’ preferences\\nfor a variety of media.\\nA well-known example is the so called “Netﬂix\\nchallenge”—a competition of automated tools for recommendation from a\\nlarge dataset of users’ motion picture preferences.\\nOne of the most successful approaches for automated recommendation\\nsystems, as proven in the Netﬂix competition, is matrix completion. Perhaps\\nthe simplest version of the problem can be described as follows.\\nThe entire dataset of user-media preference pairs is thought of as a\\npartially-observed matrix. Thus, every person is represented by a row in\\nthe matrix, and every column represents a media item (movie). For sim-\\nplicity, let us think of the observations as binary—a person either likes or\\ndislikes a particular movie. Thus, we have a matrix M ∈{0, 1, ∗}n×m where\\nn is the number of persons considered, m is the number of movies at our\\nlibrary, and 0/1 and ∗signify “dislike”, “like” and “unknown” respectively:\\nMij =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n0,\\nperson i dislikes movie j\\n1,\\nperson i likes movie j\\n∗,\\npreference unknown\\n.\\nThe natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\\nthe unknown entries. As deﬁned so far, the problem is ill-posed, since any\\ncompletion would be equally good (or bad), and no restrictions have been\\nplaced on the completions.\\nThe common restriction on completions is that the “true” matrix has\\nlow rank. Recall that if a matrix X ∈Rn×m has rank k ≤ρ = min{n, m}\\nthen it can be written as\\nX = UV , U ∈Rn×k, V ∈Rk×m.\\nThe intuitive interpretation of this property is that each entry in M\\ncan be explained by only k numbers.\\nIn matrix completion this means,\\nintuitively, that there are only k factors that determine a persons preference\\nover movies, such as genre, director, actors and so on.\\nNow the simplistic matrix completion problem can be well-formulated\\nas in the following mathematical program. Denote by ∥· ∥OB the Euclidean\\n\\n1.1. EXAMPLES OF OPTIMIZATION PROBLEMS IN MACHINE LEARNING7\\nnorm only on the observed (non starred) entries of M, i.e.,\\n∥X∥2\\nOB =\\nX\\nMij̸=∗\\nX2\\nij.\\nThe mathematical program for matrix completion is given by\\nmin\\nX∈Rn×m\\n1\\n2∥X −M∥2\\nOB\\ns.t.\\nrank(X) ≤k.\\n1.1.3\\nLearning in Linear Dynamical Systems\\nMany learning problems require memory, or the notion of state. This is\\ncaptured by the paradigm of reinforcement learning, as well of the special\\ncase of control in Linear Dynamical Systems (LDS).\\nLDS model a variety of control and robotics problems in continuous\\nvariables. The setting is that of a time series, with following parameters:\\n1. Inputs to the system, also called controls, denoted by u1, ..., uT ∈Rn.\\n2. Outputs from the system, also called observations, denoted y1, ..., yT ∈\\nRm.\\n3. The state of the system, which may either be observed or hidden,\\ndenoted xt, ..., xT ∈Rd.\\n4. The system parameters, which are transformations matrices A, B, C, D\\nin appropriate dimensions.\\nIn the online learning problem of LDS, the learner iteratively observes\\nut, yt, and has to predict ˆ\\nyt+1. The actual yt is generated according to the\\nfollowing dynamical equations:\\nxt+1 = Axt + But + εt\\nyt+1 = Cxt+1 + Dut + ζt,\\nwhere εt, ζt are noise which is distributed as a Normal random variable.\\nConsider an online sequence in which the states are visible. At time t,\\nall system states, inputs and outputs are visible up to this time step. The\\nlearner has to predict yt+1, and only afterwards observes ut+1.xt+1, yt+1.\\n\\n8\\nCHAPTER 1. INTRODUCTION\\nOne reasonable way to predict yt+1 based upon past observations is to\\ncompute the system, and use the computed transformations to predict. This\\namounts to solving the following mathematical program:\\nmin\\nA,B, ˆ\\nC, ˆ\\nD\\n(X\\nτ<t\\n(xτ+1 −Axτ + Buτ)2 + (yτ+1 −ˆ\\nCxτ + ˆ\\nDuτ)2\\n)\\n,\\nand then predicting ˆ\\nyt+1 = ˆ\\nC ˆ\\nA(xt + But) + ˆ\\nDut.\\n1.2\\nWhy is mathematical programming hard?\\nThe general formulation (1.1) is NP hard. To be more precise, we have to\\ndeﬁne the computational model we are working in as well as and the access\\nmodel to the function.\\nBefore we give a formal proof, the intuition to what makes mathematical\\noptimization hard is simple to state. In one line: it is the fact that global\\noptimality cannot be veriﬁed on the basis of local properties.\\nMost, if not all, eﬃcient optimization algorithms are iterative and based\\non a local improvement step. By this nature, any optimization algorithm\\nwill terminate when the local improvement is no longer possible, giving rise\\nto a proposed solution. However, the quality of this proposed solution may\\ndiﬀer signiﬁcantly, in general, from that of the global optimum.\\nThis intuition explains the need for a property of objectives for which\\nglobal optimality is locally veriﬁable.\\nIndeed, this is exactly the notion\\nof convexity, and the reasoning above explains its utmost importance in\\nmathematical optimization.\\nWe now to prove that mathematical programming is NP-hard.\\nThis\\nrequires discussion of the computational model as well as access model to\\nthe input.\\n1.2.1\\nThe computational model\\nThe computational model we shall adopt throughout this manuscript is that\\nof a RAM machine equipped with oracle access to the objective function\\nf : Rd 7→R and constraints set K ⊆Rd. The oracle model for the objective\\nfunction can be one of the following, depending on the speciﬁc scenario:\\n1. Value oracle:\\ngiven a point x ∈Rd, oracle returns f(x) ∈R.\\n2. Gradient (ﬁrst-order) oracle:\\ngiven a point x ∈Rd, oracle returns\\nthe gradient ∇f(x) ∈Rd.\\n\\n1.2. WHY IS MATHEMATICAL PROGRAMMING HARD?\\n9\\n3. k-th order diﬀerential oracle:\\ngiven a point x ∈Rd, oracle returns\\nthe tensor ∇kf(x) ∈Rdk.\\nThe oracle model for the constraints set is a bit more subtle. We distin-\\nguish between the following oracles:\\n1. Membership oracle:\\ngiven a point x ∈Rd, oracle returns one if\\nx ∈K and zero otherwise.\\n2. Separating hyperplane oracle:\\ngiven a point x ∈Rd, oracle either\\nreturns ”Yes” if x ∈K, or otherwise returns a hyperplane h ∈Rd such\\nthat h⊤x > 0 and ∀y ∈K , h⊤y ≤0.\\n3. Explicit sets:\\nthe most common scenario in machine learning is one\\nin which K is “natural”, such as the Euclidean ball or hypercube, or\\nthe entire Euclidean space.\\n1.2.2\\nHardness of constrained mathematical programming\\nUnder this computational model, we can show:\\nLemma 1.1. Mathematical programming is NP-hard, even for a convex\\ncontinuous constraint set K and quadratic objective functions.\\nInformal sketch. Consider the MAX-CUT problem:\\ngiven a graph G =\\n(V, E), ﬁnd a subset of the vertices that maximizes the number of edges\\ncut. Let A be the negative adjacency matrix of the graph, i.e.\\nAij =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n−1,\\n(i, j) ∈E\\n0,\\no/w\\nAlso suppose that Aii = 0.\\nNext, consider the mathematical program:\\nmin\\n\\x1a\\nfA(x) = 1\\n4(x⊤Ax −2|E|)\\n\\x1b\\n(1.3)\\n∥x∥∞= 1 .\\nConsider the cut deﬁned by the solution of this program, namely\\nSx = {i ∈V |xi = 1},\\nfor x = x⋆. Let C(S) denote the size of the cut speciﬁed by the subset of\\nedges S ⊆E. Observe that the expression 1\\n2x⊤Ax, is exactly equal to the\\n\\n10\\nCHAPTER 1. INTRODUCTION\\nnumber of edges that are cut by Sx minus the number of edges that are\\nuncut. Thus, we have\\n1\\n2xAx = C(Sx) −(E −C(Sx)) = 2C(Sx) −E,\\nand hence f(x) = C(Sx). Therefore, maximizing f(x) is equivalent to the\\nMAX-CUT problem, and is thus NP-hard. We proceed to make the con-\\nstraint set convex and continuous. Consider the mathematical program\\nmin {fA(x)}\\n(1.4)\\n∥x∥∞≤1 .\\nThis is very similar to the previous program, but we relaxed the equality\\nto be an inequality, consequently the constraint set is now the hypercube.\\nWe now claim that the solution is w.l.o.g. a vertex. To see that, consider\\ny(x) ∈{±1}d a rounding of x to the corners deﬁned by:\\nyi = y(x)i =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1,\\nw.p. 1+xi\\n2\\n−1,\\nw.p. 1−xi\\n2\\nNotice that\\nE[y] = x , ∀i ̸= j . E[yiyj] = xixj,\\nand therefore E[y(x)⊤Ay(x)] = x⊤Ax. We conclude that the optimum of\\nmathematical program 1.4 is the same as that for 1.3, and both are NP-\\nhard.\\n\\nChapter 2\\nBasic concepts in\\noptimization and analysis\\n2.1\\nBasic deﬁnitions and the notion of convexity\\nWe consider minimization of a continuous function over a convex subset of\\nEuclidean space. We mostly consider objective functions that are convex. In\\nlater chapters we relax this requirement and consider non-convex functions\\nas well.\\nHenceforth, let K ⊆Rd be a bounded convex and compact set in Eu-\\nclidean space. We denote by D an upper bound on the diameter of K:\\n∀x, y ∈K, ∥x −y∥≤D.\\nA set K is convex if for any x, y ∈K, all the points on the line segment\\nconnecting x and y also belong to K, i.e.,\\n∀α ∈[0, 1], αx + (1 −α)y ∈K.\\nA function f : K 7→R is convex if for any x, y ∈K\\n∀α ∈[0, 1], f(αx + (1 −α)y) ≤αf(x) + (1 −α)f(y).\\nGradients and subgradients.\\nThe set of all subgradients of a function\\nf at x, denoted ∂f(x), is the set of all vectors u such that\\nf(y) ≥f(x) + u⊤(y −x).\\nIt can be shown that the set of subgradients of a convex function is always\\nnon-empty.\\n11\\n\\n12\\nCHAPTER 2. BASIC CONCEPTS\\nSuppose f is diﬀerentiable, let ∇f(x)[i] =\\n∂\\n∂xi f(x) be the vector of\\npartial derivatives according to the variables, called the gradient.\\nIf the\\ngradient ∇f(x) exists, then ∇f(x) ∈∂f(x) and ∀y ∈K\\nf(y) ≥f(x) + ∇f(x)⊤(y −x).\\nHenceforth we shall denote by ∇f(x) the gradient, if it exists, or any member\\nof ∂f(x) otherwise.\\nWe denote by G > 0 an upper bound on the norm of the subgradients of\\nf over K, i.e., ∥∇f(x)∥≤G for all x ∈K. The existence of Such an upper\\nbound implies that the function f is Lipschitz continuous with parameter\\nG, that is, for all x, y ∈K\\n|f(x) −f(y)| ≤G∥x −y∥.\\nSmoothness and strong convexity.\\nThe optimization and machine learn-\\ning literature studies special types of convex functions that admit useful\\nproperties, which in turn allow for more eﬃcient optimization. Notably, we\\nsay that a function is α-strongly convex if\\nf(y) ≥f(x) + ∇f(x)⊤(y −x) + α\\n2 ∥y −x∥2.\\nA function is β-smooth if\\nf(y) ≤f(x) + ∇f(x)⊤(y −x) + β\\n2 ∥y −x∥2.\\nThe latter condition is implied by a slightly stronger Lipschitz condition\\nover the gradients, which is sometimes used to deﬁned smoothness, i.e.,\\n∥∇f(x) −∇f(y)∥≤β∥x −y∥.\\nIf the function is twice diﬀerentiable and admits a second derivative,\\nknown as a Hessian for a function of several variables, the above conditions\\nare equivalent to the following condition on the Hessian, denoted ∇2f(x):\\nSmoothness:\\n−βI ≼∇2f(x) ≼βI\\nStrong-convexity:\\nαI ≼∇2f(x),\\nwhere A ≼B if the matrix B −A is positive semideﬁnite.\\nWhen the function f is both α-strongly convex and β-smooth, we say\\nthat it is γ-well-conditioned where γ is the ratio between strong convexity\\nand smoothness, also called the condition number of f\\nγ = α\\nβ ≤1\\n\\n2.1. BASICS\\n13\\n2.1.1\\nProjections onto convex sets\\nIn the following algorithms we shall make use of a projection operation onto\\na convex set, which is deﬁned as the closest point inside the convex set to a\\ngiven point. Formally,\\nΠ\\nK(y) ≜arg min\\nx∈K\\n∥x −y∥.\\nWhen clear from the context, we shall remove the K subscript. It is left as\\nan exercise to the reader to prove that the projection of a given point over\\na closed non-empty convex set exists and is unique.\\nThe computational complexity of projections is a subtle issue that de-\\npends much on the characterization of K itself. Most generally, K can be\\nrepresented by a membership oracle—an eﬃcient procedure that is capable\\nof deciding whether a given x belongs to K or not. In this case, projections\\ncan be computed in polynomial time. In certain special cases, projections\\ncan be computed very eﬃciently in near-linear time.\\nA crucial property of projections that we shall make extensive use of is\\nthe Pythagorean theorem, which we state here for completeness:\\nFigure 2.1: Pythagorean theorem.\\nTheorem 2.1 (Pythagoras, circa 500 BC). Let K ⊆Rd be a convex set,\\ny ∈Rd and x = ΠK(y). Then for any z ∈K we have\\n∥y −z∥≥∥x −z∥.\\n\\n14\\nCHAPTER 2. BASIC CONCEPTS\\nWe note that there exists a more general version of the Pythagorean\\ntheorem. The above theorem and the deﬁnition of projections are true and\\nvalid not only for Euclidean norms, but for projections according to other\\ndistances that are not norms. In particular, an analogue of the Pythagorean\\ntheorem remains valid with respect to Bregman divergences.\\n2.1.2\\nIntroduction to optimality conditions\\nThe standard curriculum of high school mathematics contains the basic facts\\nconcerning when a function (usually in one dimension) attains a local opti-\\nmum or saddle point. The KKT (Karush-Kuhn-Tucker) conditions general-\\nize these facts to more than one dimension, and the reader is referred to the\\nbibliographic material at the end of this chapter for an in-depth rigorous\\ndiscussion of optimality conditions in general mathematical programming.\\nFor our purposes, we describe only brieﬂy and intuitively the main facts\\nthat we will require henceforth. We separate the discussion into convex and\\nnon-convex programming.\\nOptimality for convex optimization\\nA local minimum of a convex function is also a global minimum (see exercises\\nat the end of this chapter). We say that x⋆is an ε-approximate optimum if\\nthe following holds:\\n∀x ∈K . f(x⋆) ≤f(x) + ε.\\nThe generalization of the fact that a minimum of a convex diﬀerentiable\\nfunction on R is a point in which its derivative is equal to zero, is given by\\nthe multi-dimensional analogue that its gradient is zero:\\n∇f(x) = 0\\n⇐\\n⇒\\nx ∈arg min\\nx∈Rn\\nf(x).\\nWe will require a slightly more general, but equally intuitive, fact for con-\\nstrained optimization: at a minimum point of a constrained convex function,\\nthe inner product between the negative gradient and direction towards the\\ninterior of K is non-positive. This is depicted in Figure 2.2, which shows that\\n−∇f(x⋆) deﬁnes a supporting hyperplane to K. The intuition is that if the\\ninner product were positive, one could improve the objective by moving in\\nthe direction of the projected negative gradient. This fact is stated formally\\nin the following theorem.\\n\\n2.1. BASICS\\n15\\nTheorem 2.2 (Karush-Kuhn-Tucker). Let K ⊆Rd be a convex set, x⋆∈\\narg minx∈K f(x). Then for any y ∈K we have\\n∇f(x⋆)⊤(y −x⋆) ≥0.\\nFigure 2.2: Optimality conditions: negative (sub)gradient pointing out-\\nwards.\\n2.1.3\\nSolution concepts for non-convex optimization\\nWe have seen in the previous chapter that mathematical optimization is NP-\\nhard. This implies that ﬁnding global solutions for non-convex optimization\\nis NP-hard, even for smooth functions over very simple convex domains. We\\nthus consider other trackable concepts of solutions.\\nThe most common solution concept is that of ﬁrst-order optimality, a.k.a.\\nsaddle-points or stationary points. These are points that satisfy\\n∥∇f(x⋆)∥= 0.\\nUnfortunately, even ﬁnding such stationary points is NP-hard.\\nWe thus\\nsettle for approximate stationary points, which satisify\\n∥∇f(x⋆)∥≤ε.\\n\\n16\\nCHAPTER 2. BASIC CONCEPTS\\nFigure 2.3: First and second-order local optima.\\nA more stringent notion of optimality we may consider is obtained by\\nlooking at the second derivatives. We can require they behave as for global\\nminimum, see ﬁgure 2.3. Formally, we say that a point x⋆is a second-order\\nlocal minimum if it satisﬁes the two conditions:\\n∥∇f(x⋆)∥≤ε , ∇2f(x⋆) ⪰−√εI.\\nThe diﬀerences in approximation criteria for ﬁrst and second derivatives is\\nnatural, as we shall explore in non-convex approximation algorithms hence-\\nforth.\\nWe note that it is possible to further deﬁne optimality conditions for\\nhigher order derivatives, although this is less useful in the context of machine\\nlearning.\\n2.2\\nPotentials for distance to optimality\\nWhen analyzing convergence of gradient methods, it is useful to use potential\\nfunctions in lieu of function distance to optimality, such as gradient norm\\nand/or Euclidean distance to optimality. The following relationships hold\\nbetween these quantities.\\nLemma 2.3. The following properties hold for α-strongly-convex functions\\nand/or β-smooth functions over Euclidean space Rd.\\n1.\\nα\\n2 d2\\nt ≤ht\\n2. ht ≤β\\n2 d2\\nt\\n\\n2.2. POTENTIALS FOR DISTANCE TO OPTIMALITY\\n17\\n3.\\n1\\n2β∥∇t∥2 ≤ht\\n4. ht ≤\\n1\\n2α∥∇t∥2\\nProof.\\n1. ht ≥α\\n2 d2\\nt :\\nBy strong convexity, we have\\nht\\n= f(xt) −f(x⋆)\\n≥∇f(x⋆)⊤(xt −x⋆) + α\\n2 ∥xt −x⋆∥2\\n= α\\n2 ∥xt −x⋆∥2\\nwhere the last inequality follows since the gradient at the global opti-\\nmum is zero.\\n2. ht ≤β\\n2 d2\\nt :\\nBy smoothness,\\nht\\n= f(xt) −f(x⋆)\\n≤∇f(x⋆)⊤(xt −x⋆) + β\\n2 ∥xt −x⋆∥2\\n= β\\n2 ∥xt −x⋆∥2\\nwhere the last inequality follows since the gradient at the global opti-\\nmum is zero.\\n3. ht ≥\\n1\\n2β∥∇t∥2: Using smoothness, and let xt+1 = xt −η∇t for η = 1\\nβ,\\nht =\\nf(xt) −f(x⋆)\\n≥f(xt) −f(xt+1)\\n≥∇f(xt)⊤(xt −xt+1) −β\\n2 ∥xt −xt+1∥2\\n= η∥∇t∥2 −β\\n2 η2∥∇t∥2\\n=\\n1\\n2β∥∇t∥2.\\n4. ht ≤\\n1\\n2α∥∇t∥2:\\nWe have for any pair x, y ∈Rd:\\nf(y) ≥f(x) + ∇f(x)⊤(y −x) + α\\n2 ∥x −y∥2\\n≥min\\nz∈Rd\\nn\\nf(x) + ∇f(x)⊤(z −x) + α\\n2 ∥x −z∥2o\\n= f(x) −1\\n2α∥∇f(x)∥2.\\nby taking z = x −1\\nα∇f(x)\\n\\n18\\nCHAPTER 2. BASIC CONCEPTS\\nIn particular, taking x = xt , y = x⋆, we get\\nht = f(xt) −f(x⋆) ≤1\\n2α∥∇t∥2.\\n(2.1)\\n2.3\\nGradient descent and the Polyak stepsize\\nThe simplest iterative optimization algorithm is gradient descent, as given\\nin Algorithm 1. We analyze GD with the Polyak stepsize, which has the\\nadvantage of not depending on the strong convexity and/or smoothness\\nparameters of the objective function.\\nAlgorithm 1 GD with the Polyak stepsize\\n1: Input: time horizon T, x0\\n2: for t = 0, . . . , T −1 do\\n3:\\nSet ηt =\\nht\\n∥∇t∥2\\n4:\\nxt+1 = xt −ηt∇t\\n5: end for\\n6: Return ¯\\nx = arg minxt{f(xt)}\\nTo prove convergence bounds, assume ∥∇t∥≤G, and deﬁne:\\nBT\\n=\\nmin\\n(\\nGd0\\n√\\nT\\n, 2βd2\\n0\\nT\\n, 3G2\\nαT , βd2\\n0\\n\\x12\\n1 −α\\n4β\\n\\x13T )\\nTheorem 2.4. (GD with the Polyak Step Size) Algorithm 1 attains the\\nfollowing regret bound after T steps:\\nh(¯\\nx)\\n=\\nmin\\n0≤t≤T{ht} ≤BT\\nTheorem 2.4 directly follows from the following lemma. Let 0 ≤γ ≤1,\\ndeﬁne RT,γ as follows:\\nRT,γ = min\\n(\\nGd0\\n√γT , 2βd2\\n0\\nγT , 3G2\\nγαT , βd2\\n0\\n\\x12\\n1 −γ α\\n4β\\n\\x13T )\\n.\\nLemma 2.5. For 0 ≤γ ≤1, suppose that a sequence x0, . . . xt satisﬁes:\\nd2\\nt+1 ≤d2\\nt −γ\\nh2\\nt\\n∥∇t∥2\\n(2.2)\\n\\n2.3. GRADIENT DESCENT AND THE POLYAK STEPSIZE\\n19\\nthen for ¯\\nx as deﬁned in the algorithm, we have:\\nh(¯\\nx) ≤RT,γ .\\nProof. The proof analyzes diﬀerent cases:\\n1. For convex functions with gradient bounded by G,\\nd2\\nt+1 −d2\\nt\\n≤−γh2\\nt\\n∥∇t∥2 ≤−γh2\\nt\\nG2\\nSumming up over T iterations, and using Cauchy-Schwartz, we have\\n1\\nT\\nX\\nt\\nht\\n≤\\n1\\n√\\nT\\nsX\\nt\\nh2\\nt\\n≤\\nG\\n√γT\\nsX\\nt\\n(d2\\nt −d2\\nt+1) ≤Gd0\\n√γT .\\n2. For smooth functions whose gradient is bounded by G, Lemma 2.3\\nimplies:\\nd2\\nt+1 −d2\\nt ≤−γh2\\nt\\n∥∇t∥2 ≤−γht\\n2β .\\nThis implies\\n1\\nT\\nX\\nt\\nht ≤2βd2\\n0\\nγT\\n.\\n3. For strongly convex functions, Lemma 2.3 implies:\\nd2\\nt+1 −d2\\nt ≤−γ\\nh2\\nt\\n∥∇t∥2 ≤−γ h2\\nt\\nG2 ≤−γ α2d4\\nt\\n4G2 .\\nIn other words, d2\\nt+1 ≤d2\\nt (1 −γ α2d2\\nt\\n4G2 ) . Deﬁning at := γ α2d2\\nt\\n4G2 , we have:\\nat+1 ≤at(1 −at) .\\n\\n20\\nCHAPTER 2. BASIC CONCEPTS\\nThis implies that at ≤\\n1\\nt+1, which can be seen by induction1. The\\nproof is completed as follows2 :\\n1\\nT/2\\nT\\nX\\nt=T/2\\nh2\\nt\\n≤\\n2G2\\nγT\\nT\\nX\\nt=T/2\\n(d2\\nt −d2\\nt+1)\\n=\\n2G2\\nγT (d2\\nT/2 −d2\\nT )\\n=\\n8G4\\nγ2α2T (aT/2 −aT )\\n≤\\n9G4\\nγ2α2T 2 .\\nThus, there exists a t for which h2\\nt ≤\\n9G4\\nγ2α2T 2 . Taking the square root\\ncompletes the claim.\\n4. For both strongly convex and smooth functions:\\nd2\\nt+1 −d2\\nt ≤−γ\\nh2\\nt\\n∥∇t∥2 ≤−γht\\n2β ≤−γ α\\n4β d2\\nt\\nThus,\\nhT ≤βd2\\nT ≤βd2\\n0\\n\\x12\\n1 −γ α\\n4β\\n\\x13T\\n.\\nThis completes the proof of all cases.\\n1That a0 ≤1 follows from Lemma 2.3. For t = 1, a1 ≤1\\n2 since a1 ≤a0(1 −a0) and\\n0 ≤a0 ≤1. For the induction step, at ≤at−1(1 −at−1) ≤1\\nt (1 −1\\nt ) = t−1\\nt2 =\\n1\\nt+1( t2−1\\nt2 ) ≤\\n1\\nt+1.\\n2This assumes T is even. T odd leads to the same constants.\\n\\n2.4. EXERCISES\\n21\\n2.4\\nExercises\\n1. Write an explicit expression for the gradient and projection operation\\n(if needed) for each of the example optimization problems in the ﬁrst\\nchapter.\\n2. Prove that a diﬀerentiable function f(x) : R →R is convex if and only\\nif for any x, y ∈R it holds that f(x) −f(y) ≤(x −y)f′(x).\\n3. Recall that we say that a function f : Rn →R has a condition number\\nγ = α/β over K ⊆Rd if the following two inequalities hold for all\\nx, y ∈K:\\n(a) f(y) ≥f(x) + (y −x)⊤∇f(x) + α\\n2 ∥x −y∥2\\n(b) f(y) ≤f(x) + (y −x)⊤∇f(x) + β\\n2 ∥x −y∥2\\nFor matrices A, B ∈Rn×n we denote A ≽B if A −B is positive\\nsemideﬁnite. Prove that if f is twice diﬀerentiable and it holds that\\nβI ≽∇2f(x) ≽αI for any x ∈K, then the condition number of f\\nover K is α/β.\\n4. Prove:\\n(a) The sum of convex functions is convex.\\n(b) Let f be α1-strongly convex and g be α2-strongly convex. Then\\nf + g is (α1 + α2)-strongly convex.\\n(c) Let f be β1-smooth and g be β2-smooth. Then f +g is (β1 +β2)-\\nsmooth.\\n5. Let K ⊆Rd be closed, compact, non-empty and bounded. Prove that\\na necessary and suﬃcient condition for ΠK(x) to be a singleton, that\\nis for | ΠK(x)| = 1, is for K to be convex.\\n6. Prove that for convex functions, ∇f(x) ∈∂f(x), that is, the gradient\\nbelongs to the subgradient set.\\n7. Let f(x) : Rn →R be a convex diﬀerentiable function and K ⊆Rn be\\na convex set. Prove that x⋆∈K is a minimizer of f over K if and only\\nif for any y ∈K it holds that (y −x⋆)⊤∇f(x⋆) ≥0.\\n8. Consider the n-dimensional simplex\\n∆n = {x ∈Rn |\\nn\\nX\\ni=1\\nxi = 1, xi ≥0 , ∀i ∈[n]}.\\n\\n22\\nCHAPTER 2. BASIC CONCEPTS\\nGive an algorithm for computing the projection of a point x ∈Rn onto\\nthe set ∆n (a near-linear time algorithm exists).\\n\\n2.5. BIBLIOGRAPHIC REMARKS\\n23\\n2.5\\nBibliographic remarks\\nThe reader is referred to dedicated books on convex optimization for much\\nmore in-depth treatment of the topics surveyed in this background chapter.\\nFor background in convex analysis see the texts [11, 68]. The classic text-\\nbook [12] gives a broad introduction to convex optimization with numerous\\napplications. For an adaptive analysis of gradient descent with the Polyak\\nstepsize see [33].\\n\\n24\\nCHAPTER 2. BASIC CONCEPTS\\n\\nChapter 3\\nStochastic Gradient Descent\\nThe most important optimization algorithm in the context of machine learn-\\ning is stochastic gradient descent (SGD), especially for non-convex optimiza-\\ntion and in the context of deep neural networks. In this chapter we spell\\nout the algorithm and analyze it up to tight ﬁnite-time convergence rates.\\n3.1\\nTraining feedforward neural networks\\nPerhaps the most common optimization problem in machine learning is that\\nof training feedforward neural networks. In this problem, we are given a set\\nof labelled data points, such as labelled images or text. Let {xi, yi} be the\\nset of labelled data points, also called the training data.\\nThe goal is to ﬁt the weights of an artiﬁcial neural network in order to\\nminimize the loss over the data. Mathematically, the feedforward network\\nis a given weighted a-cyclic graph G = (V, E, W). Each node v is assigned\\nan activation function, which we assume is the same function for all nodes,\\ndenoted σ : Rd 7→R. Using a biological analogy, an activation function σ\\nis a function that determines how strongly a neuron (i.e. a node) ‘ﬁres’ for\\na given input by mapping the result into the desired range, usually [0, 1] or\\n[−1, 1] . Some popular examples include:\\n• Sigmoid: σ(x) =\\n1\\n1+e−x\\n• Hyperbolic tangent: tanh(x) = ex−e−x\\nex+e−x\\n• Rectiﬁed linear unit: ReLU(x) = max{0, x} (currently the most widely\\nused of the three)\\n25\\n\\n26\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nThe inputs to the input layer nodes is a given data point, while the\\ninputs to to all other nodes are the output of the nodes connected to it. We\\ndenote by ρ(v) the set of input neighbors to node v. The top node output\\nis the input to the loss function, which takes its “prediction” and the true\\nlabel to form a loss.\\nFor an input node v, its output as a function of the graph weights and\\ninput example x (of dimension d), which we denote as\\nv(W, x) = σ\\n X\\ni∈d\\nWv,ixi\\n!\\nThe output of an internal node v is a function of its inputs u ∈ρ(v) and a\\ngiven example x, which we denote as\\nv(W, x) = σ\\n\\uf8eb\\n\\uf8edX\\nu∈ρ(v)\\nWuvu(W, x)\\n\\uf8f6\\n\\uf8f8\\nIf we denote the top node as v1, then the loss of the network over data point\\n(xi, yi) is given by\\nℓ(v1(W, xi), yi).\\nThe objective function becomes\\nf(W) = E\\nxi,yi\\n\\x02\\nℓ(v1(W, xi), yi)\\n\\x03\\nFor most commonly-used activation and loss functions, the above func-\\ntion is non-convex. However, it admits important computational properties.\\nThe most signiﬁcant property is given in the following lemma.\\nLemma 3.1 (Backpropagation lemma). The gradient of f can be computed\\nin time O(|E|).\\nThe proof of this lemma is left as an exercise, but we sketch the main\\nideas. For every variable Wuv, we have by linearity of expectation that\\n∂\\n∂Wuv\\nf(W) = E\\nxi,yi\\n\\x14\\n∂\\n∂Wuv\\nℓ(v1(W, xi), yi)\\n\\x15\\n.\\nNext, using the chain rule, we claim that it suﬃces to know the partial\\nderivatives of each node w.r.t. its immediate daughters. To see this, let us\\n\\n3.2. GRADIENT DESCENT FOR SMOOTH OPTIMIZATION\\n27\\nwrite the derivative w.r.t. Wuv using the chain rule:\\n∂\\n∂Wuv\\nℓ(v1(W, xi), yi) = ∂ℓ\\n∂v1 · ∂v1\\n∂Wuv\\n= ∂ℓ\\n∂v1 ·\\nX\\nv2∈ρ(v1)\\n∂v1\\n∂v2 ·\\n∂vj\\n∂Wuv\\n= ...\\n= ∂ℓ\\n∂v1 ·\\nX\\nv2∈ρ(v1)\\n∂v1\\n∂v2 · ... ·\\nX\\nvk\\nj ∈ρ(vk−1)\\n· ∂vk\\n∂Wuv\\nWe conclude that we only need to obtain the E partial derivatives along\\nthe edges in order to compute all partial derivatives of the function. The\\nactual product at each node can be computed by a dynamic program in\\nlinear time.\\n3.2\\nGradient descent for smooth optimization\\nBefore moving to stochastic gradient descent, we consider its determinis-\\ntic counterpart:\\ngradient descent, in the context of smooth non-convex\\noptimization.\\nOur notion of solution is a point with small gradient, i.e.\\n∥∇f(x)∥≤ε.\\nAs we prove below, this requires O( 1\\nε2 ) iterations, each requiring one gra-\\ndient computation. Recall that gradients can be computed eﬃciently, linear\\nin the number of edges, in feed forward neural networks. Thus, the time to\\nobtain a ε-approximate solution becomes O( |E|m\\nε2 ) for neural networks with\\nE edges and over m examples.\\nAlgorithm 2 Gradient descent\\n1: Input: f, T, initial point x1 ∈K, sequence of step sizes {ηt}\\n2: for t = 1 to T do\\n3:\\nLet yt+1 = xt −ηt∇f(xt), xt+1 = ΠK (yt+1)\\n4: end for\\n5: return xT+1\\nAlthough the choice of ηt can make a diﬀerence in practice, in theory\\nthe convergence of the vanilla GD algorithm is well understood and given in\\nthe following theorem. Below we assume that the function is bounded such\\nthat |f(x)| ≤M.\\n\\n28\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nTheorem 3.2. For unconstrained minimization of β-smooth functions and\\nηt = 1\\nβ, GD Algorithm 2 converges as\\n1\\nT\\nX\\nt\\n∥∇t∥2 ≤4Mβ\\nT\\n.\\nProof. Denote by ∇t the shorthand for ∇f(xt), and ht = f(xt) −f(x∗).\\nThe Descent Lemma is given in the following simple equation,\\nht+1 −ht = f(xt+1) −f(xt)\\n≤∇⊤\\nt (xt+1 −xt) + β\\n2 ∥xt+1 −xt∥2\\nβ-smoothness\\n= −ηt∥∇t∥2 + β\\n2 η2\\nt ∥∇t∥2\\nalgorithm defn.\\n= −1\\n2β ∥∇t∥2\\nchoice of ηt = 1\\nβ\\nThus, summing up over T iterations, we have\\n1\\n2β\\nT\\nX\\nt=1\\n∥∇t∥2 ≤\\nX\\nt\\n(ht −ht+1) = h1 −hT+1 ≤2M\\nFor convex functions, the above theorem implies convergence in function\\nvalue due to the following lemma,\\nLemma 3.3. A convex function satisﬁes\\nht ≤D∥∇t∥,\\nand an α-strongly convex function satisﬁes\\nht ≤1\\n2α∥∇t∥2.\\nProof. The gradient upper bound for convex functions gives\\nht ≤∇t(x∗−xt) ≤D∥∇t∥\\nThe strongly convex case appears in Lemma 2.3.\\n\\n3.3. STOCHASTIC GRADIENT DESCENT\\n29\\n3.3\\nStochastic gradient descent\\nIn the context of training feed forward neural networks, the key idea of\\nStochastic Gradient Descent is to modify the updates to be:\\nWt+1 = Wt −η e\\n∇t\\n(3.1)\\nwhere e\\n∇t is a random variable with E[e\\n∇t] = ∇f (Wt) and bounded second\\nmoment E[∥e\\n∇t∥2\\n2] ≤σ2.\\nLuckily, getting the desired e\\n∇t random variable is easy in the posed\\nproblem since the objective function is already in expectation form so:\\n∇f(W) = ∇E\\nxi,yi[ℓ(v1(W, xi), yi)] = E\\nxi,yi[∇ℓ(v1(W, xi), yi)].\\nTherefore, at iteration t we can take e\\n∇t = ∇ℓ(v1(W, xi), yi) where i ∈\\n{1, ..., m} is picked uniformly at random. Based on the observation above,\\nchoosing e\\n∇t this way preserves the desired expectation. So, for each iteration\\nwe only compute the gradient w.r.t. to one random example instead of the\\nentire dataset, thereby drastically improving performance for every step. It\\nremains to analyze how this impacts convergence.\\nAlgorithm 3 Stochastic gradient descent\\n1: Input: f, T, initial point x1 ∈K, sequence of step sizes {ηt}\\n2: for t = 1 to T do\\n3:\\nLet yt+1 = xt −ηt∇f(xt), xt+1 = ΠK (yt+1)\\n4: end for\\n5: return xT+1\\nTheorem 3.4. For unconstrained minimization of β-smooth functions and\\nηt = η =\\nq\\nM\\nβσ2T , SGD Algorithm 3 converges as\\nE\\n\"\\n1\\nT\\nX\\nt\\n∥∇t∥2\\n#\\n≤2\\nr\\nMβσ2\\nT\\n.\\n\\n30\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\nProof. Denote by ∇t the shorthand for ∇f(xt), and ht = f(xt) −f(x∗).\\nThe stochastic descent lemma is given in the following equation,\\nE[ht+1 −ht] = E[f(xt+1) −f(xt)]\\n≤E[∇⊤\\nt (xt+1 −xt) + β\\n2 ∥xt+1 −xt∥2]\\nβ-smoothness\\n= −E[η∇⊤\\nt ˜\\n∇t] + β\\n2 η2 E ∥˜\\n∇t∥2\\nalgorithm defn.\\n= −η∥∇t∥2 + β\\n2 η2σ2\\nvariance bound.\\nThus, summing up over T iterations, we have for η =\\nq\\nM\\nβσ2T ,\\nE\\n\"\\n1\\nT\\nT\\nX\\nt=1\\n∥∇t∥2\\n#\\n≤\\n1\\nTη\\nP\\nt E [ht −ht+1] + η β\\n2 σ2 ≤M\\nTη + η β\\n2 σ2\\n=\\nq\\nMβσ2\\nT\\n+ 1\\n2\\nq\\nMβσ2\\nT\\n≤2\\nq\\nMβσ2\\nT\\n.\\nWe thus conclude that O( 1\\nε4 ) iterations are needed to ﬁnd a point with\\n∥∇f(x)∥≤ε, as opposed to O( 1\\nε2 ). However, each iteration takes O(|E|)\\ntime, instead of O(|E|m) time for gradient descent.\\nThis is why SGD is one of the most useful algorithms in machine learning.\\n\\n3.4. BIBLIOGRAPHIC REMARKS\\n31\\n3.4\\nBibliographic remarks\\nFor in depth treatment of backpropagation and the role of deep neural net-\\nworks in machine learning the reader is referred to [25].\\nFor detailed rigorous convergence proofs of ﬁrst order methods, see lec-\\nture notes by Nesterov [57] and Nemirovskii [53, 54], as well as the recent\\ntext [13].\\n\\n32\\nCHAPTER 3. STOCHASTIC GRADIENT DESCENT\\n\\nChapter 4\\nGeneralization and\\nNon-Smooth Optimization\\nIn previous chapter we have introduced the framework of mathematical op-\\ntimization within the context of machine learning. We have described the\\nmathematical formulation of several machine learning problems, notably\\ntraining neural networks, as optimization problems. We then described as\\nwell as analyzed the most useful optimization method to solve such formu-\\nlations: stochastic gradient descent.\\nHowever, several important questions arise:\\n1. SGD was analyzed for smooth functions. Can we minimize non-smooth\\nobjectives?\\n2. Given an ERM problem (a.k.a. learning from examples, see ﬁrst chap-\\nter), what can we say about generalization to unseen examples? How\\ndoes it aﬀect optimization?\\n3. Are there faster algorithms than SGD in the context of ML?\\nIn this chapter we address the ﬁrst two, and devote the rest of this\\nmanuscript/course to the last question.\\nHow many examples are needed to learn a certain concept? This is a\\nfundamental question of statistical/computational learning theory that has\\nbeen studied for decades (see end of chapter for bibliographic references).\\nThe classical setting of learning from examples is statistical. It assumes\\nexamples are drawn i.i.d from a ﬁxed, arbitrary and unknown distribution.\\nThe mathematical optimization formulations that we have derived for the\\nERM problem assume that we have suﬃciently many examples, such that\\n33\\n\\n34\\nCHAPTER 4. GENERALIZATION\\noptimizing a certain predictor/neural-network/machine on them will result\\nin a solution that is capable of generalizing to unseen examples. The number\\nof examples needed to generalize is called the sample complexity of the prob-\\nlem, and it depends on the concept we are learning as well as the hypothesis\\nclass over which we are trying to optimize.\\nThere are dimensionality notions in the literature, notably the VC-\\ndimension and related notions, that give precise bounds on the sample com-\\nplexity for various hypothesis classes. In this text we take an algorithmic\\napproach, which is also deterministic. Instead of studying sample complex-\\nity, which is non-algorithmic, we study algorithms for regret minimization.\\nWe will show that they imply generalization for a broad class of machines.\\n4.1\\nA note on non-smooth optimization\\nMinimization of a function that is both non-convex and non-smooth is in\\ngeneral hopeless, from an information theoretic perspective. The following\\nimage explains why. The depicted function on the interval [0, 1] has a single\\nlocal/global minimum, and if the crevasse is narrow enough, it cannot be\\nfound by any method other than extensive brute-force search, which can\\ntake arbitrarily long.\\n0\\n1\\nFigure 4.1: Intractability of nonsmooth optimization\\nSince non-convex and non-smooth optimization is hopeless, in the con-\\ntext of non-smooth functions we only consider convex optimization.\\n\\n4.2. MINIMIZING REGRET\\n35\\n4.2\\nMinimizing Regret\\nThe setting we consider for the rest of this chapter is that of online (convex)\\noptimization. In this setting a learner iteratively predicts a point xt ∈K in\\na convex set K ⊆Rd, and then receives a cost according to an adversarially\\nchosen convex function ft ∈F from family F.\\nThe goal of the algorithms introduced in this chapter is to minimize\\nworst-case regret, or diﬀerence between total cost and that of best point in\\nhindsight:\\nregret =\\nsup\\nf1,...,fT ∈F\\n( T\\nX\\nt=1\\nft(xt) −min\\nx∈K\\nT\\nX\\nt=1\\nft(x)\\n)\\n.\\nIn order to compare regret to optimization error it is useful to consider\\nthe average regret, or regret/T. Let ¯\\nxT = 1\\nT\\nPT\\nt=1 xt be the average decision.\\nIf the functions ft are all equal to a single function f : K 7→R, then Jensen’s\\ninequality implies that f(¯\\nxT ) converges to f(x⋆) if the average regret is\\nvanishing, since\\nf(¯\\nxT ) −f(x⋆) ≤1\\nT\\nT\\nX\\nt=1\\n[f(xt) −f(x⋆)] = regret\\nT\\n4.3\\nRegret implies generalization\\nStatistical learning theory for learning from examples postulates that exam-\\nples from a certain concept are sampled i.i.d. from a ﬁxed and unknown\\ndistribution. The learners’ goal is to choose a hypothesis from a certain\\nhypothesis class that can generalize to unseen examples.\\nMore formally, let D be a distribution over labelled examples {ai ∈\\nRd, bi ∈R} ∼D. Let H = {x} , x : Rd 7→R be a hypothsis class over which\\nwe are trying to learn (such as linear separators, deep neural networks,\\netc.). The generalization error of a hypothesis is the expected error of a\\nhypothesis over randomly chosen examples according to a given loss function\\nℓ: R × R 7→R, which is applied to the prediction of the hypothesis and the\\ntrue label, ℓ(x(ai), bi). Thus,\\nerror(x) =\\nE\\nai,bi∼D[ℓ(x(ai), bi)].\\nAn algorithm that attains sublinear regret over the hypothesis class H,\\nw.r.t. loss functions given by ft(x) = fa,b(x) = ℓ(x(a), b), gives rise to a\\ngeneralizing hypothesis as follows.\\n\\n36\\nCHAPTER 4. GENERALIZATION\\nLemma 4.1. Let ¯\\nx = xt for t ∈[T] be chose uniformly at random from\\n{x1, ..., xT }.Then, with expectation taken over random choice of ¯\\nx as well\\nas choices of ft ∼D,\\nE[error(¯\\nx)] ≤E[error(x∗)] + regret\\nT\\nProof. By random choice of ¯\\nx, we have\\nE[f(¯\\nx)] = E\\n\"\\n1\\nT\\nX\\nt\\nf(xt)\\n#\\nUsing the fact that ft ∼D, we have\\nE[error(¯\\nx)]\\n= Ef∼D[f(¯\\nx)]\\n= Eft[ 1\\nT\\nP\\nt ft(xt)]\\n≤Eft[ 1\\nT\\nP\\nt ft(x⋆)] + regret\\nT\\n= Ef[f(x⋆)] + regret\\nT\\n= Ef[error(x⋆)] + regret\\nT\\n4.4\\nOnline gradient descent\\nPerhaps the simplest algorithm that applies to the most general setting of\\nonline convex optimization is online gradient descent. This algorithm is an\\nonline version of standard gradient descent for oﬄine optimization we have\\nseen in the previous chapter.\\nPseudo-code for the algorithm is given in\\nAlgorithm 4, and a conceptual illustration is given in Figure 4.2.\\nIn each iteration, the algorithm takes a step from the previous point in\\nthe direction of the gradient of the previous cost. This step may result in\\na point outside of the underlying convex set. In such cases, the algorithm\\nprojects the point back to the convex set, i.e. ﬁnds its closest point in the\\nconvex set. Despite the fact that the next cost function may be completely\\ndiﬀerent than the costs observed thus far, the regret attained by the algo-\\nrithm is sublinear. This is formalized in the following theorem (recall the\\ndeﬁnition of G and D from the previous chapter).\\nTheorem 4.2. Online gradient descent with step sizes {ηt =\\nD\\nG\\n√\\nt, t ∈[T]}\\nguarantees the following for all T ≥1:\\nregretT =\\nT\\nX\\nt=1\\nft(xt) −min\\nx⋆∈K\\nT\\nX\\nt=1\\nft(x⋆) ≤3GD\\n√\\nT\\n\\n4.4. ONLINE GRADIENT DESCENT\\n37\\nFigure 4.2: Online gradient descent: the iterate xt+1 is derived by advancing\\nxt in the direction of the current gradient ∇t, and projecting back into K.\\nAlgorithm 4 online gradient descent\\n1: Input: convex set K, T, x1 ∈K, step sizes {ηt}\\n2: for t = 1 to T do\\n3:\\nPlay xt and observe cost ft(xt).\\n4:\\nUpdate and project:\\nyt+1 = xt −ηt∇ft(xt)\\nxt+1 = Π\\nK(yt+1)\\n5: end for\\nProof. Let x⋆∈arg minx∈K\\nPT\\nt=1 ft(x). Deﬁne ∇t ≜∇ft(xt). By convexity\\nft(xt) −ft(x⋆) ≤∇⊤\\nt (xt −x⋆)\\n(4.1)\\nWe ﬁrst upper-bound ∇⊤\\nt (xt −x⋆) using the update rule for xt+1 and The-\\norem 2.1 (the Pythagorean theorem):\\n∥xt+1 −x⋆∥2 =\\n\\r\\n\\r\\n\\r\\n\\rΠ\\nK(xt −ηt∇t) −x⋆\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n≤∥xt −ηt∇t −x⋆∥2\\n(4.2)\\n\\n38\\nCHAPTER 4. GENERALIZATION\\nHence,\\n∥xt+1 −x⋆∥2\\n≤\\n∥xt −x⋆∥2 + η2\\nt ∥∇t∥2 −2ηt∇⊤\\nt (xt −x⋆)\\n2∇⊤\\nt (xt −x⋆)\\n≤\\n∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\\nηt\\n+ ηtG2\\n(4.3)\\nSumming (4.1) and (4.3) from t = 1 to T, and setting ηt =\\nD\\nG\\n√\\nt (with\\n1\\nη0 ≜0):\\n2\\n T\\nX\\nt=1\\nft(xt) −ft(x⋆)\\n!\\n≤2\\nT\\nX\\nt=1\\n∇⊤\\nt (xt −x⋆)\\n≤\\nT\\nX\\nt=1\\n∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\\nηt\\n+ G2\\nT\\nX\\nt=1\\nηt\\n≤\\nT\\nX\\nt=1\\n∥xt −x⋆∥2\\n\\x12 1\\nηt\\n−\\n1\\nηt−1\\n\\x13\\n+ G2\\nT\\nX\\nt=1\\nηt\\n1\\nη0\\n≜0,\\n∥xT+1 −x∗∥2 ≥0\\n≤D2\\nT\\nX\\nt=1\\n\\x12 1\\nηt\\n−\\n1\\nηt−1\\n\\x13\\n+ G2\\nT\\nX\\nt=1\\nηt\\n≤D2 1\\nηT\\n+ G2\\nT\\nX\\nt=1\\nηt\\ntelescoping series\\n≤3DG\\n√\\nT.\\nThe last inequality follows since ηt =\\nD\\nG\\n√\\nt and PT\\nt=1\\n1\\n√\\nt ≤2\\n√\\nT.\\nThe online gradient descent algorithm is straightforward to implement,\\nand updates take linear time given the gradient. However, there is a projec-\\ntion step which may take signiﬁcantly longer.\\n4.5\\nLower bounds\\nTheorem 4.3. Any algorithm for online convex optimization incurs Ω(DG\\n√\\nT)\\nregret in the worst case. This is true even if the cost functions are generated\\nfrom a ﬁxed stationary distribution.\\nWe give a sketch of the proof; ﬁlling in all details is left as an exercise\\nat the end of this chapter.\\n\\n4.6. ONLINE GRADIENT DESCENT FOR STRONGLY CONVEX FUNCTIONS39\\nConsider an instance of OCO where the convex set K is the n-dimensional\\nhypercube, i.e.\\nK = {x ∈Rn , ∥x∥∞≤1}.\\nThere are 2n linear cost functions, one for each vertex v ∈{±1}n, deﬁned\\nas\\n∀v ∈{±1}n , fv(x) = v⊤x.\\nNotice that both the diameter of K and the bound on the norm of the cost\\nfunction gradients, denoted G, are bounded by\\nD ≤\\nv\\nu\\nu\\nt\\nn\\nX\\ni=1\\n22 = 2√n, G =\\nv\\nu\\nu\\nt\\nn\\nX\\ni=1\\n(±1)2 = √n\\nThe cost functions in each iteration are chosen at random, with uniform\\nprobability, from the set {fv, v ∈{±1}n}. Denote by vt ∈{±1}n the vertex\\nchosen in iteration t, and denote ft = fvt. By uniformity and independence,\\nfor any t and xt chosen online, Evt[ft(xt)] = Evt[v⊤\\nt xt] = 0. However,\\nE\\nv1,...,vT\\n\"\\nmin\\nx∈K\\nT\\nX\\nt=1\\nft(x)\\n#\\n= E\\n\\uf8ee\\n\\uf8f0min\\nx∈K\\nX\\ni∈[n]\\nT\\nX\\nt=1\\nvt(i) · xi\\n\\uf8f9\\n\\uf8fb\\n= n E\\n\"\\n−\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nT\\nX\\nt=1\\nvt(1)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n#\\ni.i.d. coordinates\\n= −Ω(n\\n√\\nT).\\nThe last equality is left as exercise 3.\\nThe facts above nearly complete the proof of Theorem 4.3; see the exer-\\ncises at the end of this chapter.\\n4.6\\nOnline gradient descent for strongly convex\\nfunctions\\nThe ﬁrst algorithm that achieves regret logarithmic in the number of iter-\\nations is a twist on the online gradient descent algorithm, changing only\\nthe step size. The following theorem establishes logarithmic bounds on the\\nregret if the cost functions are strongly convex.\\n\\n40\\nCHAPTER 4. GENERALIZATION\\nTheorem 4.4. For α-strongly convex loss functions, online gradient descent\\nwith step sizes ηt = 1\\nαt achieves the following guarantee for all T ≥1\\nregretT ≤\\nT\\nX\\nt=1\\n1\\nαt∥∇t∥2 ≤G2\\n2α (1 + log T).\\nProof. Let x⋆∈arg minx∈K\\nPT\\nt=1 ft(x). Recall the deﬁnition of regret\\nregretT =\\nT\\nX\\nt=1\\nft(xt) −\\nT\\nX\\nt=1\\nft(x⋆).\\nDeﬁne ∇t ≜∇ft(xt). Applying the deﬁnition of α-strong convexity to\\nthe pair of points xt,x∗, we have\\n2(ft(xt) −ft(x⋆))\\n≤\\n2∇⊤\\nt (xt −x⋆) −α∥x⋆−xt∥2.\\n(4.4)\\nWe proceed to upper-bound ∇⊤\\nt (xt −x⋆). Using the update rule for xt+1\\nand the Pythagorean theorem 2.1, we get\\n∥xt+1 −x⋆∥2 = ∥Π\\nK(xt −ηt∇t) −x⋆∥2 ≤∥xt −ηt∇t −x⋆∥2.\\nHence,\\n∥xt+1 −x⋆∥2\\n≤\\n∥xt −x⋆∥2 + η2\\nt ∥∇t∥2 −2ηt∇⊤\\nt (xt −x⋆)\\nand\\n2∇⊤\\nt (xt −x⋆)\\n≤\\n∥xt −x⋆∥2 −∥xt+1 −x⋆∥2\\nηt\\n+ ηt∥∇t∥2.\\n(4.5)\\n\\n4.7. ONLINE GRADIENT DESCENT IMPLIES SGD\\n41\\nSumming (4.5) from t = 1 to T, setting ηt =\\n1\\nαt (deﬁne\\n1\\nη0 ≜0), and\\ncombining with (4.4), we have:\\n2\\nT\\nX\\nt=1\\n(ft(xt) −ft(x⋆))\\n≤\\nT\\nX\\nt=1\\n∥xt −x⋆∥2\\n\\x12 1\\nηt\\n−\\n1\\nηt−1\\n−α\\n\\x13\\n+\\nT\\nX\\nt=1\\nηt∥∇t∥2\\nsince 1\\nη0\\n≜0, ∥xT+1 −x∗∥2 ≥0\\n=\\n0 +\\nT\\nX\\nt=1\\n1\\nαt∥∇t∥2\\n≤\\nG2\\nα (1 + log T)\\n4.7\\nOnline Gradient Descent implies SGD\\nIn this section we notice that OGD and its regret bounds imply the SGD\\nbounds we have studied in the previous chapter. The main advantage are\\nthe guarantees for non-smooth stochastic optimization, and constrained op-\\ntimization.\\nRecall that in stochastic optimization, the optimizer attempts to mini-\\nmize a convex function over a convex domain as given by the mathematical\\nprogram:\\nmin\\nx∈K f(x).\\nHowever, unlike standard oﬄine optimization, the optimizer is given access\\nto a noisy gradient oracle, deﬁned by\\nO(x) ≜˜\\n∇x s.t.\\nE[ ˜\\n∇x] = ∇f(x) , E[∥˜\\n∇x∥2] ≤G2\\nThat is, given a point in the decision set, a noisy gradient oracle returns\\na random vector whose expectation is the gradient at the point and whose\\nsecond moment is bounded by G2.\\nWe will show that regret bounds for OCO translate to convergence rates\\nfor stochastic optimization. As a special case, consider the online gradient\\n\\n42\\nCHAPTER 4. GENERALIZATION\\ndescent algorithm whose regret is bounded by\\nregretT = O(DG\\n√\\nT)\\nApplying the OGD algorithm over a sequence of linear functions that are\\ndeﬁned by the noisy gradient oracle at consecutive points, and ﬁnally re-\\nturning the average of all points along the way, we obtain the stochastic\\ngradient descent algorithm, presented in Algorithm 5.\\nAlgorithm 5 stochastic gradient descent\\n1: Input: f, K, T, x1 ∈K, step sizes {ηt}\\n2: for t = 1 to T do\\n3:\\nLet ˜\\n∇t = O(xt) and deﬁne: ft(x) ≜⟨˜\\n∇t, x⟩\\n4:\\nUpdate and project:\\nyt+1 = xt −ηt ˜\\n∇t\\nxt+1 = Π\\nK(yt+1)\\n5: end for\\n6: return ¯\\nxT ≜1\\nT\\nPT\\nt=1 xt\\nTheorem 4.5. Algorithm 5 with step sizes ηt =\\nD\\nG\\n√\\nt guarantees\\nE[f(¯\\nxT )] ≤min\\nx⋆∈K f(x⋆) + 3GD\\n√\\nT\\nProof. By the regret guarantee of OGD, we have\\nE[f(¯\\nxT )] −f(x⋆)\\n≤E[ 1\\nT\\nX\\nt\\nf(xt)] −f(x⋆)\\nconvexity of f (Jensen)\\n≤1\\nT E[\\nX\\nt\\n⟨∇f(xt), xt −x⋆⟩]\\nconvexity again\\n= 1\\nT E[\\nX\\nt\\n⟨˜\\n∇t, xt −x⋆⟩]\\nnoisy gradient estimator\\n= 1\\nT E[\\nX\\nt\\nft(xt) −ft(x⋆)]\\nAlgorithm 5, line (3)\\n≤regretT\\nT\\ndeﬁnition\\n≤3GD\\n√\\nT\\ntheorem 4.2\\n\\n4.7. ONLINE GRADIENT DESCENT IMPLIES SGD\\n43\\nIt is important to note that in the proof above, we have used the fact\\nthat the regret bounds of online gradient descent hold against an adaptive\\nadversary. This need arises since the cost functions ft deﬁned in Algorithm\\n5 depend on the choice of decision xt ∈K.\\nIn addition, the careful reader may notice that by plugging in diﬀerent\\nstep sizes (also called learning rates) and applying SGD to strongly convex\\nfunctions, one can attain ˜\\nO(1/T) convergence rates. Details of this deriva-\\ntion are left as exercise 1.\\n\\n44\\nCHAPTER 4. GENERALIZATION\\n4.8\\nExercises\\n1. Prove that SGD for a strongly convex function can, with appropriate\\nparameters ηt, converge as ˜\\nO( 1\\nT ). You may assume that the gradient\\nestimators have Euclidean norms bounded by the constant G.\\n2. Design an OCO algorithm that attains the same asymptotic regret\\nbound as OGD, up to factors logarithmic in G and D, without knowing\\nthe parameters G and D ahead of time.\\n3. In this exercise we prove a tight lower bound on the regret of any\\nalgorithm for online convex optimization.\\n(a) For any sequence of T fair coin tosses, let Nh be the number of\\nhead outcomes and Nt be the number of tails. Give an asymp-\\ntotically tight upper and lower bound on E[|Nh −Nt|] (i.e., order\\nof growth of this random variable as a function of T, up to mul-\\ntiplicative and additive constants).\\n(b) Consider a 2-expert problem, in which the losses are inversely\\ncorrelated: either expert one incurs a loss of one and the second\\nexpert zero, or vice versa. Use the fact above to design a set-\\nting in which any experts algorithm incurs regret asymptotically\\nmatching the upper bound.\\n(c) Consider the general OCO setting over a convex set K. Design\\na setting in which the cost functions have gradients whose norm\\nis bounded by G, and obtain a lower bound on the regret as\\na function of G, the diameter of K, and the number of game\\niterations.\\n\\n4.9. BIBLIOGRAPHIC REMARKS\\n45\\n4.9\\nBibliographic remarks\\nThe OCO framework was introduced by Zinkevich in [87], where the OGD\\nalgorithm was introduced and analyzed. Precursors to this algorithm, albeit\\nfor less general settings, were introduced and analyzed in [47]. Logarithmic\\nregret algorithms for Online Convex Optimization were introduced and an-\\nalyzed in [32]. For more detailed exposition on this prediction framework\\nand its applications see [31].\\nThe SGD algorithm dates back to Robbins and Monro [67]. Application\\nof SGD to soft-margin SVM training was explored in [74]. Tight conver-\\ngence rates of SGD for strongly convex and non-smooth functions were only\\nrecently obtained in [35],[62],[76].\\n\\n46\\nCHAPTER 4. GENERALIZATION\\n\\nChapter 5\\nRegularization\\nIn this chapter we consider a generalization of the gradient descent called\\nby diﬀerent names in diﬀerent communities (such as mirrored-descent, or\\nregularized-follow-the-leader). The common theme of this generalization is\\ncalled Regularization, a concept that is founded in generalization theory.\\nSince this course focuses on optimization rather than generalization, we\\nshall refer the reader to the generalization aspect of regularization, and\\nfocus hereby on optimization algorithms.\\nWe start by motivating this general family of methods using the funda-\\nmental problem of decision theory.\\n5.1\\nMotivation: prediction from expert advice\\nConsider the following fundamental iterative decision making problem:\\nAt each time step t = 1, 2, . . . , T, the decision maker faces a choice\\nbetween two actions A or B (i.e., buy or sell a certain stock). The decision\\nmaker has assistance in the form of N “experts” that oﬀer their advice. After\\na choice between the two actions has been made, the decision maker receives\\nfeedback in the form of a loss associated with each decision. For simplicity\\none of the actions receives a loss of zero (i.e., the “correct” decision) and\\nthe other a loss of one.\\nWe make the following elementary observations:\\n1. A decision maker that chooses an action uniformly at random each\\niteration, trivially attains a loss of T\\n2 and is “correct” 50% of the time.\\n47\\n\\n48\\nCHAPTER 5. REGULARIZATION\\n2. In terms of the number of mistakes, no algorithm can do better in the\\nworst case! In a later exercise, we will devise a randomized setting in\\nwhich the expected number of mistakes of any algorithm is at least T\\n2 .\\nWe are thus motivated to consider a relative performance metric: can\\nthe decision maker make as few mistakes as the best expert in hindsight?\\nThe next theorem shows that the answer in the worst case is negative for a\\ndeterministic decision maker.\\nTheorem 5.1. Let L ≤T\\n2 denote the number of mistakes made by the best\\nexpert in hindsight. Then there does not exist a deterministic algorithm that\\ncan guarantee less than 2L mistakes.\\nProof. Assume that there are only two experts and one always chooses op-\\ntion A while the other always chooses option B. Consider the setting in\\nwhich an adversary always chooses the opposite of our prediction (she can\\ndo so, since our algorithm is deterministic). Then, the total number of mis-\\ntakes the algorithm makes is T. However, the best expert makes no more\\nthan T\\n2 mistakes (at every iteration exactly one of the two experts is mis-\\ntaken). Therefore, there is no algorithm that can always guarantee less than\\n2L mistakes.\\nThis observation motivates the design of random decision making algo-\\nrithms, and indeed, the OCO framework gracefully models decisions on a\\ncontinuous probability space. Henceforth we prove Lemmas 5.3 and 5.4 that\\nshow the following:\\nTheorem 5.2. Let ε ∈(0, 1\\n2). Suppose the best expert makes L mistakes.\\nThen:\\n1. There is an eﬃcient deterministic algorithm that can guarantee less\\nthan 2(1 + ε)L + 2 log N\\nε\\nmistakes;\\n2. There is an eﬃcient randomized algorithm for which the expected num-\\nber of mistakes is at most (1 + ε)L + log N\\nε\\n.\\n\\n5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE\\n49\\n5.1.1\\nThe weighted majority algorithm\\nThe weighted majority (WM) algorithm is intuitive to describe: each expert\\ni is assigned a weight Wt(i) at every iteration t. Initially, we set W1(i) = 1\\nfor all experts i ∈[N]. For all t ∈[T] let St(A), St(B) ⊆[N] be the set of\\nexperts that choose A (and respectively B) at time t. Deﬁne,\\nWt(A) =\\nX\\ni∈St(A)\\nWt(i)\\nWt(B) =\\nX\\ni∈St(B)\\nWt(i)\\nand predict according to\\nat =\\n(\\nA\\nif Wt(A) ≥Wt(B)\\nB\\notherwise.\\nNext, update the weights Wt(i) as follows:\\nWt+1(i) =\\n(\\nWt(i)\\nif expert i was correct\\nWt(i)(1 −ε)\\nif expert i was wrong ,\\nwhere ε is a parameter of the algorithm that will aﬀect its performance.\\nThis concludes the description of the WM algorithm. We proceed to bound\\nthe number of mistakes it makes.\\nLemma 5.3. Denote by Mt the number of mistakes the algorithm makes\\nuntil time t, and by Mt(i) the number of mistakes made by expert i until\\ntime t. Then, for any expert i ∈[N] we have\\nMT ≤2(1 + ε)MT (i) + 2 log N\\nε\\n.\\nWe can optimize ε to minimize the above bound. The expression on the\\nright hand side is of the form f(x) = ax + b/x, that reaches its minimum\\nat x =\\np\\nb/a. Therefore the bound is minimized at ε⋆=\\np\\nlog N/MT (i).\\nUsing this optimal value of ε, we get that for the best expert i⋆\\nMT ≤2MT (i⋆) + O\\n\\x10p\\nMT (i⋆) log N\\n\\x11\\n.\\nOf course, this value of ε⋆cannot be used in advance since we do not know\\nwhich expert is the best one ahead of time (and therefore we do not know the\\nvalue of MT (i⋆)). However, we shall see later on that the same asymptotic\\nbound can be obtained even without this prior knowledge.\\nLet us now prove Lemma 5.3.\\n\\n50\\nCHAPTER 5. REGULARIZATION\\nProof. Let Φt = PN\\ni=1 Wt(i) for all t ∈[T], and note that Φ1 = N.\\nNotice that Φt+1 ≤Φt. However, on iterations in which the WM algo-\\nrithm erred, we have\\nΦt+1 ≤Φt(1 −ε\\n2),\\nthe reason being that experts with at least half of total weight were wrong\\n(else WM would not have erred), and therefore\\nΦt+1 ≤1\\n2Φt(1 −ε) + 1\\n2Φt = Φt(1 −ε\\n2).\\nFrom both observations,\\nΦt ≤Φ1(1 −ε\\n2)Mt = N(1 −ε\\n2)Mt.\\nOn the other hand, by deﬁnition we have for any expert i that\\nWT (i) = (1 −ε)MT (i).\\nSince the value of WT (i) is always less than the sum of all weights ΦT , we\\nconclude that\\n(1 −ε)MT (i) = WT (i) ≤ΦT ≤N(1 −ε\\n2)MT .\\nTaking the logarithm of both sides we get\\nMT (i) log(1 −ε) ≤log N + MT log (1 −ε\\n2).\\nNext, we use the approximations\\n−x −x2 ≤log (1 −x) ≤−x\\n0 < x < 1\\n2,\\nwhich follow from the Taylor series of the logarithm function, to obtain that\\n−MT (i)(ε + ε2) ≤log N −MT\\nε\\n2,\\nand the lemma follows.\\n\\n5.1. MOTIVATION: PREDICTION FROM EXPERT ADVICE\\n51\\n5.1.2\\nRandomized weighted majority\\nIn the randomized version of the WM algorithm, denoted RWM, we choose\\nexpert i w.p. pt(i) = Wt(i)/ PN\\nj=1 Wt(j) at time t.\\nLemma 5.4. Let Mt denote the number of mistakes made by RWM until\\niteration t. Then, for any expert i ∈[N] we have\\nE[MT ] ≤(1 + ε)MT (i) + log N\\nε\\n.\\nThe proof of this lemma is very similar to the previous one, where the factor\\nof two is saved by the use of randomness:\\nProof. As before, let Φt = PN\\ni=1 Wt(i) for all t ∈[T], and note that Φ1 = N.\\nLet ˜\\nmt = Mt −Mt−1 be the indicator variable that equals one if the RWM\\nalgorithm makes a mistake on iteration t. Let mt(i) equal one if the i’th\\nexpert makes a mistake on iteration t and zero otherwise. Inspecting the\\nsum of the weights:\\nΦt+1 =\\nX\\ni\\nWt(i)(1 −εmt(i))\\n= Φt(1 −ε\\nX\\ni\\npt(i)mt(i))\\npt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n= Φt(1 −ε E[ ˜\\nmt])\\n≤Φte−εE[ ˜\\nmt].\\n1 + x ≤ex\\nOn the other hand, by deﬁnition we have for any expert i that\\nWT (i) = (1 −ε)MT (i)\\nSince the value of WT (i) is always less than the sum of all weights ΦT , we\\nconclude that\\n(1 −ε)MT (i) = WT (i) ≤ΦT ≤Ne−εE[MT ].\\nTaking the logarithm of both sides we get\\nMT (i) log(1 −ε) ≤log N −ε E[MT ]\\nNext, we use the approximation\\n−x −x2 ≤log (1 −x) ≤−x\\n,\\n0 < x < 1\\n2\\n\\n52\\nCHAPTER 5. REGULARIZATION\\nto obtain\\n−MT (i)(ε + ε2) ≤log N −ε E[MT ],\\nand the lemma follows.\\n5.1.3\\nHedge\\nThe RWM algorithm is in fact more general: instead of considering a dis-\\ncrete number of mistakes, we can consider measuring the performance of an\\nexpert by a non-negative real number ℓt(i), which we refer to as the loss\\nof the expert i at iteration t. The randomized weighted majority algorithm\\nguarantees that a decision maker following its advice will incur an average\\nexpected loss approaching that of the best expert in hindsight.\\nHistorically, this was observed by a diﬀerent and closely related algorithm\\ncalled Hedge.\\nAlgorithm 6 Hedge\\n1: Initialize: ∀i ∈[N], W1(i) = 1\\n2: for t = 1 to T do\\n3:\\nPick it ∼R Wt, i.e., it = i with probability xt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n4:\\nIncur loss ℓt(it).\\n5:\\nUpdate weights Wt+1(i) = Wt(i)e−εℓt(i)\\n6: end for\\nHenceforth, denote in vector notation the expected loss of the algorithm\\nby\\nE[ℓt(it)] =\\nN\\nX\\ni=1\\nxt(i)ℓt(i) = x⊤\\nt ℓt\\nTheorem 5.5. Let ℓ2\\nt denote the N-dimensional vector of square losses,\\ni.e., ℓ2\\nt (i) = ℓt(i)2, let ε > 0, and assume all losses to be non-negative. The\\nHedge algorithm satisﬁes for any expert i⋆∈[N]:\\nT\\nX\\nt=1\\nx⊤\\nt ℓt ≤\\nT\\nX\\nt=1\\nℓt(i⋆) + ε\\nT\\nX\\nt=1\\nx⊤\\nt ℓ2\\nt + log N\\nε\\n\\n5.2. THE REGULARIZATION FRAMEWORK\\n53\\nProof. As before, let Φt = PN\\ni=1 Wt(i) for all t ∈[T], and note that Φ1 = N.\\nInspecting the sum of weights:\\nΦt+1\\n= P\\ni Wt(i)e−εℓt(i)\\n= Φt\\nP\\ni xt(i)e−εℓt(i)\\nxt(i) =\\nWt(i)\\nP\\nj Wt(j)\\n≤Φt\\nP\\ni xt(i)(1 −εℓt(i) + ε2ℓt(i)2))\\nfor x ≥0,\\ne−x ≤1 −x + x2\\n= Φt(1 −εx⊤\\nt ℓt + ε2x⊤\\nt ℓ2\\nt )\\n≤Φte−εx⊤\\nt ℓt+ε2x⊤\\nt ℓ2\\nt .\\n1 + x ≤ex\\nOn the other hand, by deﬁnition, for expert i⋆we have that\\nWT (i⋆) = e−ε PT\\nt=1 ℓt(i⋆)\\nSince the value of WT (i⋆) is always less than the sum of all weights Φt, we\\nconclude that\\nWT (i⋆) ≤ΦT ≤Ne−ε P\\nt x⊤\\nt ℓt+ε2 P\\nt x⊤\\nt ℓ2\\nt .\\nTaking the logarithm of both sides we get\\n−ε\\nT\\nX\\nt=1\\nℓt(i⋆) ≤log N −ε\\nT\\nX\\nt=1\\nx⊤\\nt ℓt + ε2\\nT\\nX\\nt=1\\nx⊤\\nt ℓ2\\nt\\nand the theorem follows by simplifying.\\n5.2\\nThe Regularization framework\\nIn the previous section we studied the multiplicative weights update method\\nfor decision making. A natural question is: couldn’t we have used online\\ngradient descent for the same exact purpose?\\nIndeed, the setting of prediction from expert advice naturally follows\\ninto the framework of online convex optimization. To see this, consider the\\nloss functions given by\\nft(x) = ℓ⊤\\nt x = E\\ni∼x[ℓt(i)],\\nwhich capture the expected loss of choosing an expert from distribution\\nx ∈∆n as a linear function.\\n\\n54\\nCHAPTER 5. REGULARIZATION\\nThe regret guarantees we have studied for OGD imply a regret of\\nO(GD\\n√\\nT) = O(\\n√\\nnT).\\nHere we have used the fact that the Eucliean diameter of the simplex is two,\\nand that the losses are bounded by one, hence the Euclidean norm of the\\ngradient vector ℓt is bounded by √n.\\nIn contrast, the Hedge algorithm attains regret of O(√T log n) for the\\nsame problem. How can we explain this discrepancy?!\\n5.2.1\\nThe RFTL algorithm\\nBoth OGD and Hedge are, in fact, instantiations of a more general meta-\\nalgorithm called RFTL (Regularized-Follow-The-Leader).\\nIn an OCO setting of regret minimization, the most straightforward\\napproach for the online player is to use at any time the optimal decision\\n(i.e., point in the convex set) in hindsight. Formally, let\\nxt+1 = arg min\\nx∈K\\nt\\nX\\nτ=1\\nfτ(x).\\nThis ﬂavor of strategy is known as “ﬁctitious play” in economics, and has\\nbeen named “Follow the Leader” (FTL) in machine learning. It is not hard\\nto see that this simple strategy fails miserably in a worst-case sense. That\\nis, this strategy’s regret can be linear in the number of iterations, as the\\nfollowing example shows: Consider K = [−1, 1], let f1(x) = 1\\n2x, and let fτ\\nfor τ = 2, . . . , T alternate between −x or x. Thus,\\nt\\nX\\nτ=1\\nfτ(x) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n2x,\\nt is odd\\n−1\\n2x,\\notherwise\\nThe FTL strategy will keep shifting between xt = −1 and xt = 1, always\\nmaking the wrong choice.\\nThe intuitive FTL strategy fails in the example above because it is un-\\nstable. Can we modify the FTL strategy such that it won’t change decisions\\noften, thereby causing it to attain low regret?\\nThis question motivates the need for a general means of stabilizing the\\nFTL method. Such a means is referred to as “regularization”.\\n\\n5.2. THE REGULARIZATION FRAMEWORK\\n55\\nAlgorithm 7 Regularized Follow The Leader\\n1: Input: η > 0, regularization function R, and a convex compact set K.\\n2: Let x1 = arg minx∈K {R(x)}.\\n3: for t = 1 to T do\\n4:\\nPredict xt.\\n5:\\nObserve the payoﬀfunction ft and let ∇t = ∇ft(xt).\\n6:\\nUpdate\\nxt+1 = arg min\\nx∈K\\n(\\nη\\nt\\nX\\ns=1\\n∇⊤\\ns x + R(x)\\n)\\n7: end for\\nThe generic RFTL meta-algorithm is deﬁned in Algorithm 7. The reg-\\nularization function R is assumed to be strongly convex, smooth, and twice\\ndiﬀerentiable.\\n5.2.2\\nMirrored Descent\\nAn alternative view of this algorithm is in terms of iterative updates, which\\ncan be spelled out using the above deﬁnition directly. The resulting algo-\\nrithm is called ”Mirrored Descent”.\\nOMD is an iterative algorithm that computes the current decision using a\\nsimple gradient update rule and the previous decision, much like OGD. The\\ngenerality of the method stems from the update being carried out in a “dual”\\nspace, where the duality notion is deﬁned by the choice of regularization:\\nthe gradient of the regularization function deﬁnes a mapping from Rn onto\\nitself, which is a vector ﬁeld. The gradient updates are then carried out in\\nthis vector ﬁeld.\\nFor the RFTL algorithm the intuition was straightforward—the regular-\\nization was used to ensure stability of the decision. For OMD, regularization\\nhas an additional purpose: regularization transforms the space in which gra-\\ndient updates are performed. This transformation enables better bounds in\\nterms of the geometry of the space.\\nThe OMD algorithm comes in two ﬂavors: an agile and a lazy version.\\nThe lazy version keeps track of a point in Euclidean space and projects onto\\nthe convex decision set K only at decision time. In contrast, the agile version\\nmaintains a feasible point at all times, much like OGD.\\n\\n56\\nCHAPTER 5. REGULARIZATION\\nAlgorithm 8 Online Mirrored Descent\\n1: Input: parameter η > 0, regularization function R(x).\\n2: Let y1 be such that ∇R(y1) = 0 and x1 = arg minx∈K BR(x||y1).\\n3: for t = 1 to T do\\n4:\\nPlay xt.\\n5:\\nObserve the payoﬀfunction ft and let ∇t = ∇ft(xt).\\n6:\\nUpdate yt according to the rule:\\n[Lazy version]\\n∇R(yt+1) = ∇R(yt) −η ∇t\\n[Agile version]\\n∇R(yt+1) = ∇R(xt) −η ∇t\\nProject according to BR:\\nxt+1 = arg min\\nx∈K\\nBR(x||yt+1)\\n7: end for\\nA myriad of questions arise, but ﬁrst, let us see how does this algorithm\\ngive rise to both OGD.\\nWe note that there are other important special cases of the RFTL meta-\\nalgorithm: those are derived with matrix-norm regularization—namely, the\\nvon Neumann entropy function, and the log-determinant function, as well\\nas self-concordant barrier regularization. Perhaps most importantly for opti-\\nmization, also the AdaGrad algorithm is obtained via changing regularization—\\nwhich we shall explore in detail in the next chapter.\\n5.2.3\\nDeriving online gradient descent\\nTo derive the online gradient descent algorithm, we take R(x) =\\n1\\n2∥x −\\nx0∥2\\n2 for an arbitrary x0 ∈K. Projection with respect to this divergence\\nis the standard Euclidean projection (left as an exercise), and in addition,\\n∇R(x) = x−x0. Hence, the update rule for the OMD Algorithm 8 becomes:\\nxt = Π\\nK(yt), yt = yt−1 −η∇t−1\\nlazy version\\nxt = Π\\nK(yt), yt = xt−1 −η∇t−1\\nagile version\\nThe latter algorithm is exactly online gradient descent, as described in\\nAlgorithm 4 in Chapter 4. Furthermore, both variants are identical for the\\ncase in which K is the unit ball.\\n\\n5.3. TECHNICAL BACKGROUND: REGULARIZATION FUNCTIONS57\\nWe later prove general regret bounds that will imply a O(GD\\n√\\nT) regret\\nfor OGD as a special case of mirrored descent.\\n5.2.4\\nDeriving multiplicative updates\\nLet R(x) = x log x = P\\ni xi log xi be the negative entropy function, where\\nlog x is to be interpreted elementwise. Then ∇R(x) = 1 + log x, and hence\\nthe update rules for the OMD algorithm become:\\nxt = arg min\\nx∈K\\nBR(x||yt), log yt = log yt−1 −η∇t−1\\nlazy version\\nxt = arg min\\nx∈K\\nBR(x||yt), log yt = log xt−1 −η∇t−1\\nagile version\\nWith this choice of regularizer, a notable special case is the experts\\nproblem we encountered in §5.1, for which the decision set K is the n-\\ndimensional simplex ∆n = {x ∈Rn\\n+ | P\\ni xi = 1}. In this special case, the\\nprojection according to the negative entropy becomes scaling by the ℓ1 norm\\n(left as an exercise), which implies that both update rules amount to the\\nsame algorithm:\\nxt+1(i) =\\nxt(i) · e−η∇t(i)\\nPn\\nj=1 xt(j) · e−η∇t(j) ,\\nwhich is exactly the Hedge algorithm! The general theorem we shall prove\\nhenceforth recovers the O(√T log n) bound for prediction from expert advice\\nfor this algorithm.\\n5.3\\nTechnical background: regularization functions\\nIn the rest of this chapter we analyze the mirrored descent algorithm. For\\nthis purpose, consider regularization functions, denoted R : K 7→R, which\\nare strongly convex and smooth (recall deﬁnitions in §2.1).\\nAlthough it is not strictly necessary, we assume that the regularization\\nfunctions in this chapter are twice diﬀerentiable over K and, for all points\\nx ∈int(K) in the interior of the decision set, have a Hessian ∇2R(x) that\\nis, by the strong convexity of R, positive deﬁnite.\\nWe denote the diameter of the set K relative to the function R as\\nDR =\\nr\\nmax\\nx,y∈K{R(x) −R(y)}\\n\\n58\\nCHAPTER 5. REGULARIZATION\\nHenceforth we make use of general norms and their dual. The dual norm\\nto a norm ∥· ∥is given by the following deﬁnition:\\n∥y∥∗≜max\\n∥x∥≤1⟨x, y⟩\\nA positive deﬁnite matrix A gives rise to the matrix norm ∥x∥A =\\n√\\nx⊤Ax.\\nThe dual norm of a matrix norm is ∥x∥∗\\nA = ∥x∥A−1.\\nThe generalized Cauchy-Schwarz theorem asserts ⟨x, y⟩≤∥x∥∥y∥∗and\\nin particular for matrix norms, ⟨x, y⟩≤∥x∥A∥y∥∗\\nA.\\nIn our derivations, we usually consider matrix norms with respect to\\n∇2R(x), the Hessian of the regularization function R(x). In such cases, we\\nuse the notation\\n∥x∥y ≜∥x∥∇2R(y)\\nand similarly\\n∥x∥∗\\ny ≜∥x∥∇−2R(y)\\nA crucial quantity in the analysis with regularization is the remainder\\nterm of the Taylor approximation of the regularization function, and es-\\npecially the remainder term of the ﬁrst order Taylor approximation. The\\ndiﬀerence between the value of the regularization function at x and the value\\nof the ﬁrst order Taylor approximation is known as the Bregman divergence,\\ngiven by\\nDeﬁnition 5.6. Denote by BR(x||y) the Bregman divergence with respect\\nto the function R, deﬁned as\\nBR(x||y) = R(x) −R(y) −∇R(y)⊤(x −y)\\nFor twice diﬀerentiable functions, Taylor expansion and the mean-value\\ntheorem assert that the Bregman divergence is equal to the second derivative\\nat an intermediate point, i.e., (see exercises)\\nBR(x||y) = 1\\n2∥x −y∥2\\nz,\\nfor some point z ∈[x, y], meaning there exists some α ∈[0, 1] such that\\nz = αx+(1−α)y. Therefore, the Bregman divergence deﬁnes a local norm,\\nwhich has a dual norm. We shall denote this dual norm by\\n∥· ∥∗\\nx,y ≜∥· ∥∗\\nz.\\n\\n5.4. REGRET BOUNDS FOR MIRRORED DESCENT\\n59\\nWith this notation we have\\nBR(x||y) = 1\\n2∥x −y∥2\\nx,y.\\nIn online convex optimization, we commonly refer to the Bregman divergence\\nbetween two consecutive decision points xt and xt+1.\\nIn such cases, we\\nshorthand notation for the norm deﬁned by the Bregman divergence with\\nrespect to R on the intermediate point in [xt, xt+1] as ∥· ∥t ≜∥· ∥xt,xt+1.\\nThe latter norm is called the local norm at iteration t. With this notation,\\nwe have BR(xt||xt+1) = 1\\n2∥xt −xt+1∥2\\nt .\\nFinally, we consider below generalized projections that use the Bregman\\ndivergence as a distance instead of a norm. Formally, the projection of a\\npoint y according to the Bregman divergence with respect to function R is\\ngiven by\\narg min\\nx∈K\\nBR(x||y)\\n5.4\\nRegret bounds for Mirrored Descent\\nIn this subsection we prove regret bounds for the agile version of the RFTL\\nalgorithm. The analysis is quite diﬀerent than the one for the lazy version,\\nand of independent interest.\\nTheorem 5.7. The RFTL Algorithm 8 attains for every u ∈K the following\\nbound on the regret:\\nregretT ≤2η\\nT\\nX\\nt=1\\n∥∇t∥∗2\\nt + R(u) −R(x1)\\nη\\n.\\nIf an upper bound on the local norms is known, i.e. ∥∇t∥∗\\nt ≤GR for all\\ntimes t, then we can further optimize over the choice of η to obtain\\nregretT ≤2DRGR\\n√\\n2T.\\nProof. Since the functions ft are convex, for any x∗∈K,\\nft(xt) −ft(x∗) ≤∇ft(xt)⊤(xt −x∗).\\n\\n60\\nCHAPTER 5. REGULARIZATION\\nThe following property of Bregman divergences follows easily from the def-\\ninition: for any vectors x, y, z,\\n(x −y)⊤(∇R(z) −∇R(y)) = BR(x, y) −BR(x, z) + BR(y, z).\\nCombining both observations,\\n2(ft(xt) −ft(x∗)) ≤2∇ft(xt)⊤(xt −x∗)\\n= 1\\nη(∇R(yt+1) −∇R(xt))⊤(x∗−xt)\\n= 1\\nη[BR(x∗, xt) −BR(x∗, yt+1) + BR(xt, yt+1)]\\n≤1\\nη[BR(x∗, xt) −BR(x∗, xt+1) + BR(xt, yt+1)]\\nwhere the last inequality follows from the generalized Pythagorean inequality\\n(see [15] Lemma 11.3), as xt+1 is the projection w.r.t the Bregman divergence\\nof yt+1 and x∗∈K is in the convex set. Summing over all iterations,\\n2regret\\n≤\\n1\\nη[BR(x∗, x1) −BR(x∗, xT )] +\\nT\\nX\\nt=1\\n1\\nηBR(xt, yt+1)\\n≤\\n1\\nηD2 +\\nT\\nX\\nt=1\\n1\\nηBR(xt, yt+1)\\n(5.1)\\nWe proceed to bound BR(xt, yt+1). By deﬁnition of Bregman divergence,\\nand the generalized Cauchy-Schwartz inequality,\\nBR(xt, yt+1) + BR(yt+1, xt) = (∇R(xt) −∇R(yt+1))⊤(xt −yt+1)\\n= η∇ft(xt)⊤(xt −yt+1)\\n≤η∥∇ft(xt)∥∗∥xt −yt+1∥\\n≤1\\n2η2G2\\n∗+ 1\\n2∥xt −yt+1∥2.\\nwhere in the last inequality follows from (a−b)2 ≥0. Thus, by our assump-\\ntion BR(x, y) ≥1\\n2∥x −y∥2, we have\\nBR(xt, yt+1) ≤1\\n2η2G2\\n∗+ 1\\n2∥xt −yt+1∥2 −BR(yt+1, xt) ≤1\\n2η2G2\\n∗.\\nPlugging back into Equation (5.1), and by non-negativity of the Bregman\\ndivergence, we get\\nregret ≤1\\n2[1\\nηD2 + 1\\n2ηTG2\\n∗] ≤DG∗\\n√\\nT ,\\n\\n5.4. REGRET BOUNDS FOR MIRRORED DESCENT\\n61\\nby taking η =\\nD\\n2\\n√\\nTG∗\\n\\n62\\nCHAPTER 5. REGULARIZATION\\n5.5\\nExercises\\n1.\\n(a) Show that the dual norm to a matrix norm given by A ≻0\\ncorresponds to the matrix norm of A−1.\\n(b) Prove the generalized Cauchy-Schwarz inequality for any norm,\\ni.e.,\\n⟨x, y⟩≤∥x∥∥y∥∗\\n2. Prove that the Bregman divergence is equal to the local norm at an\\nintermediate point, that is:\\nBR(x||y) = 1\\n2∥x −y∥2\\nz,\\nwhere z ∈[x, y] and the interval [x, y] is deﬁned as\\n[x, y] = {v = αx + (1 −α)y , α ∈[0, 1]}\\n3. Let R(x) = 1\\n2∥x−x0∥2 be the (shifted) Euclidean regularization func-\\ntion.\\nProve that the corresponding Bregman divergence is the Eu-\\nclidean metric. Conclude that projections with respect to this diver-\\ngence are standard Euclidean projections.\\n4. Prove that both agile and lazy versions of the OMD meta-algorithm\\nare equivalent in the case that the regularization is Euclidean and the\\ndecision set is the Euclidean ball.\\n5. For this problem the decision set is the n-dimensional simplex. Let\\nR(x) = x log x be the negative entropy regularization function. Prove\\nthat the corresponding Bregman divergence is the relative entropy,\\nand prove that the diameter DR of the n-dimensional simplex with\\nrespect to this function is bounded by log n. Show that projections\\nwith respect to this divergence over the simplex amounts to scaling by\\nthe ℓ1 norm.\\n6. ∗A set K ⊆Rd is symmetric if x ∈K implies −x ∈K. Symmetric\\nsets gives rise to a natural deﬁnition of a norm. Deﬁne the function\\n∥· ∥K : Rd 7→R as\\n∥x∥K = arg min\\nα>0\\n\\x1a 1\\nαx ∈K\\n\\x1b\\nProve that ∥· ∥K is a norm if and only if K is convex.\\n\\n5.6. BIBLIOGRAPHIC REMARKS\\n63\\n5.6\\nBibliographic Remarks\\nRegularization in the context of online learning was ﬁrst studied in [26]\\nand [48]. The inﬂuential paper of Kalai and Vempala [45] coined the term\\n“follow-the-leader” and introduced many of the techniques that followed\\nin OCO. The latter paper studies random perturbation as a regularization\\nand analyzes the follow-the-perturbed-leader algorithm, following an early\\ndevelopment by [29] that was overlooked in learning for many years.\\nIn the context of OCO, the term follow-the-regularized-leader was coined\\nin [73, 71], and at roughly the same time an essentially identical algorithm\\nwas called “RFTL” in [1]. The equivalence of RFTL and Online Mirrored\\nDescent was observed by [34].\\n\\n64\\nCHAPTER 5. REGULARIZATION\\n\\nChapter 6\\nAdaptive Regularization\\nIn the previous chapter we have studied a geometric extension of online /\\nstochastic / determinisitic gradient descent. The technique to achieve it is\\ncalled regularization, and we have seen how for the problem of prediction\\nfrom expert advice, it can potentially given exponential improvements in\\nthe dependence on the dimension.\\nA natural question that arises is whether we can automatically learn the\\noptimal regularization, i.e. best algorithm from the mirrored-descent class,\\nfor the problem at hand?\\nThe answer is positive in a strong sense: it is theoretically possible to\\nlearn the optimal regularization online and in a data-speciﬁc way. Not only\\nthat, the resulting algorithms exhibit the most signiﬁcant speedups in train-\\ning deep neural networks from all accelerations studied thus far.\\n6.1\\nAdaptive Learning Rates: Intuition\\nThe intuition for adaptive regularization is simple: consider an optimization\\nproblem which is axis-aligned, in which each coordinate is independent of\\nthe rest. It is reasonable to ﬁne tune the learning rate for each coordinate\\nseparately - to achieve optimal convergence in that particular subspace of\\nthe problem, independently of the rest.\\nThus, it is reasonable to change the SGD update rule from xt+1 ←\\nxt −η∇t, to the more robust\\nxt+1 ←xt −Dt∇t,\\nwhere Dt is a diagonal matrix that contains in coordinate (i, i) the learning\\nrate for coordinate i in the gradient.\\nRecall from the previous sections\\n65\\n\\n66\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nthat the optimal learning rate for stochastic non-convex optimization is of\\nthe order O( 1\\n√\\nt). More precisely, in Theorem 3.4, we have seen that this\\nlearning rate should be on the order of O(\\n1\\n√\\ntσ2 ), where σ2 is the variance of\\nthe stochastic gradients. The empirical estimator of the latter is P\\ni<t ∥∇i∥2.\\nThus, the robust version of stochastic gradient descent for smooth non-\\nconvex optimization should behave as the above equation, with\\nDt(i, i) =\\n1\\npP\\ni<t ∇t(i)2 .\\nThis is exactly the diagonal version of the AdaGrad algorithm! We continue\\nto rigorously derive it and prove its performance guarantee.\\n6.2\\nA Regularization Viewpoint\\nIn the previous chapter we have introduced regularization as a general\\nmethodology for deriving online convex optimization algorithms. Theorem\\n5.7 bounds the regret of the Mirrored Descent algorithm for any strongly\\nconvex regularizer as\\nregretT ≤max\\nu∈K\\ns\\n2\\nX\\nt\\n∥∇t∥∗2\\nt BR(u||x1).\\nIn addition, we have seen how to derive the online gradient descent and the\\nmultiplicative weights algorithms as special cases of the RFTL methodology.\\nWe consider the following question: thus far we have thought of R as\\na strongly convex function. But which strongly convex function should we\\nchoose to minimize regret? This is a deep and diﬃcult question which has\\nbeen considered in the optimization literature since its early developments.\\nThe ML approach is to learn the optimal regularization online. That is,\\na regularizer that adapts to the sequence of cost functions and is in a sense\\nthe “optimal” regularization to use in hindsight. We formalize this in the\\nnext section.\\n6.3\\nTools from Matrix Calculus\\nMany of the inequalities that we are familiar with for positive real numbers\\nhold for positive semi-deﬁnite matrices as well.\\nWe henceforth need the\\nfollowing inequality, which is left as an exercise,\\n\\n6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS\\n67\\nProposition 6.1. For positive deﬁnite matrices A ≽B ≻0:\\n2Tr((A −B)1/2) + Tr(A−1/2B) ≤2Tr(A1/2).\\nNext, we require a structural result which explicitly gives the optimal\\nregularization as a function of the gradients of the cost functions. For a\\nproof see the exercises.\\nProposition 6.2. Let A ≽0. The minimizer of the following minimization\\nproblem:\\nmin\\nX\\nTr(X−1A)\\nsubject to X ≽0\\nTr(X) ≤1,\\nis X = A1/2/Tr(A1/2), and the minimum objective value is Tr2(A1/2).\\n6.4\\nThe AdaGrad Algorithm and Its Analysis\\nTo be more formal, let us consider the set of all strongly convex regulariza-\\ntion functions with a ﬁxed and bounded Hessian in the set\\n∀x ∈K . ∇2R(x) = ∇2 ∈H ≜{X ∈Rn×n ; Tr(X) ≤1 , X ≽0}\\nThe set H is a restricted class of regularization functions (which does not\\ninclude the entropic regularization). However, it is a general enough class\\nto capture online gradient descent along with any rotation of the Euclidean\\nregularization.\\nAlgorithm 9 AdaGrad (Full Matrix version)\\n1: Input: parameters η, x1 ∈K.\\n2: Initialize: S0 = G0 = 0,\\n3: for t = 1 to T do\\n4:\\nPredict xt, suﬀer loss ft(xt).\\n5:\\nUpdate:\\nSt = St−1 + ∇t∇⊤\\nt , Gt = St1/2\\nyt+1 = xt −ηG−1\\nt ∇t\\nxt+1 = arg min\\nx∈K\\n∥yt+1 −x∥2\\nGt\\n6: end for\\n\\n68\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nThe problem of learning the optimal regularization has given rise to Algo-\\nrithm 9, known as the AdaGrad (Adaptive subGradient method) algorithm.\\nIn the algorithm deﬁnition and throughout this chapter, the notation A−1\\nrefers to the Moore-Penrose pseudoinverse of the matrix A. Perhaps sur-\\nprisingly, the regret of AdaGrad is at most a constant factor larger than the\\nminimum regret of all RFTL algorithm with regularization functions whose\\nHessian is ﬁxed and belongs to the class H. The regret bound on AdaGrad\\nis formally stated in the following theorem.\\nTheorem 6.3. Let {xt} be deﬁned by Algorithm 9 with parameters η = D,\\nwhere\\nD = max\\nu∈K ∥u −x1∥2.\\nThen for any x⋆∈K,\\nregretT (AdaGrad) ≤2D\\ns\\nmin\\nH∈H\\nX\\nt\\n∥∇t∥∗2\\nH .\\nBefore proving this theorem, notice that it delivers on one of the promised\\naccounts: comparing to the bound of Theorem 5.7 and ignoring the diameter\\nD and dimensionality, the regret bound is as good as the regret of RFTL\\nfor the class of regularization functions.\\nWe proceed to prove Theorem 6.3. First, a direct corollary of Proposition\\n6.2 is that\\nCorollary 6.4.\\ns\\nmin\\nH∈H\\nX\\nt\\n∥∇t∥∗2\\nH\\n=\\nq\\nminH∈H Tr(H−1 P\\nt ∇t∇⊤\\nt )\\n= Tr\\nqP\\nt ∇t∇⊤\\nt = Tr(GT )\\nHence, to prove Theorem 6.3, it suﬃces to prove the following lemma.\\nLemma 6.5.\\nregretT (AdaGrad) ≤2DTr(GT ) = 2D\\ns\\nmin\\nH∈H\\nX\\nt\\n∥∇t∥∗2\\nH .\\nProof. By the deﬁnition of yt+1:\\nyt+1 −x⋆= xt −x⋆−ηGt−1∇t,\\n(6.1)\\n\\n6.4. THE ADAGRAD ALGORITHM AND ITS ANALYSIS\\n69\\nand\\nGt(yt+1 −x⋆) = Gt(xt −x⋆) −η∇t.\\n(6.2)\\nMultiplying the transpose of (6.1) by (6.2) we get\\n(yt+1 −x⋆)⊤Gt(yt+1 −x⋆) =\\n(xt−x⋆)⊤Gt(xt−x⋆) −2η∇⊤\\nt (xt−x⋆) + η2∇⊤\\nt G−1\\nt ∇t.\\n(6.3)\\nSince xt+1 is the projection of yt+1 in the norm induced by Gt, we have (see\\n§2.1.1)\\n(yt+1 −x⋆)⊤Gt(yt+1 −x⋆) = ∥yt+1 −x⋆∥2\\nGt ≥∥xt+1 −x⋆∥2\\nGt.\\nThis inequality is the reason for using generalized projections as opposed\\nto standard projections, which were used in the analysis of online gradient\\ndescent (see §4.4 Equation (4.2)). This fact together with (6.3) gives\\n∇⊤\\nt (xt−x⋆) ≤η\\n2∇⊤\\nt G−1\\nt ∇t + 1\\n2η\\n\\x00∥xt −x⋆∥2\\nGt −∥xt+1 −x⋆∥2\\nGt\\n\\x01\\n.\\nNow, summing up over t = 1 to T we get that\\nT\\nX\\nt=1\\n∇⊤\\nt (xt −x⋆) ≤η\\n2\\nT\\nX\\nt=1\\n∇⊤\\nt G−1\\nt ∇t + 1\\n2η∥x1 −x⋆∥2\\nG0\\n(6.4)\\n+ 1\\n2η\\nT\\nX\\nt=1\\n\\x10\\n∥xt −x⋆∥2\\nGt −∥xt −x⋆∥2\\nGt−1\\n\\x11\\n−1\\n2η∥xT+1 −x⋆∥2\\nGT\\n≤η\\n2\\nT\\nX\\nt=1\\n∇⊤\\nt G−1\\nt ∇t + 1\\n2η\\nT\\nX\\nt=1\\n(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆).\\nIn the last inequality we use the fact that G0 = 0. We proceed to bound\\neach of the terms above separately.\\nLemma 6.6. With St, Gt as deﬁned in Algorithm 9,\\nT\\nX\\nt=1\\n∇⊤\\nt G−1\\nt ∇t ≤2\\nT\\nX\\nt=1\\n∇⊤\\nt G−1\\nT ∇t ≤2Tr(GT ).\\nProof. We prove the lemma by induction. The base case follows since\\n∇⊤\\n1 G−1\\n1 ∇1 = Tr(G−1\\n1 ∇1∇⊤\\n1 )\\n= Tr(G−1\\n1 G2\\n1)\\n= Tr(G1).\\n\\n70\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\nAssuming the lemma holds for T −1, we get by the inductive hypothesis\\nT\\nX\\nt=1\\n∇⊤\\nt G−1\\nt ∇t ≤2Tr(GT−1) + ∇⊤\\nT G−1\\nT ∇T\\n= 2Tr((G2\\nT −∇T ∇⊤\\nT )1/2) + Tr(G−1\\nT ∇T ∇⊤\\nT )\\n≤2Tr(GT ).\\nHere, the last inequality is due to the matrix inequality 6.1.\\nLemma 6.7.\\nT\\nX\\nt=1\\n(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆) ≤D2Tr(GT ).\\nProof. By deﬁnition St ≽St−1, and hence Gt ≽Gt−1. Thus,\\nT\\nX\\nt=1\\n(xt−x⋆)⊤(Gt −Gt−1)(xt−x⋆)\\n≤\\nT\\nX\\nt=1\\nD2λmax(Gt −Gt−1)\\n≤D2\\nT\\nX\\nt=1\\nTr(Gt −Gt−1)\\nA ≽0 ⇒λmax(A) ≤Tr(A)\\n= D2\\nT\\nX\\nt=1\\n(Tr(Gt) −Tr(Gt−1))\\nlinearity of the trace\\n≤D2Tr(GT ).\\nPlugging both lemmas into Equation (6.4), we obtain\\nT\\nX\\nt=1\\n∇⊤\\nt (xt −x⋆) ≤ηTr(GT ) + 1\\n2ηD2Tr(GT ) ≤2DTr(GT ).\\n\\n6.5. DIAGONAL ADAGRAD\\n71\\n6.5\\nDiagonal AdaGrad\\nThe AdaGrad algorithm maintains potentially dense matrices, and requires\\nthe computation of the square root of these matrices. This is usually pro-\\nhibitive in machine learning applications in which the dimension is very\\nlarge.\\nFortunately, the same ideas can be applied with almost no com-\\nputational overhead on top of vanilla SGD, using the diagonal version of\\nAdaGrad given by:\\nAlgorithm 10 AdaGrad (diagonal version)\\n1: Input: parameters η, x1 ∈K.\\n2: Initialize: S0 = G0 = 0,\\n3: for t = 1 to T do\\n4:\\nPredict xt, suﬀer loss ft(xt).\\n5:\\nUpdate:\\nSt = St−1 + diag(∇t∇⊤\\nt ), Gt = St1/2\\nyt+1 = xt −ηG−1\\nt ∇t\\nxt+1 = arg min\\nx∈K\\n∥yt+1 −x∥2\\nGt\\n6: end for\\nIn contrast to the full-matrix version, this version can be implemented in\\nlinear time and space, since diagonal matrices can be manipulated as vectors.\\nThus, memory overhead is only a single d-dimensional vector, which is used\\nto represent the diagonal preconditioning (regularization) matrix, and the\\ncomputational overhead is a few vector manipulations per iteration.\\nVery similar to the full matrix case, the diagonal AdaGrad algorithm\\ncan be analyzed and the following performance bound obtained:\\nTheorem 6.8. Let {xt} be deﬁned by Algorithm 10 with parameters η =\\nD∞, where\\nD∞= max\\nu∈K ∥u −x1∥∞,\\nand let diag(H) be the set of all diagonal matrices in H.\\nThen for any\\nx⋆∈K,\\nregretT (D-AdaGrad) ≤2D∞\\ns\\nmin\\nH∈diag(H)\\nX\\nt\\n∥∇t∥∗2\\nH .\\n\\n72\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\n6.6\\nState-of-the-art: from Adam to Shampoo and\\nbeyond\\nSince the introduction of the adaptive regularization technique in the con-\\ntext of regret minimization, several improvements were introduced that now\\ncompose state-of-the-art. A few notable advancements include:\\nAdaDelta: The algorithm keeps an exponential average of past gradients and uses\\nthat in the update step.\\nAdam: Adds a sliding window to AdaGrad, as well as adding a form of mo-\\nmentum via estimating the second moments of past gradients and\\nadjusting the update accordingly.\\nShampoo: Interpolates between full-matrix and diagonal adagrad in the context\\nof deep neural networks: use of the special layer structure to reduce\\nmemory constraints.\\nAdaFactor: Suggests a Shampoo-like approach to reduce memory footprint even\\nfurther, to allow the training of huge models.\\nGGT: While full-matrix AdaGrad is computationally slow due to the cost\\nof manipulating matrices, this algorithm uses recent gradients (a thin\\nmatrix G), and via linear algebraic manipulations reduces computa-\\ntion by never computing GG⊤, but rather only G⊤G, which is low\\ndimensional.\\nSM3 , ET: Diagonal AdaGrad requires an extra O(n) memory to store diag(Gt).\\nThese algorithms, inspired by AdaFactor, approximate Gt as a low\\nrank tensor to save memory and computation.\\n\\n6.7. EXERCISES\\n73\\n6.7\\nExercises\\n1. ∗Prove that for positive deﬁnite matrices A ≽B ≻0 it holds that\\n(a) A1/2 ≽B1/2\\n(b) 2Tr((A −B)1/2) + Tr(A−1/2B) ≤2Tr(A1/2).\\n2. ∗Consider the following minimization problem where A ≻0:\\nmin\\nX\\nTr(X−1A)\\nsubject to\\nX ≻0\\nTr(X) ≤1.\\nProve that its minimizer is given by X = A1/2/Tr(A1/2), and the\\nminimum is obtained at Tr2(A1/2).\\n\\n74\\nCHAPTER 6. ADAPTIVE REGULARIZATION\\n6.8\\nBibliographic Remarks\\nThe AdaGrad algorithm was introduced in [19, 18], its diagonal version\\nwas also discovered in parallel in [52]. Adam [46] and RMSprop [39] are\\nwidely used methods based on adaptive regularization. A cleaner analysis\\nwas recently proposed in [27], see also [17].\\nAdaptive regularization has received much attention recently, see e.g.,\\n[60, 85]. Newer algorithmic developments on adaptive regularization include\\nShampoo [28], GGT [3], AdaFactor [77], Extreme Tensoring [16] and SM3\\n[6].\\n\\nChapter 7\\nVariance Reduction\\nIn the previous chapter we have studied the ﬁrst of our three acceleration\\ntechniques over SGD, adaptive regularization, which is a geometric tool for\\nacceleration. In this chapter we introduce the second ﬁrst-order accelera-\\ntion technique, called variance reduction. This technique is probabilistic in\\nnature, and applies to more restricted settings of mathematical optimiza-\\ntion in which the objective function has a ﬁnite-sum structure. Namely, we\\nconsider optimization problems of the form\\nmin\\nx∈K f(x) , f(x) = 1\\nm\\nm\\nX\\ni=1\\nfi(x) .\\n(7.1)\\nSuch optimization problems are canonical in training of ML models, con-\\nvex and non-convex. However, in the context of machine learning we should\\nremember that the ultimate goal is generalization rather than training.\\n7.1\\nVariance reduction: Intuition\\nThe intuition for variance reduction is simple, and comes from trying to\\nimprove the naive convergence bounds for SGD that we have covered in the\\nﬁrst lesson.\\nRecall the SGD update rule xt+1 ←xt−η ˆ\\n∇t, in which ˆ\\n∇t is an unbiased\\nestimator for the gradient such that\\nE[ ˆ\\n∇t] = ∇t , E[∥ˆ\\n∇t∥2\\n2] ≤σ2.\\nWe have seen in Theorem 3.4, that for this update rule,\\nE\\n\"\\n1\\nT\\nX\\nt\\n∥∇t∥2\\n#\\n≤2\\nr\\nMβσ2\\nT\\n.\\n75\\n\\n76\\nCHAPTER 7. VARIANCE REDUCTION\\nThe convergence is proportional to the second moment of the gradient esti-\\nmator, and thus it makes sense to try to reduce this second moment. The\\nvariance reduction technique attempts to do so by using the average of all\\nprevious gradients, as we show next.\\n7.2\\nSetting and deﬁnitions\\nWe consider the ERM optimization problem over an average of loss func-\\ntions. Before we begin, we need a few preliminaries and assumptions:\\n1. We denote distance to optimality according to function value as\\nht = f(xt) −f(x∗),\\nand in the k’th epoch of an algorithm, we denote hk\\nt = f(xk\\nt ) −f(x∗).\\n2. We denote ˜\\nhk = max\\n\\x08\\n4hk\\n0 , 8αD2\\nk\\n\\t\\nover an epoch.\\n3. Assume all stochastic gradients have bounded second moments\\n∥ˆ\\n∇t∥2\\n2 ≤σ2.\\n4. We will assume that the individual functions fi in formulation (7.1)\\nare also ˆ\\nβ-smooth and have ˆ\\nβ-Lipschitz gradient, namely\\n∥∇fi(x) −∇fi(y)∥≤ˆ\\nβ∥x −y∥.\\n5. We will use, proved in Lemma 2.3, that for β-smooth and α-strongly\\nconvex f we have\\nht ≥1\\n2β ∥∇t∥2\\nand\\nα\\n2 d2\\nt = α\\n2 ∥xt −x∗∥2 ≤ht ≤1\\n2α∥∇t∥2.\\n6. Recall that a function f is γ-well-conditioned if it is β-smooth, α-\\nstrongly convex and γ ≤α\\nβ .\\n\\n7.3. THE VARIANCE REDUCTION ADVANTAGE\\n77\\n7.3\\nThe variance reduction advantage\\nConsider gradient descent for γ-well conditioned functions, and speciﬁcally\\nused for ML training as in formulation (7.1) . It is well known that GD\\nattains linear convergence rate as we now prove for completeness:\\nTheorem 7.1. For unconstrained minimization of γ-well-conditioned func-\\ntions and ηt = 1\\nβ, the Gradient Descent Algorithm 2 converges as\\nht+1 ≤h1e−γt.\\nProof.\\nht+1 −ht = f(xt+1) −f(xt)\\n≤∇⊤\\nt (xt+1 −xt) + β\\n2 ∥xt+1 −xt∥2\\nβ-smoothness\\n= −ηt∥∇t∥2 + β\\n2 η2\\nt ∥∇t∥2\\nalgorithm defn.\\n= −1\\n2β ∥∇t∥2\\nchoice of ηt = 1\\nβ\\n≤−α\\nβ ht.\\nby (2.1)\\nThus,\\nht+1 ≤ht(1 −α\\nβ ) ≤· · · ≤h1(1 −γ)t ≤h1e−γt\\nwhere the last inequality follows from 1 −x ≤e−x for all x ∈R.\\nHowever, what is the overall computational cost? Assuming that we can\\ncompute the gradient of each loss function corresponding to the individual\\ntraining examples in O(d) time, the overall running time to compute the\\ngradient is O(md).\\nIn order to attain approximation ε to the objective, the algorithm re-\\nquires O( 1\\nγ log 1\\nε) iterations, as per the Theorem above. Thus, the overall\\nrunning time becomes O( md\\nγ log 1\\nε). As we show below, variance reduction\\ncan reduce this running time to be O((m+ 1\\n˜\\nγ2 )d log 1\\nε), where ˜\\nγ is a diﬀerent\\ncondition number for the same problem, that is in general smaller than the\\noriginal. Thus, in one line, the variance reduction advantage can be sum-\\nmarized as:\\nmd\\nγ log 1\\nε 7→(m + 1\\n˜\\nγ2 )d log 1\\nε .\\n\\n78\\nCHAPTER 7. VARIANCE REDUCTION\\n7.4\\nA simple variance-reduced algorithm\\nThe following simple variance-reduced algorithm illustrates the main ideas\\nof the technique.\\nThe algorithm is a stochastic gradient descent variant\\nwhich proceeds in epochs. Strong convexity implies that the distance to the\\noptimum shrinks with function value, so it is safe to decrease the distance\\nupper bound every epoch.\\nThe main innovation is in line 7, which constructs the gradient estimator.\\nInstead of the usual trick - which is to sample one example at random - here\\nthe estimator uses the entire gradient computed at the beginning of the\\ncurrent epoch.\\nAlgorithm 11 Epoch GD\\n1: Input: f, T, x1\\n0 ∈K, upper bound D1 ≥∥x1\\n0 −x∗∥, step sizes {ηt}\\n2: for k = 1 to log 1\\nε do\\n3:\\nLet BDk(xk\\n0) be the ball of radius Dk around xk\\n0.\\n4:\\ncompute full gradient ∇k\\n0 = ∇f(xk\\n0)\\n5:\\nfor t = 1 to T do\\n6:\\nSample it ∈[m] uniformly at random, let ft = fit.\\n7:\\nconstruct stochastic gradient ˆ\\n∇k\\nt = ∇ft(xk\\nt ) −∇ft(xk\\n0) + ∇k\\n0\\n8:\\nLet yk\\nt+1 = xk\\nt −ηt ˆ\\n∇k\\nt , xt+1 = ΠBDk(xk\\n0) (yt+1)\\n9:\\nend for\\n10:\\nSet xk+1\\n0\\n= 1\\nT\\nPT\\nt=1 xk\\nt . Dk+1 ←Dk/2.\\n11: end for\\n12: return x0\\nT+1\\nThe main guarantee for this algorithm is the following theorem, which\\ndelivers upon the aforementioned improvement,\\nTheorem 7.2. Algorithm 11 returns an ε-approximate solution to optimiza-\\ntion problem (7.1) in total time\\nO\\n\\x12\\x12\\nm + 1\\n˜\\nγ2\\n\\x13\\nd log 1\\nε\\n\\x13\\n.\\nLet ˜\\nγ = α\\nˆ\\nβ < γ. Then the proof of this theorem follows from the following\\nlemma.\\nLemma 7.3. For T = ˜\\nO\\n\\x10\\n1\\n˜\\nγ2\\n\\x11\\n, we have\\nE[˜\\nhk+1] ≤1\\n2\\n˜\\nhk.\\n\\n7.4. A SIMPLE VARIANCE-REDUCED ALGORITHM\\n79\\nProof. As a ﬁrst step, we bound the variance of the gradients. Due to the\\nfact that xk\\nt ∈BDk(xk\\n0), we have that for k′ > k, ∥xk\\nt −xk′\\nt ∥2 ≤4D2\\nk. Thus,\\n∥ˆ\\n∇k\\nt ∥2\\n= ∥∇ft(xk\\nt ) −∇ft(xk\\n0) + ∇f(xk\\n0)∥2\\ndeﬁnition\\n≤2∥∇ft(xk\\nt ) −∇ft(xk\\n0)∥2 + 2∥∇f(xk\\n0)∥2\\n(a + b)2 ≤2a2 + 2b2\\n≤2ˆ\\nβ2∥xk\\nt −xk\\n0∥2 + 4βhk\\n0\\nsmoothness\\n≤8ˆ\\nβ2D2\\nk + 4βhk\\n0\\nprojection step\\n≤ˆ\\nβ2 1\\nα˜\\nhk + 4βhk\\n0 ≤˜\\nhk(\\nˆ\\nβ2\\nα + β)\\nNext, using the regret bound for strongly convex functions, we have\\nE[hk+1\\n0\\n]\\n≤E[ 1\\nT\\nP\\nt hk\\nt ]\\nJensen\\n≤\\n1\\nαT E[P\\nt\\n1\\nt ∥ˆ\\n∇k\\nt ∥2]\\nTheorem 4.4\\n≤\\n1\\nαT\\nP\\nt\\n1\\nt ˜\\nhk(\\nˆ\\nβ2\\nα + β)\\nabove\\n≤log T\\nT ˜\\nhk( 1\\n˜\\nγ2 + 1\\nγ )\\n˜\\nγ = α\\nˆ\\nβ\\nWhich implies the Lemma by choice of T, deﬁnition of ˜\\nhk = max\\n\\x08\\n4hk\\n0 , 8αD2\\nk\\n\\t\\n,\\nand exponential decrease of Dk.\\nThe expectation is over the stochastic gradient deﬁnition, and is required\\nfor using Theorem 4.4.\\nTo obtain the theorem from the lemma above, we need to strengthen\\nit to a high probability statement using a martingale argument.\\nThis is\\npossible since the randomness in construction of the stochastic gradients is\\ni.i.d.\\nThe lemma now implies the theorem by noting that O(log 1\\nε) epochs\\nsuﬃces to get ε-approximation. Each epoch requires the computation of one\\nfull gradient, in time O(md), and ˜\\nO( 1\\n˜\\nγ2 ) iterations that require stochastic\\ngradient computation, in time O(d).\\n\\n80\\nCHAPTER 7. VARIANCE REDUCTION\\n7.5\\nBibliographic Remarks\\nThe variance reduction technique was ﬁrst introduced as part of the SAG\\nalgorithm [70]. Since then a host of algorithms were developed using the\\ntechnique. The simplest exposition of the technique was given in [44]. The\\nexposition in this chapter is developed from the Epoch GD algorithm [37],\\nwhich uses a related technique for stochastic strongly convex optimization,\\nas developed in [86].\\n\\nChapter 8\\nNesterov Acceleration\\nIn previous chapters we have studied our bread and butter technique, SGD,\\nas well as two acceleration techniques of adaptive regularization and variance\\nreduction.\\nIn this chapter we study the historically earliest acceleration\\ntechnique, known as Nesterov acceleration, or simply “acceleration”.\\nFor smooth and convex functions, Nesterov acceleration improves the\\nconvergence rate to optimality to O( 1\\nT 2 ), a quadratic improvement over\\nvanilla gradient descent. Similar accelerations are possible when the func-\\ntion is also strongly convex: an accelerated rate of e−√γT , where γ is the\\ncondition number, vs. e−γT of vanilla gradient descent. This improvement\\nis theoretically very signiﬁcant.\\nHowever, in terms of applicability, Nesterov acceleration is theoretically\\nthe most restricted in the context of machine learning: it requires a smooth\\nand convex objective. More importantly, the learning rates of this method\\nare very brittle, and the method is not robust to noise. Since noise is pre-\\ndominant in machine learning, the theoretical guarantees in stochastic op-\\ntimization environments are very restricted.\\nHowever, the heuristic of momentum, which historically inspired acceler-\\nation, is extremely useful for non-convex stochastic optimization (although\\nnot known to yield signiﬁcant improvements in theory).\\n8.1\\nAlgorithm and implementation\\nNesterov acceleration applies to the general setting of constrained smooth\\nconvex optimization:\\nmin\\nx∈Rd f(x).\\n(8.1)\\n81\\n\\n82\\nCHAPTER 8. NESTEROV ACCELERATION\\nFor simplicity of presentation, we restrict ourselves to the unconstrained\\nconvex and smooth case.\\nNevertheless, the method can be extended to\\nconstrained smooth convex, and potentially strongly convex, settings.\\nThe simple method presented in Algorithm 12 below is computationally\\nequivalent to gradient descent.\\nThe only overhead is saving three state\\nvectors (that can be reduced to two) instead of one for gradient descent.\\nThe following simple accelerated algorithm illustrates the main ideas of the\\ntechnique.\\nAlgorithm 12 Simpliﬁed Nesterov Acceleration\\n1: Input: f, T, initial point x0, parameters η, β, τ.\\n2: for t = 1 to T do\\n3:\\nSet xt+1 = τzt + (1 −τ)yt, and denote ∇t+1 = ∇f(xt+1).\\n4:\\nLet yt+1 = xt+1 −1\\nβ∇t+1\\n5:\\nLet zt+1 = zt −η∇t+1\\n6: end for\\n7: return ¯\\nx = 1\\nT\\nP\\nt xt\\n8.2\\nAnalysis\\nThe main guarantee for this algorithm is the following theorem.\\nTheorem 8.1. Algorithm 12 converges to an ε-approximate solution to op-\\ntimization problem (8.1) in O( 1\\n√ε) iterations.\\nThe proof starts with the following lemma which follows from our earlier\\nstandard derivations.\\nLemma 8.2.\\nη∇⊤\\nt+1(zt −x∗) ≤2η2β(f(xt+1) −f(yt+1)) +\\n\\x02\\n∥zt −x∗∥2 −∥zt+1 −x∗∥2\\x03\\n.\\nProof. The proof is very similar to that of Theorem 4.2. By deﬁnition of zt,\\n1\\n∥zt+1 −x∗∥2\\n= ∥zt −η∇t+1 −x∗∥2\\n= ∥zt −x∗∥2 −η∇⊤\\nt+1(zt −x∗) + η2∥∇t+1∥2\\n≤∥zt −x∗∥2 −η∇⊤\\nt+1(zt −x∗) + 2η2β(f(xt+1) −f(yt+1))\\nLemma 2.3 part 3\\n1Henceforth we use Lemma 2.3 part 3.\\nThis proof of this Lemma shows that for\\ny = x −1\\nβ ∇f(x), it holds that f(x) −f(y) ≥\\n1\\n2β ∥∇f(x)∥2.\\n\\n8.2. ANALYSIS\\n83\\nLemma 8.3. For 2ηβ = 1−τ\\nτ , we have that\\nη∇⊤\\nt+1(xt+1 −x∗) ≤2η2β(f(yt) −f(yt+1)) +\\n\\x02\\n∥zt −x∗∥2 −∥zt+1 −x∗∥2\\x03\\n.\\nProof.\\nη∇⊤\\nt+1(xt+1 −x∗) −η∇⊤\\nt+1(zt −x∗)\\n= η∇⊤\\nt+1(xt+1 −zt)\\n= (1−τ)η\\nτ\\n∇⊤\\nt+1(yt −xt+1)\\nτ(xt+1 −zt) = (1 −τ)(yt −xt+1)\\n≤(1−τ)η\\nτ\\n(f(yt) −f(xt+1)).\\nconvexity\\nThus, in combination with Lemma 8.2, and the condition of the Lemma, we\\nget the inequality.\\nWe can now sketch the proof of the main theorem.\\nProof. Telescope Lemma 8.3 for all iterations to obtain:\\nThT\\n= T(f(¯\\nx) −f(x∗))\\n≤P\\nt ∇⊤\\nt (xt −x∗)\\n≤2ηβ P\\nt(f(yt) −f(yt+1)) + 1\\nη\\nP\\nt\\n\\x02\\n∥zt −x∗∥2 −∥zt+1 −x∗∥2\\x03\\n≤2ηβ(f(y1) −f(yT+1)) + 1\\nη\\n\\x02\\n∥z1 −x∗∥2 −∥zT+1 −x∗∥2\\x03\\n≤√2βh1D,\\noptimizing η\\nwhere h1 is an upper bound on the distance f(y1) −f(x∗), and D bounds\\nthe Euclidean distance of zt to the optimum. Thus, we get a recurrence of\\nthe form\\nhT ≤\\n√h1\\nT\\n.\\nRestarting Algorithm 12 and adapting the learning rate according to hT\\ngives a rate of convergence of O( 1\\nT 2 ) to optimality.\\n\\n84\\nCHAPTER 8. NESTEROV ACCELERATION\\n8.3\\nBibliographic Remarks\\nAccelerated rates of order O( 1\\nT 2 ) were obtained by Nemirovski as early as\\nthe late seventies. The ﬁrst practically eﬃcient accelerated algorithm is due\\nto Nesterov [56] , see also [57]. The simpliﬁed proof presented hereby is due\\nto [5].\\n\\nChapter 9\\nThe conditional gradient\\nmethod\\nIn many computational and learning scenarios the main bottleneck of opti-\\nmization, both online and oﬄine, is the computation of projections onto the\\nunderlying decision set (see §2.1.1). In this chapter we discuss projection-free\\nmethods in convex optimization, and some of their applications in machine\\nlearning.\\nThe motivating example throughout this chapter is the problem of ma-\\ntrix completion, which is a widely used and accepted model in the con-\\nstruction of recommendation systems. For matrix completion and related\\nproblems, projections amount to expensive linear algebraic operations and\\navoiding them is crucial in big data applications.\\nHenceforth we describe the conditional gradient algorithm, also known as\\nthe Frank-Wolfe algorithm. Afterwards, we describe problems for which lin-\\near optimization can be carried out much more eﬃciently than projections.\\nWe conclude with an application to exploration in reinforcement learning.\\n9.1\\nReview: relevant concepts from linear algebra\\nThis chapter addresses rectangular matrices, which model applications such\\nas recommendation systems naturally. Consider a matrix X ∈Rn×m. A\\nnon-negative number σ ∈R+ is said to be a singular value for X if there are\\ntwo vectors u ∈Rn, v ∈Rm such that\\nX⊤u = σv,\\nXv = σu.\\n85\\n\\n86\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nThe vectors u, v are called the left and right singular vectors respectively.\\nThe non-zero singular values are the square roots of the eigenvalues of the\\nmatrix XX⊤(and X⊤X). The matrix X can be written as\\nX = UΣV ⊤, U ∈Rn×ρ , V ⊤∈Rρ×m,\\nwhere ρ = min{n, m}, the matrix U is an orthogonal basis of the left singular\\nvectors of X, the matrix V is an orthogonal basis of right singular vectors,\\nand Σ is a diagonal matrix of singular values. This form is called the singular\\nvalue decomposition for X.\\nThe number of non-zero singular values for X is called its rank, which\\nwe denote by k ≤ρ. The nuclear norm of X is deﬁned as the ℓ1 norm of its\\nsingular values, and denoted by\\n∥X∥∗=\\nρ\\nX\\ni=1\\nσi\\nIt can be shown (see exercises) that the nuclear norm is equal to the trace\\nof the square root of the matrix times its transpose, i.e.,\\n∥X∥∗= Tr(\\n√\\nX⊤X)\\nWe denote by A • B the inner product of two matrices as vectors in Rn×m,\\nthat is\\nA • B =\\nn\\nX\\ni=1\\nm\\nX\\nj=1\\nAijBij = Tr(AB⊤)\\n9.2\\nMotivation: matrix completion and recommen-\\ndation systems\\nMedia recommendations have changed signiﬁcantly with the advent of the\\nInternet and rise of online media stores. The large amounts of data collected\\nallow for eﬃcient clustering and accurate prediction of users’ preferences\\nfor a variety of media.\\nA well-known example is the so called “Netﬂix\\nchallenge”—a competition of automated tools for recommendation from a\\nlarge dataset of users’ motion picture preferences.\\nOne of the most successful approaches for automated recommendation\\nsystems, as proven in the Netﬂix competition, is matrix completion. Perhaps\\nthe simplest version of the problem can be described as follows.\\nThe entire dataset of user-media preference pairs is thought of as a\\npartially-observed matrix. Thus, every person is represented by a row in\\n\\n9.2. MOTIVATION\\n87\\nthe matrix, and every column represents a media item (movie). For sim-\\nplicity, let us think of the observations as binary—a person either likes or\\ndislikes a particular movie. Thus, we have a matrix M ∈{0, 1, ∗}n×m where\\nn is the number of persons considered, m is the number of movies at our\\nlibrary, and 0/1 and ∗signify “dislike”, “like” and “unknown” respectively:\\nMij =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n0,\\nperson i dislikes movie j\\n1,\\nperson i likes movie j\\n∗,\\npreference unknown\\n.\\nThe natural goal is to complete the matrix, i.e. correctly assign 0 or 1 to\\nthe unknown entries. As deﬁned so far, the problem is ill-posed, since any\\ncompletion would be equally good (or bad), and no restrictions have been\\nplaced on the completions.\\nThe common restriction on completions is that the “true” matrix has\\nlow rank. Recall that a matrix X ∈Rn×m has rank k < ρ = min{n, m} if\\nand only if it can be written as\\nX = UV , U ∈Rn×k, V ∈Rk×m.\\nThe intuitive interpretation of this property is that each entry in M\\ncan be explained by only k numbers.\\nIn matrix completion this means,\\nintuitively, that there are only k factors that determine a persons preference\\nover movies, such as genre, director, actors and so on.\\nNow the simplistic matrix completion problem can be well-formulated\\nas in the following mathematical program. Denote by ∥· ∥OB the Euclidean\\nnorm only on the observed (non starred) entries of M, i.e.,\\n∥X∥2\\nOB =\\nX\\nMij̸=∗\\nX2\\nij.\\nThe mathematical program for matrix completion is given by\\nmin\\nX∈Rn×m\\n1\\n2∥X −M∥2\\nOB\\ns.t.\\nrank(X) ≤k.\\nSince the constraint over the rank of a matrix is non-convex, it is stan-\\ndard to consider a relaxation that replaces the rank constraint by the nuclear\\nnorm. It is known that the nuclear norm is a lower bound on the matrix\\n\\n88\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nrank if the singular values are bounded by one (see exercises). Thus, we\\narrive at the following convex program for matrix completion:\\nmin\\nX∈Rn×m\\n1\\n2∥X −M∥2\\nOB\\n(9.1)\\ns.t.\\n∥X∥∗≤k.\\nWe consider algorithms to solve this convex optimization problem next.\\n9.3\\nThe Frank-Wolfe method\\nIn this section we consider minimization of a convex function over a convex\\ndomain.\\nThe conditional gradient (CG) method, or Frank-Wolfe algorithm, is a\\nsimple algorithm for minimizing a smooth convex function f over a convex\\nset K ⊆Rn. The appeal of the method is that it is a ﬁrst order interior point\\nmethod - the iterates always lie inside the convex set, and thus no projections\\nare needed, and the update step on each iteration simply requires minimizing\\na linear objective over the set. The basic method is given in Algorithm 13.\\nAlgorithm 13 Conditional gradient\\n1: Input: step sizes {ηt ∈(0, 1], t ∈[T]}, initial point x1 ∈K.\\n2: for t = 1 to T do\\n3:\\nvt ←arg minx∈K\\n\\x08\\nx⊤∇f(xt)\\n\\t\\n.\\n4:\\nxt+1 ←xt + ηt(vt −xt).\\n5: end for\\nNote that in the CG method, the update to the iterate xt may be not be\\nin the direction of the gradient, as vt is the result of a linear optimization\\nprocedure in the direction of the negative gradient.\\nThis is depicted in\\nFigure 9.1.\\nThe following theorem gives an essentially tight performance guarantee\\nof this algorithm over smooth functions. Recall our notation from Chapter\\n2: x⋆denotes the global minimizer of f over K, D denotes the diameter of\\nthe set K, and ht = f(xt)−f(x⋆) denotes the suboptimality of the objective\\nvalue in iteration t.\\nTheorem 9.1. The CG algorithm applied to β-smooth functions with step\\nsizes ηt = min{2H\\nt , 1}, for H ≥max{1, h1}, attains the following conver-\\ngence guarantee:\\nht ≤2βHD2\\nt\\n\\n9.3. THE FRANK-WOLFE METHOD\\n89\\nFigure 9.1: Direction of progression of the conditional gradient algorithm.\\nProof. As done before in this manuscript, we denote ∇t = ∇f(xt), and also\\ndenote H ≥max{h1, 1}, such that ηt = min{2H\\nt , 1}. For any set of step\\nsizes, we have\\nf(xt+1) −f(x⋆) = f(xt + ηt(vt −xt)) −f(x⋆)\\n≤f(xt) −f(x⋆) + ηt(vt −xt)⊤∇t + η2\\nt\\nβ\\n2 ∥vt −xt∥2\\nβ-smoothness\\n≤f(xt) −f(x⋆) + ηt(x⋆−xt)⊤∇t + η2\\nt\\nβ\\n2 ∥vt −xt∥2\\nvt optimality\\n≤f(xt) −f(x⋆) + ηt(f(x⋆) −f(xt)) + η2\\nt\\nβ\\n2 ∥vt −xt∥2\\nconvexity of f\\n≤(1 −ηt)(f(xt) −f(x⋆)) + η2\\nt β\\n2 D2.\\n(9.2)\\n\\n90\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nWe reached the recursion ht+1 ≤(1 −ηt)ht + η2\\nt\\nβD2\\n2 , and by induction,\\nht+1 ≤(1 −ηt)ht + η2\\nt\\nβD2\\n2\\n≤(1 −ηt)2βHD2\\nt\\n+ η2\\nt\\nβD2\\n2\\ninduction hypothesis\\n≤(1 −2H\\nt )2βHD2\\nt\\n+ 4H2\\nt2\\nβD2\\n2\\nvalue of ηt\\n= 2βHD2\\nt\\n−2H2βD2\\nt2\\n≤2βHD2\\nt\\n(1 −1\\nt )\\nsince H ≥1\\n≤2βHD2\\nt + 1 .\\nt−1\\nt\\n≤\\nt\\nt+1\\n9.4\\nProjections vs. linear optimization\\nThe conditional gradient (Frank-Wolfe) algorithm described before does not\\nresort to projections, but rather computes a linear optimization problem of\\nthe form\\narg min\\nx∈K\\nn\\nx⊤u\\no\\n.\\n(9.3)\\nWhen is the CG method computationally preferable? The overall compu-\\ntational complexity of an iterative optimization algorithm is the product\\nof the number of iterations and the computational cost per iteration. The\\nCG method does not converge as well as the most eﬃcient gradient descent\\nalgorithms, meaning it requires more iterations to produce a solution of a\\ncomparable level of accuracy. However, for many interesting scenarios the\\ncomputational cost of a linear optimization step (9.3) is signiﬁcantly lower\\nthan that of a projection step.\\nLet us point out several examples of problems for which we have very eﬃ-\\ncient linear optimization algorithms, whereas our state-of-the-art algorithms\\nfor computing projections are signiﬁcantly slower.\\nRecommendation systems and matrix prediction.\\nIn the example\\npointed out in the preceding section of matrix completion, known methods\\n\\n9.4. PROJECTIONS VS. LINEAR OPTIMIZATION\\n91\\nfor projection onto the spectahedron, or more generally the bounded nuclear-\\nnorm ball, require singular value decompositions, which take superlinear\\ntime via our best known methods. In contrast, the CG method requires\\nmaximal eigenvector computations which can be carried out in linear time\\nvia the power method (or the more sophisticated Lanczos algorithm).\\nNetwork routing and convex graph problems.\\nVarious routing and\\ngraph problems can be modeled as convex optimization problems over a\\nconvex set called the ﬂow polytope.\\nConsider a directed acyclic graph with m edges, a source node marked\\ns and a target node marked t. Every path from s to t in the graph can be\\nrepresented by its identifying vector, that is a vector in {0, 1}m in which the\\nentries that are set to 1 correspond to edges of the path. The ﬂow polytope\\nof the graph is the convex hull of all such identifying vectors of the simple\\npaths from s to t. This polytope is also exactly the set of all unit s–t ﬂows\\nin the graph if we assume that each edge has a unit ﬂow capacity (a ﬂow\\nis represented here as a vector in Rm in which each entry is the amount of\\nﬂow through the corresponding edge).\\nSince the ﬂow polytope is just the convex hull of s–t paths in the graph,\\nminimizing a linear objective over it amounts to ﬁnding a minimum weight\\npath given weights for the edges. For the shortest path problem we have\\nvery eﬃcient combinatorial optimization algorithms, namely Dijkstra’s al-\\ngorithm.\\nThus, applying the CG algorithm to solve any convex optimization prob-\\nlem over the ﬂow polytope will only require iterative shortest path compu-\\ntations.\\nRanking and permutations.\\nA common way to represent a permutation\\nor ordering is by a permutation matrix.\\nSuch are square matrices over\\n{0, 1}n×n that contain exactly one 1 entry in each row and column.\\nDoubly-stochastic matrices are square, real-valued matrices with non-\\nnegative entries, in which the sum of entries of each row and each column\\namounts to 1.\\nThe polytope that deﬁnes all doubly-stochastic matrices\\nis called the Birkhoﬀ-von Neumann polytope. The Birkhoﬀ-von Neumann\\ntheorem states that this polytope is the convex hull of exactly all n × n\\npermutation matrices.\\nSince a permutation matrix corresponds to a perfect matching in a fully\\nconnected bipartite graph, linear minimization over this polytope corre-\\nsponds to ﬁnding a minimum weight perfect matching in a bipartite graph.\\n\\n92\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\nConsider a convex optimization problem over the Birkhoﬀ-von Neumann\\npolytope.\\nThe CG algorithm will iteratively solve a linear optimization\\nproblem over the BVN polytope, thus iteratively solving a minimum weight\\nperfect matching in a bipartite graph problem, which is a well-studied com-\\nbinatorial optimization problem for which we know of eﬃcient algorithms.\\nIn contrast, other gradient based methods will require projections, which\\nare quadratic optimization problems over the BVN polytope.\\nMatroid polytopes.\\nA matroid is pair (E, I) where E is a set of elements\\nand I is a set of subsets of E called the independent sets which satisfy vari-\\nous interesting proprieties that resemble the concept of linear independence\\nin vector spaces. Matroids have been studied extensively in combinatorial\\noptimization and a key example of a matroid is the graphical matroid in\\nwhich the set E is the set of edges of a given graph and the set I is the set of\\nall subsets of E which are cycle-free. In this case, I contains all the spanning\\ntrees of the graph. A subset S ∈I could be represented by its identifying\\nvector which lies in {0, 1}|E| which also gives rise to the matroid polytope\\nwhich is just the convex hull of all identifying vectors of sets in I. It can\\nbe shown that some matroid polytopes are deﬁned by exponentially many\\nlinear inequalities (exponential in |E|), which makes optimization over them\\ndiﬃcult.\\nOn the other hand, linear optimization over matroid polytopes is easy\\nusing a simple greedy procedure which runs in nearly linear time. Thus, the\\nCG method serves as an eﬃcient algorithm to solve any convex optimization\\nproblem over matroids iteratively using only a simple greedy procedure.\\n\\n9.5. EXERCISES\\n93\\n9.5\\nExercises\\n1. Prove that if the singular values are smaller than or equal to one, then\\nthe nuclear norm is a lower bound on the rank, i.e., show\\nrank(X) ≥∥X∥∗.\\n2. Prove that the trace is related to the nuclear norm via\\n∥X∥∗= Tr(\\n√\\nXX⊤) = Tr(\\n√\\nX⊤X).\\n3. Show that maximizing a linear function over the spectahedron is equiv-\\nalent to a maximal eigenvector computation. That is, show that the\\nfollowing mathematical program:\\nmin X • C\\nX ∈Sd = {X ∈Rd×d , X ≽0 , Tr(X) ≤1},\\nis equivalent to the following:\\nmin\\nx∈Rd x⊤Cx\\ns.t. ∥x∥2 ≤1.\\n4. Download the MovieLens dataset from the web. Implement an online\\nrecommendation system based on the matrix completion model: im-\\nplement the OCG and OGD algorithms for matrix completion. Bench-\\nmark your results.\\n\\n94\\nCHAPTER 9. THE CONDITIONAL GRADIENT METHOD\\n9.6\\nBibliographic Remarks\\nThe matrix completion model has been extremely popular since its inception\\nin the context of recommendation systems [80, 66, 69, 50, 14, 75].\\nThe conditional gradient algorithm was devised in the seminal paper\\nby Frank and Wolfe [21]. Due to the applicability of the FW algorithm to\\nlarge-scale constrained problems, it has been a method of choice in recent\\nmachine learning applications, to name a few: [42, 49, 41, 20, 30, 36, 72, 7,\\n82, 22, 23, 8].\\nThe online conditional gradient algorithm is due to [36]. An optimal\\nregret algorithm, attaining the O(\\n√\\nT) bound, for the special case of poly-\\nhedral sets was devised in [23].\\n\\nChapter 10\\nSecond order methods for\\nmachine learning\\nAt this point in our course, we have exhausted the main techniques in\\nﬁrst-order (or gradient-based) optimization.\\nWe have studied the main\\nworkhorse - stochastic gradient descent, the three acceleration techniques,\\nand projection-free gradient methods. Have we exhausted optimization for\\nML?\\nIn this section we discuss using higher derivatives of the objective func-\\ntion to accelerate optimization. The canonical method is Newton’s method,\\nwhich involves the second derivative or Hessian in high dimensions. The\\nvanilla approach is computationally expensive since it involves matrix inver-\\nsion in high dimensions that machine learning problems usually require.\\nHowever, recent progress in random estimators gives rise to linear-time\\nsecond order methods, for which each iteration is as computationally cheap\\nas gradient descent.\\n10.1\\nMotivating example: linear regression\\nIn the problem of linear regression we are given a set of measurements {ai ∈\\nRd, bi ∈R}, and the goal is to ﬁnd a set of weights that explains them best\\nin the mean squared error sense. As a mathematical program, the goal is to\\noptimize:\\nmin\\nx∈Rd\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1\\n2\\nX\\ni∈[m]\\n\\x10\\na⊤\\ni x −bi\\n\\x112\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe,\\n95\\n\\n96\\nCHAPTER 10. SECOND ORDER METHODS\\nor in matrix form,\\nmin\\nx f(x) =\\n\\x1a1\\n2∥Ax −b∥2\\n\\x1b\\n.\\nHere A ∈Rm×d, b ∈Rm. Notice that the objective function f is smooth,\\nbut not necessarily strongly convex. Therefore, all algorithms that we have\\nstudied so far without exception, which are all ﬁrst order methods, attain\\nrates which are poly( 1\\nε).\\nHowever, the linear regression problem has a closed form solution that\\ncan be computed by taking the gradient to be zero, i.e. (Ax −b)⊤A = 0,\\nwhich gives\\nx = (A⊤A)−1A⊤b.\\nThe Newton direction is given by the inverse Hessian multiplied by the\\ngradient, ∇−2f(x)∇f(x). Observe that a single Newton step, i.e. moving in\\nthe Newton direction with step size one, from any direction gets us directly\\nto the optimal solution in one iteration! (see exercises)\\nMore generally, Newton’s method yields O(log 1\\nε) convergence rates for\\na large class of functions without dependence on the condition number of\\nthe function! We study this property next.\\n10.2\\nSelf-Concordant Functions\\nIn this section we deﬁne and collect some of the properties of a special class\\nof functions, called self-concordant functions. These functions allow New-\\nton’s method to run in time which is independent of the condition number.\\nThe class of self-concordant functions is expressive and includes quadratic\\nfunctions, logarithms of inner products, a variety of barriers such as the log\\ndeterminant, and many more.\\nAn excellent reference for this material is the lecture notes on this subject\\nby Nemirovski [55]. We begin by deﬁning self-concordant functions.\\nDeﬁnition 10.1 (Self-Concordant Functions). Let K ⊆Rn be a non-empty\\nopen convex set, and and let f : K 7→R be a C3 convex function. Then, f\\nis said to be self-concordant if\\n|∇3f(x)[h, h, h]| ≤2(h⊤∇2f(x)h)3/2,\\nwhere we have\\n∇kf(x)[h1, . . . , hk] ≜\\n∂k\\n∂t1 . . . ∂tk\\n|t1=···=tkf(x + t1h1 + · · · + tkhk).\\n\\n10.2. NEWTON’S METHOD\\n97\\nAnother key object in the analysis of self concordant functions is the\\nnotion of a Dikin Ellipsoid, which is the unit ball around a point in the\\nnorm given by the Hessian ∥· ∥∇2f at the point. We will refer to this norm\\nas the local norm around a point and denote it as ∥· ∥x. Formally,\\nDeﬁnition 10.2 (Dikin ellipsoid). The Dikin ellipsoid of radius r centered\\nat a point x is deﬁned as\\nEr(x) ≜{y | ∥y −x∥∇2f(x) ≤r}\\nOne of the key properties of self-concordant functions that we use is that\\ninside the Dikin ellipsoid, the function is well conditioned with respect to\\nthe local norm at the center. The next lemma makes this formal. The proof\\nof this lemma can be found in [55].\\nLemma 10.3 (See [55]). For all h such that ∥h∥x < 1 we have that\\n(1 −∥h∥x)2∇2f(x) ⪯∇2f(x + h) ⪯\\n1\\n(1 −∥h∥x)2 ∇2f(x)\\nAnother key quantity, which is used both as a potential function as well\\nas a dampening for the step size in the analysis of Newton’s method, is the\\nNewton Decrement:\\nλx ≜∥∇f(x)∥∗\\nx =\\nq\\n∇f(x)⊤∇−2f(x)∇f(x).\\nThe following lemma quantiﬁes how λx behaves as a potential by showing\\nthat once it drops below 1, it ensures that the minimum of the function lies\\nin the current Dikin ellipsoid. This is the property which we use crucially\\nin our analysis. The proof can be found in [55].\\nLemma 10.4 (See [55]). If λx < 1 then\\n∥x −x∗∥x ≤\\nλx\\n1 −λx\\n10.3\\nNewton’s method for self-concordant func-\\ntions\\nBefore introducing the linear time second order methods, we start by intro-\\nducing a robust Newton’s method and its properties. The pseudo-code is\\ngiven in Algorithm 14.\\n\\n98\\nCHAPTER 10. SECOND ORDER METHODS\\nThe usual analysis of Newton’s method allows for quadratic convergence,\\ni.e. error ε in O(log log 1\\nε) iterations for convex objectives. However, we\\nprefer to present a version of Newton’s method which is robust to certain\\nrandom estimators of the Newton direction. This yields a slower rate of\\nO(log 1\\nε).\\nThe faster running time per iteration, which does not require\\nmatrix manipulations, more than makes up for this.\\nAlgorithm 14 Robust Newton’s method\\nInput: T, x1\\nfor t = 1 to T do\\nSet c = 1\\n8, η = min{c,\\nc\\n8λxt }. Let 1\\n2∇−2f(xt) ⪯˜\\n∇−2\\nt\\n⪯2∇−2f(xt).\\nxt+1 = xt −η ˜\\n∇−2\\nt ∇f(xt)\\nend for\\nreturn xT+1\\nIt is important to notice that every two consecutive points are within\\nthe same Dikin ellipsoid of radius 1\\n2. Denote ∇t = ∇xt, and similarly for\\nthe Hessian. Then we have:\\n∥xt −xt+1∥2\\nxt = η2∇⊤\\nt ˜\\n∇−2\\nt ∇2\\nt ˜\\n∇−2\\nt ∇t ≤4η2λ2\\nt ≤1\\n2.\\nThe advantage of Newton’s method as applied to self-concordant func-\\ntions is its linear convergence rate, as given in the following theorem.\\nTheorem 10.5. Let f be self-concordant, and f(x1) ≤M, then\\nht = f(xt) −f(x∗) ≤O(M + log 1\\nε)\\nThe proof of this theorem is composed of two steps, according to the\\nmagnitude of the Newton decrement.\\nPhase 1: damped Newton\\nLemma 10.6. As long as λx ≥1\\n8, we have that\\nht ≤−1\\n4c\\n\\n10.3. NEWTON’S METHOD FOR SELF-CONCORDANT FUNCTIONS99\\nProof. Using similar analysis to the descent lemma we have that\\nf(xt+1) −f(xt)\\n≤∇⊤\\nt (xt+1 −xt) + 1\\n2(xt −xt+1)⊤∇2(ζ)(xt −xt+1)\\nTaylor\\n≤∇⊤\\nt (xt+1 −xt) + 1\\n4(xt −xt+1)⊤∇2(xt)(xt −xt+1)\\nxt+1 ∈E1/2(xt)\\n= −η∇⊤\\nt ˜\\n∇−2\\nt ∇t + 1\\n4η2∇⊤\\nt ˜\\n∇−2\\nt ∇2\\nt ˜\\n∇−2\\nt ∇t\\n= −ηλ2\\nt + 1\\n4η2λ2\\nt ≤−1\\n16c\\nThe conclusion from this step is that after O(M) steps, Algorithm 14\\nreaches a point for which λx ≤1\\n8. According to Lemma 10.4, we also have\\nthat ∥x −x∗∥x ≤1\\n4, that is, the optimum is in the same Dikin ellipsoid as\\nthe current point.\\nPhase 2: pure Newton\\nIn the second phase our step size is changed to\\nbe larger. In this case, we are guaranteed that the Newton decrement is less\\nthan one, and thus we know that the global optimum is in the same Dikin\\nellipsoid as the current point. In this ellipsoid, all Hessians are equivalent\\nup to a factor of two, and thus Mirrored-Descent with the inverse Hessian\\nas preconditioner becomes gradient descent. We make this formal below.\\nAlgorithm 15 Preconditioned Gradient Descent\\nInput: P, T\\nfor t = 1 to T do\\nxt+1 = xt −ηP −1∇f(xt)\\nend for\\nreturn xT+1\\nLemma 10.7. Suppose that 1\\n2P ⪯∇2f(x) ⪯2P, and ∥x1 −x∗∥P ≤1\\n2, then\\nAlgorithm 15 converges as\\nht+1 ≤h1e−1\\n8 t.\\nThis theorem follows from noticing that the function g(z) = f(P −1/2x)\\nis 1\\n2-strongly convex and 2-smooth, and using Theorem 3.2. It can be shown\\nthat gradient descent on g is equivalent to Newton’s method in f. Details\\nare left as an exercise.\\nAn immediate corollary is that Newton’s method converges at a rate of\\nO(log 1\\nε) in this phase.\\n\\n100\\nCHAPTER 10. SECOND ORDER METHODS\\n10.4\\nLinear-time second-order methods\\nNewton’s algorithm is of foundational importance in the study of mathemat-\\nical programming in general. A major application are interior point methods\\nfor convex optimization, which are the most important polynomial-time al-\\ngorithms for general constrained convex optimization.\\nHowever, the main downside of this method is the need to maintain and\\nmanipulate matrices - namely the Hessians. This is completely impractical\\nfor machine learning applications in which the dimension is huge.\\nAnother signiﬁcant downside is the non-robust nature of the algorithm,\\nwhich makes applying it in stochastic environments challenging.\\nIn this section we show how to apply Newton’s method to machine\\nlearning problems.\\nThis involves relatively new developments that allow\\nfor linear-time per-iteration complexity, similar to SGD, and theoretically\\nsuperior running times.\\nAt the time of writing, however, these methods\\nare practical only for convex optimization, and have not shown superior\\nperformance on optimization tasks involving deep neural networks.\\nThe ﬁrst step to developing a linear time Newton’s method is an eﬃcient\\nstochastic estimator for the Newton direction, and the Hessian inverse.\\n10.4.1\\nEstimators for the Hessian Inverse\\nThe key idea underlying the construction is the following well known fact\\nabout the Taylor series expansion of the matrix inverse.\\nLemma 10.8. For a matrix A ∈Rd×d such that A ⪰0 and ∥A∥≤1, we\\nhave that\\nA−1 =\\n∞\\nX\\ni=0\\n(I −A)i\\nWe propose two unbiased estimators based on the above series. To deﬁne\\nthe ﬁrst estimator pick a probability distribution over non-negative integers\\n{pi} and sample ˆ\\ni from the above distribution. Let X1, . . . Xˆ\\ni be independent\\nsamples of the Hessian ∇2f and deﬁne the estimator as\\nDeﬁnition 10.9 (Estimator 1).\\n˜\\n∇−2f = 1\\npˆ\\ni\\nˆ\\ni\\nY\\nj=1\\n(I −Xj)\\n\\n10.4. LINEAR-TIME SECOND-ORDER METHODS\\n101\\nObserve that our estimator of the Hessian inverse is unbiased, i.e. E[ ˆ\\nX] =\\n∇−2f at any point. Estimator 1 has the disadvantage that in a single sample\\nit incorporates only one term of the Taylor series.\\nThe second estimator below is based on the observation that the above\\nseries has the following succinct recursive deﬁnition, and is more eﬃcient.\\nFor a matrix A deﬁne\\nA−1\\nj\\n=\\nj\\nX\\ni=0\\n(I −A)i\\ni.e. the ﬁrst j terms of the above Taylor expansion. It is easy to see that\\nthe following recursion holds for A−1\\nj\\nA−1\\nj\\n= I + (I −A)A−1\\nj−1\\nUsing the above recursive formulation, we now describe an unbiased\\nestimator of ∇−2f by deriving an unbiased estimator ˜\\n∇−2fj for ∇−2fj.\\nDeﬁnition 10.10 (Estimator 2). Given j independent and unbiased samples\\n{X1 . . . Xj} of the hessian ∇2f. Deﬁne { ˜\\n∇−2f0 . . . ˜\\n∇−2fj} recursively as\\nfollows\\n˜\\n∇−2f0 = I\\n˜\\n∇−2ft = I + (I −Xj) ˜\\n∇−2ft−1\\nIt can be readily seen that E[ ˜\\n∇−2fj] = ∇−2fj and therefore E[ ˜\\n∇−2fj] →\\n∇−2f as j →∞giving us an unbiased estimator in the limit.\\n10.4.2\\nIncorporating the estimator\\nBoth of the above estimators can be computed using only Hessian-vector\\nproducts, rather than matrix manipulations. For many machine learning\\nproblems, Hessian-vector products can be computed in linear time. Exam-\\nples include:\\n1. Convex regression and SVM objectives over training data have the\\nform\\nmin\\nw f(w) = E\\ni [ℓ(w⊤xi)],\\nwhere ℓis a convex function. The Hessian can thus be written as\\n∇2f(w) = E\\ni [ℓ′′(w⊤xi)xix⊤\\ni ]\\n\\n102\\nCHAPTER 10. SECOND ORDER METHODS\\nThus, the ﬁrst Newton direction estimator can now be written as\\n˜\\n∇2f(w)∇w = E\\nj∼D[\\nj\\nY\\ni=1\\n(I −ℓ′′(w⊤xi)xix⊤\\ni )]∇w.\\nNotice that this estimator can be computed using j vector-vector prod-\\nucts if the ordinal j was randomly chosen.\\n2. Non-convex optimization over neural networks: a similar derivation as\\nabove shows that the estimator can be computed only using Hessian-\\nvector products. The special structure of neural networks allow this\\ncomputation in a constant number of backpropagation steps, i.e. linear\\ntime in the network size, this is called the “Pearlmutter trick”, see [61].\\nWe note that non-convex optimization presents special challenges for\\nsecond order methods, since the Hessian need not be positive semi-\\ndeﬁnite.\\nNevertheless, the techniques presented hereby can still be\\nused to provide theoretical speedups for second order methods over\\nﬁrst order methods in terms of convergence to local minima.\\nThe\\ndetails are beyond our scope, and can be found in [2].\\nPutting everything together.\\nThese estimators we have studied can\\nbe used to create unbiased estimators to the Newton direction of the form\\n˜\\n∇−2\\nx ∇x for ˜\\n∇−2\\nx\\nwhich satisﬁes\\n1\\n2∇−2f(xt) ⪯˜\\n∇−2\\nt\\n⪯2∇−2f(xt).\\nThese can be incorporated into Algorithm 14, which we proved is capable\\nof obtaining fast convergence with approximate Newton directions of this\\nform.\\n\\n10.5. EXERCISES\\n103\\n10.5\\nExercises\\n1. Prove that a single Newton step for linear regression yields the optimal\\nsolution.\\n2. Let f : Rd 7→R, and consider the aﬃne transformation y = Ax, for\\nA ∈Rd×d being a symmetric matrix. Prove that\\nyt+1 ←yt −η∇f(yt)\\nis equivalent to\\nxt+1 ←xt −ηA−2∇f(xt).\\n3. Prove that the function g(z) deﬁned in phase 2 of the robust Newton\\nalgorithm is 1\\n2-strongly convex and 2-smooth. Conclude with a proof\\nof Theorem 10.7.\\n\\n104\\nCHAPTER 10. SECOND ORDER METHODS\\n10.6\\nBibliographic Remarks\\nThe modern application of Newton’s method to convex optimization was\\nput forth in the seminal work of Nesterov and Nemirovski [58] on interior\\npoint methods. A wonderful exposition is Nemirovski’s lecture notes [55].\\nThe fact that Hessian-vector products can be computed in linear time\\nfor feed forward neural networks was described in [61]. Linear time second\\norder methods for machine learning and the Hessian-vector product model\\nin machine learning was introduced in [4]. This was extended to non-convex\\noptimization for deep learning in [2].\\n\\nChapter 11\\nHyperparameter\\nOptimization\\nThus far in this class, we have been talking about continuous mathematical\\noptimization, where the search space of our optimization problem is continu-\\nous and mostly convex. For example, we have learned about how to optimize\\nthe weights of a deep neural network, which take continuous real values, via\\nvarious optimization algorithms (SGD, AdaGrad, Newton’s method, etc.).\\nHowever, in the process of training a neural network, there are some meta\\nparameters, which we call hyperparameters, that have a profound eﬀect on\\nthe ﬁnal outcome. These are global, mostly discrete, parameters that are\\ntreated diﬀerently by algorithm designers as well as by engineers. Examples\\ninclude the architecture of the neural network (number of layers, width of\\neach layer, type of activation function, ...), the optimization scheme for up-\\ndating weights (SGD/AdaGrad, initial learning rate, decay rate of learning\\nrate, momentum parameter, ...), and many more. Roughly speaking, these\\nhyperparameters are chosen before the training starts.\\nThe purpose of this chapter is to formalize this problem as an optimiza-\\ntion problem in machine learning, which requires a diﬀerent methodology\\nthan we have treated in the rest of this course. We remark that hyperpa-\\nrameter optimization is still an active area of research and its theoretical\\nproperties are not well understood as of this time.\\n11.1\\nFormalizing the problem\\nWhat makes hyperparameters diﬀerent from “regular” parameters?\\n105\\n\\n106\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\n1. The search space is often discrete (for example, number of layers). As\\nsuch, there is no natural notion of gradient or diﬀerentials and it is\\nnot clear how to apply the iterative methods we have studied thus far.\\n2. Even evaluating the objective function is extremely expensive (think\\nof evaluating the test error of the trained neural network). Thus it is\\ncrucial to minimize the number of function evaluations, whereas other\\ncomputations are signiﬁcantly less expensive.\\n3. Evaluating the function can be done in parallel. As an example, train-\\ning feedforward deep neural networks over diﬀerent architectures can\\nbe done in parallel.\\nMore formally, we consider the following optimization problem\\nmin\\nxi∈GF(qi)\\nf(x),\\nwhere x is the representation of discrete hyperparameters, each taking value\\nfrom qi ≥2 possible discrete values and thus in GF(q), the Galois ﬁeld of\\norder q. The example to keep in mind is that the objective f(x) is the test\\nerror of the neural network trained with hyperparameters x. Note that x\\nhas a search space of size Q\\ni qi ≥2n, exponentially large in the number of\\ndiﬀerent hyperparameters.\\n11.2\\nHyperparameter optimization algorithms\\nThe properties of the problem mentioned before prohibits the use of the\\nalgorithms we have studied thus far, which are all suitable for continuous\\noptimization. A naive method is to perform a grid search over all hyperpa-\\nrameters, but this quickly becomes infeasible. An emerging ﬁeld of research\\nin recent years, called AutoML, aims to choose hyperparameters automati-\\ncally. The following techniques are in common use:\\n• Grid search, try all possible assignments of hyperparameters and\\nreturn the best. This becomes infeasible very quickly with n - the\\nnumber of hyperparameters.\\n• Random search, where one randomly picks some choices of hyper-\\nparameters, evaluates their function objective, and chooses the one\\nchoice of hyperparameters giving best performance. An advantage of\\nthis method is that it is easy to implement in parallel.\\n\\n11.3. A SPECTRAL METHOD\\n107\\n• Successive Halving and Hyperband, random search combined\\nwith early stopping using multi-armed bandit techniques. These gain\\na small constant factor improvement over random search.\\n• Bayesian optimization, a statistical approach which has a prior over\\nthe objective and tries to iteratively pick an evaluation point which\\nreduces the variance in objective value.\\nFinally it picks the point\\nthat attains the lowest objective objective with highest conﬁdence.\\nThis approach is sequential in nature and thus diﬃcult to parallelize.\\nAnother important question is how to choose a good prior.\\nThe hyperparameter optimization problem is essentially a combinato-\\nrial optimization problem with exponentially large search space. Without\\nfurther assumptions, this optimization problem is information-theoretically\\nhard. Such assumptions are explored in the next section with an accompa-\\nnying algorithm.\\nFinally, we note that a simple but hard-to-beat benchmark is random\\nsearch with double budget. That is, compare the performance of a method\\nto that of random search, but allow random search double the query budget\\nof your own method.\\n11.3\\nA Spectral Method\\nFor simplicity, in this section we consider the case in which hyperparam-\\neters are binary. This retains the diﬃculty of the setting, but makes the\\nmathematical derivation simpler. The optimization problem now becomes\\nmin\\nx∈{−1,1}n\\nf(x).\\n(11.1)\\nThe method we describe in this section is inspired by the following key\\nobservation: although the whole search space of hyperparameters is exponen-\\ntially large, it is often the case in practice that only a few hyperparameters\\ntogether play a signiﬁcant role in the performance of a deep neural network.\\nTo make this intuition more precise, we need some deﬁnitions and facts\\nfrom Fourier analysis of Boolean functions.\\nFact 11.1. Any function f : {−1, 1}n →[−1, 1] can be uniquely represented\\nin the Fourier basis\\nf(x) =\\nX\\nS⊆[n]\\nαs ˆ\\nχS(x),\\n\\n108\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\nwhere each Fourier basis function\\nˆ\\nχS(x) =\\nY\\ni∈S\\nxi.\\nis a monomial, and thus f(x) has a polynomial representation.\\nNow we are ready to formalize our key observation in the following as-\\nsumption:\\nAssumption 11.2. The objective function f in the hyperparameter opti-\\nmization problem (11.1) is low degree and sparse in the Fourier basis, i.e.\\nf(x) ≈\\nX\\n|S|≤d\\nαS ˆ\\nχS(x),\\n∥α\\nα\\nα∥1 ≤k,\\n(11.2)\\nwhere d is the upper bound of polynomial degree, and k is the sparsity of\\nFourier coeﬃcient α\\nα\\nα (indexed by S) in ℓ1 sense (which is a convex relaxation\\nof ∥α\\nα\\nα∥0, the true sparsity).\\nRemark 11.3. Clearly this assumption does not always hold. For example,\\nmany deep reinforcement learning algorithms nowadays rely heavily on the\\nchoice of the random seed, which can also be seen as a hyperparameter. If\\nx ∈{−1, 1}32 is the bit representation of a int32 random seed, then there is\\nno reason to assume that a few of these bits should play a more signiﬁcant\\nrole than the others.\\nUnder this assumption, all we need to do now is to ﬁnd out the few im-\\nportant sets of variables S’s, as well as their coeﬃcients αS’s, in the approx-\\nimation (11.2). Fortunately, there is already a whole area of research, called\\ncompressed sensing, that aims to recover a high-dimensional but sparse vec-\\ntor, using only a few linear measurements. Next, we will brieﬂy introduce\\nthe problem of compressed sensing, and one useful result from the litera-\\nture. After that, we will introduce the Harmonica algorithm, which applies\\ncompressed sensing techniques to solve the hyperparameter optimization\\nproblem (11.1).\\n11.3.1\\nBackground: Compressed Sensing\\nThe problem of compressed sensing is as follows. Suppose there is a hidden\\nsignal x ∈Rn that we cannot observe. In order to recover x, we design a\\nmeasurement matrix A ∈Rm×n, and obtain noisy linear measurements y =\\nAx + η\\nη\\nη ∈Rm, where η\\nη\\nη is some random noise. The diﬃculty arises when we\\n\\n11.3. A SPECTRAL METHOD\\n109\\nhave a limited budget for measurements, i.e. m ≪n. Note that even without\\nnoise, recovering x is non-trivial since y = Ax is an underdetermined linear\\nsystem, therefore if there is one solution x that solves this linear system,\\nthere will be inﬁnitely many solutions. The key to this problem is to assume\\nthat x is k-sparse, that is, ∥x∥0 ≤k. This assumption has been justiﬁed\\nin various real-world applications; for example, natural images tend to be\\nsparse in the Fourier/wavelet domain, a property which forms the bases of\\nmany image compression algorithms.\\nUnder the assumption of sparsity, the natural way to recover x is to\\nsolve a least squares problem, subject to some sparsity constraint ∥x∥0 ≤k.\\nHowever, ℓ0 norm is diﬃcult to handle, and it is often replaced by ℓ1 norm,\\nits convex relaxation. One useful result from the literature of compressed\\nsensing is the following.\\nProposition 11.4 (Informal statement of Theorem 4.4 in [63]). Assume\\nthe ground-truth signal x ∈Rn is k-sparse.\\nThen, with high probability,\\nusing a randomly designed A ∈Rm×n that is “near-orthogonal” (random\\nGaussian matrix, subsampled Fourier basis, etc.), with m = O(k log(n)/ε)\\nand ∥η\\nη\\nη∥2 = O(√m), x can be recovered by a convex program\\nmin\\nz∈Rn ∥y −Az∥2\\n2\\ns.t.\\n∥z∥1 ≤k,\\n(11.3)\\nwith accuracy ∥x −z∥2 ≤ε.\\nThis result is remarkable; in particular, it says that the number of mea-\\nsurements needed to recover a sparse signal is independent of the dimension\\nn (up to a logarithm term), but only depends on the sparsity k and the\\ndesired accuracy ε. 1\\nRemark 11.5. The convex program (11.3) is equivalent to the following\\nLASSO problem\\nmin\\nz∈Rn ∥y −Az∥2\\n2 + λ∥z∥1,\\nwith a proper choice of regularization parameter λ. The LASSO problem\\nis an unconstrained convex program, and has eﬃcient solvers, as per the\\nalgorithms we have studied in this course.\\n1It also depends on the desired high-probability bound, which is omitted in this informal\\nstatement.\\n\\n110\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\n11.3.2\\nThe Spectral Algorithm\\nThe main idea is that, under Assumption 11.2, we can view the problem\\nof hyperparameter optimization as recovering the sparse signal α\\nα\\nα from lin-\\near measurements. More speciﬁcally, we need to query T random samples,\\nf(x1), . . . , f(xT ), and then solve the LASSO problem\\nmin\\nα\\nα\\nα\\nT\\nX\\nt=1\\n(\\nX\\n|S|≤d\\nαS ˆ\\nχS(xt) −f(xt))2 + λ∥α\\nα\\nα∥1,\\n(11.4)\\nwhere the regularization term λ∥α\\nα\\nα∥1 controls the sparsity of α\\nα\\nα. Also note\\nthat the constraint |S| ≤d not only implies that the solution is a low-degree\\npolynomial, but also helps to reduce the “eﬀective” dimension of α\\nα\\nα from 2n\\nto O(nd), which makes it feasible to solve this LASSO problem.\\nDenote by S1, . . . , Ss the indices of the s largest coeﬃcients of the LASSO\\nsolution, and deﬁne\\ng(x) =\\nX\\ni∈[s]\\nαSi ˆ\\nχSi(x),\\nwhich involves only a few dimensions of x since the LASSO solution is sparse\\nand low-degree.\\nThe next step is to set the variables outside ∪i∈[s]Si to\\narbitrary values, and compute a minimizer x∗∈arg min g(x).\\nIn other\\nwords, we have reduced the original problem of optimizing f(x) over n\\nvariables, to the problem of optimizing g(x) (an approximation of f(x))\\nover only a few variables (which is now feasible to solve). One remarkable\\nfeature of this algorithm is that the returned solution x∗may not belong to\\nthe samples {x1, . . . , xT }, which is not the case for other existing methods\\n(such as random search).\\nUsing theoretical results from compressed sensing (e.g.\\nProposition\\n11.4), we can derive the following guarantee for the sparse recovery of α\\nα\\nα\\nvia LASSO.\\nTheorem 11.6 (Informal statement of Lemma 7 in [38]). Assume f is k-\\nsparse in the Fourier expansion. Then, with T = O(k2 log(n)/ε) samples,\\nthe solution of the LASSO problem (11.4) achieves ε accuracy.\\nFinally, the above derivation can be considered as only one stage in a\\nmulti-stage process, each iteratively setting the value of a few more variables\\nthat are the most signiﬁcant.\\n\\n11.4. BIBLIOGRAPHIC REMARKS\\n111\\n11.4\\nBibliographic Remarks\\nFor a nice exposition on hyperparameter optimization see [64, 65], in which\\nthe the benchmark of comparing to Random Search with double queries was\\nproposed.\\nPerhaps the simplest approach to HPO is random sampling of diﬀerent\\nchoices of parameters and picking the best amongst the chosen evaluations\\n[9].\\nSuccessive Halving (SH) algorithm was introduced [43].\\nHyperband\\nfurther improves SH by automatically tuning the hyperparameters in SH\\n[51].\\nThe Bayesian optimization (BO) methodology is currently the most stud-\\nied in HPO. For recent studies and algorithms of this ﬂavor see [10, 78, 81,\\n79, 24, 84, 40].\\nThe spectral approach for hyperparameter optimization was introduced\\nin [38]. For an in-depth treatment of compressed sensing see the survey of\\n[63], and for Fourier analysis of Boolean functions see [59].\\n\\n112\\nCHAPTER 11. HYPERPARAMETER OPTIMIZATION\\n\\nBibliography\\n[1] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin.\\nCompeting\\nin the dark: An eﬃcient algorithm for bandit linear optimization. In\\nProceedings of the 21st Annual Conference on Learning Theory, pages\\n263–274, 2008.\\n[2] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and\\nTengyu Ma. Finding approximate local minima faster than gradient\\ndescent. In Proceedings of the 49th Annual ACM SIGACT Symposium\\non Theory of Computing, pages 1195–1199. ACM, 2017.\\n[3] Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh,\\nCyril Zhang, and Yi Zhang. The case for full-matrix adaptive regular-\\nization. arXiv preprint arXiv:1806.02958, 2018.\\n[4] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochas-\\ntic optimization for machine learning in linear time. The Journal of\\nMachine Learning Research, 18(1):4148–4187, 2017.\\n[5] Zeyuan Allen-Zhu and Lorenzo Orecchia.\\nLinear coupling:\\nAn ul-\\ntimate uniﬁcation of gradient and mirror descent.\\narXiv preprint\\narXiv:1407.1537, 2014.\\n[6] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-\\neﬃcient adaptive optimization for large-scale learning. arXiv preprint\\narXiv:1901.11150, 2019.\\n[7] Francis Bach, Simon Lacoste-Julien, and Guillaume Obozinski.\\nOn\\nthe equivalence between herding and conditional gradient algorithms.\\nIn John Langford and Joelle Pineau, editors, Proceedings of the 29th\\nInternational Conference on Machine Learning (ICML-12), ICML ’12,\\npages 1359–1366, New York, NY, USA, July 2012. Omnipress.\\n113\\n\\n114\\nBIBLIOGRAPHY\\n[8] Aur´\\nelien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-\\nFlorina Balcan, and Fei Sha.\\nDistributed frank-wolfe algorithm: A\\nuniﬁed framework for communication-eﬃcient sparse learning. CoRR,\\nabs/1404.2644, 2014.\\n[9] James Bergstra and Yoshua Bengio.\\nRandom search for hyper-\\nparameter optimization. J. Mach. Learn. Res., 13:281–305, February\\n2012.\\n[10] James S. Bergstra, R´\\nemi Bardenet, Yoshua Bengio, and Bal´\\nazs K´\\negl.\\nAlgorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. S.\\nZemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Ad-\\nvances in Neural Information Processing Systems 24, pages 2546–2554.\\nCurran Associates, Inc., 2011.\\n[11] J.M. Borwein and A.S. Lewis. Convex Analysis and Nonlinear Opti-\\nmization: Theory and Examples. CMS Books in Mathematics. Springer,\\n2006.\\n[12] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Uni-\\nversity Press, March 2004.\\n[13] S´\\nebastien Bubeck. Convex optimization: Algorithms and complexity.\\nFoundations and Trends in Machine Learning, 8(3–4):231–357, 2015.\\n[14] E. Candes and B. Recht. Exact matrix completion via convex optimiza-\\ntion. Foundations of Computational Mathematics, 9:717–772, 2009.\\n[15] Nicol`\\no Cesa-Bianchi and G´\\nabor Lugosi.\\nPrediction, Learning, and\\nGames. Cambridge University Press, 2006.\\n[16] Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang.\\nExtreme tensoring for low-memory preconditioning.\\narXiv preprint\\narXiv:1902.04620, 2019.\\n[17] Qi Deng, Yi Cheng, and Guanghui Lan. Optimal adaptive and accel-\\nerated stochastic gradient descent. arXiv preprint arXiv:1810.00553,\\n2018.\\n[18] John Duchi, Elad Hazan, and Yoram Singer.\\nAdaptive subgradient\\nmethods for online learning and stochastic optimization. The Journal\\nof Machine Learning Research, 12:2121–2159, 2011.\\n\\nBIBLIOGRAPHY\\n115\\n[19] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient\\nmethods for online learning and stochastic optimization. In COLT 2010\\n- The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29,\\n2010, pages 257–269, 2010.\\n[20] Miroslav Dud´\\nık, Za¨\\nıd Harchaoui, and J´\\nerˆ\\nome Malick. Lifted coordinate\\ndescent for learning with trace-norm regularization. Journal of Machine\\nLearning Research - Proceedings Track, 22:327–336, 2012.\\n[21] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval\\nResearch Logistics Quarterly, 3:149–154, 1956.\\n[22] Dan Garber and Elad Hazan. Approximating semideﬁnite programs in\\nsublinear time. In NIPS, pages 1080–1088, 2011.\\n[23] Dan Garber and Elad Hazan.\\nPlaying non-linear games with linear\\noracles. In FOCS, pages 420–428, 2013.\\n[24] Jacob R. Gardner, Matt J. Kusner, Zhixiang Eddie Xu, Kilian Q. Wein-\\nberger, and John P. Cunningham. Bayesian optimization with inequal-\\nity constraints.\\nIn Proceedings of the 31th International Conference\\non Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014,\\npages 937–945, 2014.\\n[25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning.\\nMIT Press, 2016. http://www.deeplearningbook.org.\\n[26] A .J. Grove, N. Littlestone, and D. Schuurmans. General convergence\\nresults for linear discriminant updates. Machine Learning, 43(3):173–\\n210, 2001.\\n[27] Vineet Gupta, Tomer Koren, and Yoram Singer. A uniﬁed approach\\nto adaptive regularization in online and stochastic optimization. arXiv\\npreprint arXiv:1706.06569, 2017.\\n[28] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Precondi-\\ntioned stochastic tensor optimization. arXiv preprint arXiv:1802.09568,\\n2018.\\n[29] James Hannan. Approximation to bayes risk in repeated play. In M.\\nDresher, A. W. Tucker, and P. Wolfe, editors, Contributions to the\\nTheory of Games, volume 3, pages 97–139, 1957.\\n\\n116\\nBIBLIOGRAPHY\\n[30] Za¨\\nıd Harchaoui, Matthijs Douze, Mattis Paulin, Miroslav Dud´\\nık, and\\nJ´\\nerˆ\\nome Malick. Large-scale image classiﬁcation with trace-norm regu-\\nlarization. In CVPR, pages 3386–3393, 2012.\\n[31] Elad Hazan. Introduction to online convex optimization. Foundations\\nand Trends ˆ\\nA R\\n⃝in Optimization, 2(3-4):157–325, 2016.\\n[32] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret al-\\ngorithms for online convex optimization. In Machine Learning, volume\\n69(2–3), pages 169–192, 2007.\\n[33] Elad Hazan and Sham Kakade. Revisiting the polyak step size. arXiv\\npreprint arXiv:1905.00313, 2019.\\n[34] Elad Hazan and Satyen Kale. Extracting certainty from uncertainty:\\nRegret bounded by variation in costs. In The 21st Annual Conference\\non Learning Theory (COLT), pages 57–68, 2008.\\n[35] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\\nan optimal algorithm for stochastic strongly-convex optimization. Jour-\\nnal of Machine Learning Research - Proceedings Track, pages 421–436,\\n2011.\\n[36] Elad Hazan and Satyen Kale. Projection-free online learning. In ICML,\\n2012.\\n[37] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier:\\noptimal algorithms for stochastic strongly-convex optimization.\\nThe\\nJournal of Machine Learning Research, 15(1):2489–2512, 2014.\\n[38] Elad Hazan, Adam Klivans, and Yang Yuan. Hyperparameter opti-\\nmization: A spectral approach. ICLR, 2018.\\n[39] Geoﬀrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural net-\\nworks for machine learning lecture 6a overview of mini-batch gradient\\ndescent. Cited on, 14, 2012.\\n[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\\nmaker.\\nEﬃcient hyperparameter optimization for deep learning al-\\ngorithms using deterministic RBF surrogates.\\nIn Proceedings of the\\nThirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9,\\n2017, San Francisco, California, USA., pages 822–829, 2017.\\n\\nBIBLIOGRAPHY\\n117\\n[41] Martin Jaggi.\\nRevisiting frank-wolfe: Projection-free sparse convex\\noptimization. In ICML, 2013.\\n[42] Martin Jaggi and Marek Sulovsk´\\ny. A simple algorithm for nuclear norm\\nregularized problems. In ICML, pages 471–478, 2010.\\n[43] Kevin G. Jamieson and Ameet Talwalkar.\\nNon-stochastic best arm\\nidentiﬁcation and hyperparameter optimization. In Proceedings of the\\n19th International Conference on Artiﬁcial Intelligence and Statistics,\\nAISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 240–248, 2016.\\n[44] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent\\nusing predictive variance reduction. In Advances in neural information\\nprocessing systems, pages 315–323, 2013.\\n[45] Adam Kalai and Santosh Vempala. Eﬃcient algorithms for online de-\\ncision problems. Journal of Computer and System Sciences, 71(3):291–\\n307, 2005.\\n[46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\n[47] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient ver-\\nsus gradient descent for linear predictors. Inf. Comput., 132(1):1–63,\\n1997.\\n[48] Jyrki Kivinen and Manfred K. Warmuth.\\nRelative loss bounds for\\nmultidimensional regression problems. Machine Learning, 45(3):301–\\n329, 2001.\\n[49] Simon Lacoste-Julien, Martin Jaggi, Mark W. Schmidt, and Patrick\\nPletscher.\\nBlock-coordinate frank-wolfe optimization for structural\\nsvms. In Proceedings of the 30th International Conference on Machine\\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 53–\\n61, 2013.\\n[50] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. A. Tropp. Prac-\\ntical large-scale optimization for max-norm regularization. In NIPS,\\npages 1297–1305, 2010.\\n[51] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.\\nHyperband: A Novel Bandit-Based Approach to Hyperparameter Op-\\ntimization. ArXiv e-prints, March 2016.\\n\\n118\\nBIBLIOGRAPHY\\n[52] H. Brendan McMahan and Matthew J. Streeter. Adaptive bound op-\\ntimization for online convex optimization. In COLT 2010 - The 23rd\\nConference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages\\n244–256, 2010.\\n[53] Arkadi S. Nemirovski and David B. Yudin. Problem Complexity and\\nMethod Eﬃciency in Optimization. John Wiley UK/USA, 1983.\\n[54] A.S. Nemirovskii. Interior point polynomial time methods in convex\\nprogramming, 2004. Lecture Notes.\\n[55] AS Nemirovskii.\\nInterior point polynomial time methods in convex\\nprogramming. Lecture Notes, 2004.\\n[56] Y. Nesterov. A method of solving a convex programming problem with\\nconvergence rate O(1/k2). Soviet Mathematics Doklady, 27(2):372–376,\\n1983.\\n[57] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic\\nCourse. Applied Optimization. Springer, 2004.\\n[58] Y. E. Nesterov and A. S. Nemirovskii. Interior Point Polynomial Al-\\ngorithms in Convex Programming. SIAM, Philadelphia, 1994.\\n[59] Ryan O’Donnell. Analysis of Boolean Functions. Cambridge University\\nPress, New York, NY, USA, 2014.\\n[60] Francesco Orabona and Koby Crammer. New adaptive algorithms for\\nonline classiﬁcation. In Proceedings of the 24th Annual Conference on\\nNeural Information Processing Systems 2010., pages 1840–1848, 2010.\\n[61] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural\\ncomputation, 6(1):147–160, 1994.\\n[62] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.\\nMaking\\ngradient descent optimal for strongly convex stochastic optimization.\\nIn ICML, 2012.\\n[63] Holger Rauhut. Compressive sensing and structured random matrices.\\nTheoretical foundations and numerical methods for sparse recovery, 9:1–\\n92, 2010.\\n[64] Benjamin Recht. Embracing the random. http://www.argmin.net/\\n2016/06/23/hyperband/, 2016.\\n\\nBIBLIOGRAPHY\\n119\\n[65] Benjamin Recht. The news on auto-tuning. http://www.argmin.net/\\n2016/06/20/hypertuning/, 2016.\\n[66] Jasson D. M. Rennie and Nathan Srebro. Fast maximum margin matrix\\nfactorization for collaborative prediction. In Proceedings of the 22Nd\\nInternational Conference on Machine Learning, ICML ’05, pages 713–\\n719, New York, NY, USA, 2005. ACM.\\n[67] Herbert Robbins and Sutton Monro.\\nA stochastic approximation\\nmethod. The Annals of Mathematical Statistics, 22(3):400–407, 09 1951.\\n[68] R.T. Rockafellar. Convex Analysis. Convex Analysis. Princeton Uni-\\nversity Press, 1997.\\n[69] R. Salakhutdinov and N. Srebro.\\nCollaborative ﬁltering in a non-\\nuniform world: Learning with the weighted trace norm. In NIPS, pages\\n2056–2064, 2010.\\n[70] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing ﬁnite\\nsums with the stochastic average gradient. Mathematical Programming,\\n162(1-2):83–112, 2017.\\n[71] Shai Shalev-Shwartz. Online Learning: Theory, Algorithms, and Ap-\\nplications. PhD thesis, The Hebrew University of Jerusalem, 2007.\\n[72] Shai Shalev-Shwartz, Alon Gonen, and Ohad Shamir. Large-scale con-\\nvex minimization with a low-rank constraint. In ICML, pages 329–336,\\n2011.\\n[73] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of\\nonline learning algorithms. Machine Learning, 69(2-3):115–142, 2007.\\n[74] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cot-\\nter.\\nPegasos: primal estimated sub-gradient solver for svm.\\nMath.\\nProgram., 127(1):3–30, 2011.\\n[75] O. Shamir and S. Shalev-Shwartz.\\nCollaborative ﬁltering with the\\ntrace norm: Learning, bounding, and transducing. JMLR - Proceedings\\nTrack, 19:661–678, 2011.\\n[76] Ohad Shamir and Tong Zhang.\\nStochastic gradient descent for\\nnon-smooth optimization: Convergence results and optimal averaging\\nschemes. In ICML, 2013.\\n\\n120\\nBIBLIOGRAPHY\\n[77] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates\\nwith sublinear memory cost. arXiv preprint arXiv:1804.04235, 2018.\\n[78] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian\\noptimization of machine learning algorithms. In Advances in Neural\\nInformation Processing Systems 25: 26th Annual Conference on Neural\\nInformation Processing Systems 2012. Proceedings of a meeting held\\nDecember 3-6, 2012, Lake Tahoe, Nevada, United States., pages 2960–\\n2968, 2012.\\n[79] Jasper Snoek, Kevin Swersky, Richard S. Zemel, and Ryan P. Adams.\\nInput warping for bayesian optimization of non-stationary functions. In\\nProceedings of the 31th International Conference on Machine Learning,\\nICML 2014, Beijing, China, 21-26 June 2014, pages 1674–1682, 2014.\\n[80] Nathan Srebro. Learning with Matrix Factorizations. PhD thesis, Mas-\\nsachusetts Institute of Technology, 2004.\\n[81] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task\\nbayesian optimization. In Advances in Neural Information Processing\\nSystems 26: 27th Annual Conference on Neural Information Processing\\nSystems 2013. Proceedings of a meeting held December 5-8, 2013, Lake\\nTahoe, Nevada, United States., pages 2004–2012, 2013.\\n[82] Ambuj Tewari, Pradeep D. Ravikumar, and Inderjit S. Dhillon. Greedy\\nalgorithms for structurally constrained high dimensional problems. In\\nNIPS, pages 882–890, 2011.\\n[83] A. M. Turing.\\nComputing machinery and intelligence.\\nMind,\\n59(236):433–460, 1950.\\n[84] Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando\\nde Freitas. Bayesian optimization in high dimensions via random em-\\nbeddings. In IJCAI 2013, Proceedings of the 23rd International Joint\\nConference on Artiﬁcial Intelligence, Beijing, China, August 3-9, 2013,\\npages 1778–1784, 2013.\\n[85] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp\\nconvergence over nonconvex landscapes, from any initialization. arXiv\\npreprint arXiv:1806.01811, 2018.\\n[86] Lijun Zhang, Mehrdad Mahdavi, and Rong Jin. Linear convergence\\nwith condition number independent access of full gradients. In Advances\\nin Neural Information Processing Systems, pages 980–988, 2013.\\n\\nBIBLIOGRAPHY\\n121\\n[87] Martin Zinkevich.\\nOnline convex programming and generalized in-\\nﬁnitesimal gradient ascent.\\nIn Proceedings of the 20th International\\nConference on Machine Learning, pages 928–936, 2003.\\n'},\n",
       " {'title': 'An Optimal Control View of Adversarial Machine Learning',\n",
       "  'summary': \"I describe an optimal control view of adversarial machine learning, where the\\ndynamical system is the machine learner, the input are adversarial actions, and\\nthe control costs are defined by the adversary's goals to do harm and be hard\\nto detect. This view encompasses many types of adversarial machine learning,\\nincluding test-item attacks, training-data poisoning, and adversarial reward\\nshaping. The view encourages adversarial machine learning researcher to utilize\\nadvances in control theory and reinforcement learning.\",\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1811.04422v1',\n",
       "  'authors': ['Xiaojin Zhu'],\n",
       "  'text': 'arXiv:1811.04422v1  [cs.LG]  11 Nov 2018\\nAn Optimal Control View of Adversarial Machine Learning\\nXiaojin Zhu\\nDepartment of Computer Sciences, University of Wisconsin-Madison\\nAbstract\\nI describe an optimal control view of adversarial machine learning, where the dynamical system is the\\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\\ngoals to do harm and be hard to detect. This view encompasses many types of adversarial machine\\nlearning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view\\nencourages adversarial machine learning researcher to utilize advances in control theory and reinforcement\\nlearning.\\n1\\nAdversarial Machine Learning is not Machine Learning\\nMachine learning has its mathematical foundation in concentration inequalities. This is a consequence of\\nthe independent and identically-distributed (i.i.d.) data assumption. In contrast, I suggest that adversarial\\nmachine learning may adopt optimal control as its mathematical foundation [3,25]. There are telltale signs:\\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. structures – as control input might be.\\n2\\nOptimal Control\\nI will focus on deterministic discrete-time optimal control because it matches many existing adversarial\\nattacks. Extensions to stochastic and continuous control are relevant to adversarial machine learning, too.\\nThe system to be controlled is called the plant, which is deﬁned by the system dynamics:\\nxt+1 = f(xt, ut)\\n(1)\\nwhere xt ∈Xt is the state of the system, ut ∈Ut is the control input, and Ut is the control constraint\\nset. The function f deﬁnes the evolution of state under external control. The time index t ranges from 0\\nto T −1, and the time horizon T can be ﬁnite or inﬁnite. The quality of control is speciﬁed by the running\\ncost:\\ngt(xt, ut)\\n(2)\\nwhich deﬁnes the step-by-step control cost, and the terminal cost for ﬁnite horizon:\\ngT (xT )\\n(3)\\nwhich deﬁnes the quality of the ﬁnal state. The optimal control problem is to ﬁnd control inputs u0 . . . uT −1\\nin order to minimize the objective:\\nmin\\nu0...uT −1\\ngT (xT ) +\\nT −1\\nX\\nt=0\\ngt(xt, ut)\\n(4)\\ns.t.\\nxt+1 = f(xt, ut), ut ∈Ut, ∀t\\nx0 given\\n1\\n\\nMore generally, the controller aims to ﬁnd control policies φt(xt) = ut, namely functions that map observed\\nstates to inputs. In optimal control the dynamics f is known to the controller. There are two styles of\\nsolutions: dynamic programming and Pontryagin minimum principle [2,10,17]. When f is not fully known,\\nthe problem becomes either robust control where control is carried out in a minimax fashion to accommodate\\nthe worst case dynamics [28], or reinforcement learning where the controller probes the dynamics [23].\\n3\\nAdversarial Machine Learning as Control\\nNow let us translate adversarial machine learning into a control formulation. Adversarial machine learning\\nstudies vulnerability throughout the learning pipeline [4, 13, 20, 26]. As examples, I present training-data\\npoisoning, test-time attacks, and adversarial reward shaping below. In all cases, the adversary attempts to\\ncontrol the machine learning system, and the control costs reﬂect the adversary’s desire to do harm and be\\nhard to detect.\\nUnfortunately, the notations from the control community and the machine learning community clash.\\nFor example, x denotes the state in control but the feature vector in machine learning. I will use the machine\\nlearning convention below.\\n3.1\\nTraining-Data Poisoning\\nIn training-data poisoning the adversary can modify the training data. The machine learner then trains a\\n“wrong” model from the poisoned data. The adversary’s goal is for the “wrong” model to be useful for some\\nnefarious purpose. I use supervised learning for illustration.\\n3.1.1\\nBatch Learner\\nAt this point, it becomes useful to distinguish batch learning and sequential (online) learning. If the machine\\nlearner performs batch learning, then the adversary has a degenerate one-step control problem. One-step\\ncontrol has not been the focus of the control community and there may not be ample algorithmic solutions to\\nborrow from. Still, it is illustrative to pose batch training set poisoning as a control problem. I use Support\\nVector Machine (SVM) with a batch training set as an example below:\\n• The state is the learner’s model h : X 7→Y. For instance, for SVM h is the classiﬁer parametrized by\\na weight vector w. I will use h and w interchangeably.\\n• The control u0 is a whole training set, for instance u0 = {(xi, yi)}1:n.\\n• The control constraint set U0 consists of training sets available to the adversary; if the adversary\\ncan arbitrary modify a training set for supervised learning (including changing features and labels,\\ninserting and deleting items), this could be U0 = ∪∞\\nn=0(X × Y)n, namely all training sets of all sizes.\\nThis is a large control space.\\n• The system dynamics (1) is deﬁned by the learner’s learning algorithm. For the SVM learner, this\\nwould be empirical risk minimization with hinge loss ℓ() and a regularizer:\\nw1 = f(u0) ∈argminw\\nn\\nX\\ni=1\\nℓ(w, xi, yi) + λ∥w∥2.\\n(5)\\nThe batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf() if it knows the form (5), ℓ(), and the value of λ.\\n• The time horizon T = 1.\\n2\\n\\n• The adversary’s running cost g0(u0) measures the poisoning eﬀort in preparing the training set u0.\\nThis is typically deﬁned with respect to a given “clean” data set ˜\\nu before poisoning in the form of\\ng0(u0) = distance(u0, ˜\\nu).\\n(6)\\nThe running cost is domain dependent. For example, the distance function may count the number of\\nmodiﬁed training items; or sum up the Euclidean distance of changes in feature vectors.\\n• The adversary’s terminal cost g1(w1) measures the lack of intended harm. The terminal cost is also\\ndomain dependent. For example:\\n– If the adversary must force the learner into exactly arriving at some target model w∗, then\\ng1(w1) = I∞[w1 ̸= w∗].\\nHere Iy[z] = y if z is true and 0 otherwise, which acts as a hard\\nconstraint.\\n– If the adversary only needs the learner to get near w∗then g1(w1) = ∥w1 −w∗∥for some norm.\\n– If the adversary wants to ensure that a speciﬁc future item x∗is classiﬁed ǫ-conﬁdently as positive,\\nit can use g1(w1) = I∞[w1 /\\n∈W∗] with the target set W∗= {w : w⊤x∗≥ǫ}. More generally,\\nW∗can be a polytope deﬁned by multiple future classiﬁcation constraints.\\nWith these deﬁnitions, the adversary’s one-step control problem (4) specializes to\\nmin\\nu0\\ng1(w1) + g0(w0, u0)\\n(7)\\ns.t.\\nw1 = f(w0, u0)\\nUnsurprisingly, the adversary’s one-step control problem is equivalent to a Stackelberg game and bi-level\\noptimization (the lower level optimization is hidden in f), a well-known formulation for training-data poi-\\nsoning [12,21].\\n3.1.2\\nSequential Learner\\nThe adversary performs classic discrete-time control if the learner is sequential:\\n• The learner starts from an initial model w0, which is the initial state.\\n• The control input at time t is ut = (xt, yt), namely the tth training item for t = 0, 1, . . .\\n• The dynamics is the sequential update algorithm of the learner. For example, the learner may perform\\none step of gradient descent:\\nwt+1 = f(wt, ut) = wt −ηt∇ℓ(wt, xt, yt).\\n(8)\\n• The adversary’s running cost gt(wt, ut) typically measures the eﬀort of preparing ut. For example, it\\ncould measure the magnitude of change ∥ut −˜\\nut∥with respect to a “clean” reference training sequence\\n˜\\nu. Or it could be the constant 1 which reﬂects the desire to have a short control sequence.\\n• The adversary’s terminal cost gT (wT ) is the same as in the batch case.\\nThe problem (4) then produces the optimal training sequence poisoning. Earlier attempts on sequential\\nteaching can be found in [1,18,19].\\n3\\n\\n3.2\\nTest-Time Attack\\nTest-time attack diﬀers from training-data poisoning in that a machine learning model h : X 7→Y is already-\\ntrained and given. Also given is a “test item” x. There are several variants of test-time attacks, I use the\\nfollowing one for illustration: The adversary seeks to minimally perturb x into x′ such that the machine\\nlearning model classiﬁes x and x′ diﬀerently. That is,\\nmin\\nx′\\ndistance(x, x′)\\n(9)\\ns.t.\\nh(x) ̸= h(y).\\nThe distance function is domain-dependent, though in practice the adversary often uses a mathematically\\nconvenient surrogate such as some p-norm ∥x −x′∥p.\\nOne way to formulate test-time attack as optimal control is to treat the test-item itself as the state, and\\nthe adversarial actions as control input. Let us ﬁrst look at the popular example of test-time attack against\\nimage classiﬁcation:\\n• Let the initial state x0 = x be the clean image.\\n• The adversary’s control input u0 is the vector of pixel value changes.\\n• The control constraint set is U0 = {u : x0 + u ∈[0, 1]d} to ensure that the modiﬁed image has valid\\npixel values (assumed to be normalized in [0, 1]).\\n• The dynamical system is trivially vector addition: x1 = f(x0, u0) = x0 + u0.\\n• The adversary’s running cost is g0(x0, u0) = distance(x0, x1).\\n• The adversary’s terminal cost is g1(x1) = I∞[h(x1) = h(x0)]. Note the machine learning model h is\\nonly used to deﬁne the hard constraint terminal cost; h itself is not modiﬁed.\\nWith these deﬁnitions this is a one-step control problem (4) that is equivalent to the test-time attack\\nproblem (9).\\nThis control view on test-time attack is more interesting when the adversary’s actions are sequential\\nU0, U1, . . ., and the system dynamics render the action sequence non-commutative. The adversary’s running\\ncost gt then measures the eﬀort in performing the action at step t. One limitation of the optimal control\\nview is that the action cost is assumed to be additive over the steps.\\n3.3\\nDefense Against Test-Time Attack by Adversarial Training\\nSome defense strategies can be viewed as optimal control, too. One defense against test-time attack is to\\nrequire the learned model h to have the large-margin property with respect to a training set. Let (x, y)\\nbe any training item, and ǫ a margin parameter. Then the large-margin property states that the decision\\nboundary induced by h should not pass ǫ-close to (x, y):\\n∀x′ : (∥x′ −x∥p ≤ǫ) ⇒h(x′) = y.\\n(10)\\nThis is an uncountable number of constraints. It is relatively easy to enforce for linear learners such as\\nSVMs, but impractical otherwise.\\nAdversarial training can be viewed as a heuristic to approximate the uncountable constraint (10) with\\na ﬁnite number of active constraints: one performs test-time attack against the current h from x to ﬁnd\\nan adversarial item x(1), such that ∥x(1) −x∥p ≤ǫ but h(x(1)) ̸= y. Instead of adding a single constraint\\nh(x(1)) = y, an additional training item (x(1), y) is then added to the training set. The machine learning\\nalgorithm learns a diﬀerent h, with the hope (but not constraining) that h(x(1)) = y. This process repeats\\nfor k iteration, resulting in k additional training items (x(i), y) for i = 1 . . . k.\\nIt should be clear that such defense is similar to training-data poisoning, in that the defender uses data\\nto modify the learned model. This is especially interesting when the learner performs sequential updates.\\nOne way to formulate adversarial training defense as control is the following:\\n4\\n\\n• The state is the model ht. Initially h0 can be the model trained on the original training data.\\n• The control input ut = (xt, yt) is an additional training item with the trivial constraint set Ut = X×y.\\n• The dynamics ht+1 = f(ht, ut) is one-step update of the model, e.g. by back-propagation.\\n• The defender’s running cost gt(ht, ut) can simply be 1 to reﬂect the desire for less eﬀort (the running\\ncost sums up to k).\\n• The defender’s terminal cost gT (hT ) penalizes small margin of the ﬁnal model hT with respect to the\\noriginal training data.\\nOf course, the resulting control problem (4) does not directly utilize adversarial examples. One way to\\nincorporate them is to restrict Ut to a set of adversarial examples found by invoking test-time attackers on\\nht, similar to the heuristic in [7]. These adversarial examples do not even need to be successful attacks.\\n3.4\\nAdversarial Reward Shaping\\nWhen adversarial attacks are applied to sequential decision makers such as multi-armed bandits or reinforce-\\nment learning agents, a typical attack goal is to force the latter to learn a wrong policy useful to the adversary.\\nThe adversary may do so by manipulating the rewards and the states experienced by the learner [11,14].\\nTo simplify the exposition, I focus on adversarial reward shaping against stochastic multi-armed bandit,\\nbecause this does not involve deception through perceived states. To review, in stochastic multi-armed bandit\\nthe learner at iteration t chooses one of k arms, denoted by It ∈[k], to pull according to some strategy [6].\\nFor example, the (α, ψ)-Upper Conﬁdence Bound (UCB) strategy chooses the arm\\nIt ∈argmaxi∈[k]ˆ\\nµi,Ti(t−1) + ψ∗−1\\n\\x12 α log t\\nTi(t −1)\\n\\x13\\n(11)\\nwhere Ti(t −1) is the number of times arm i has been pulled up to time t −1, ˆ\\nµi,Ti(t−1) is the empirical\\nmean of arm i so far, and ψ∗is the dual of a convex function ψ. The environment generates a stochastic\\nreward rIt ∼νIt. The learner updates its estimate of the pulled arm:\\nˆ\\nµIt,TIt(t) =\\nˆ\\nµIt,TIt(t−1)TIt(t −1) + rIt\\nTIt(t −1) + 1\\n(12)\\nwhich in turn aﬀects which arm it will pull in the next iteration. The learner’s goal is to minimize the\\npseudo-regret T µmax −E PT\\nt=1 µIt where µi = Eνi and µmax = maxi∈[k] µi. Stochastic multi-armed bandit\\nstrategies oﬀer upper bounds on the pseudo-regret.\\nWith adversarial reward shaping, an adversary fully observes the bandit. The adversary intercepts the\\nenvironmental reward rIt in each iteration, and may choose to modify (“shape”) the reward into\\nrIt + ut\\nwith some ut ∈R before sending the modiﬁed reward to the learner. The adversary’s goal is to use minimal\\nreward shaping to force the learner into performing speciﬁc wrong actions. For example, the adversary may\\nwant the learner to frequently pull a particular target arm i∗∈[k]. It should be noted that the adversary’s\\ngoal may not be the exact opposite of the learner’s goal: the target arm i∗is not necessarily the one with\\nthe worst mean reward, and the adversary may not seek pseudo-regret maximization.\\nAdversarial reward shaping can be formulated as stochastic optimal control:\\n• The state st, now called control state to avoid confusion with the Markov Decision Process states\\nexperienced by an reinforcement learning agent, consists of the suﬃcient statistic tuple at time t:\\nst = (T1(t −1), ˆ\\nµ1,T1(t−1), . . . , Tk(t −1), ˆ\\nµk,Tk(t−1), It).\\n5\\n\\n• The control input is ut ∈Ut with Ut = R in the unconstrained shaping case, or the appropriate Ut if\\nthe rewards must be binary, for example.\\n• The dynamics st+1 = f(st, ut) is straightforward via empirical mean update (12), TIt increment, and\\nnew arm choice (11).\\n• The adversary’s running cost gt(st, ut) reﬂects shaping eﬀort and target arm achievement in iteration\\nt. For instance,\\ngt(st, ut) = u2\\nt + Iλ[It ̸= i∗].\\n(13)\\nwhere λ > 0 is a trade oﬀparameter.\\n• There is not necessarily a time horizon T or a terminal cost gT (sT ).\\nThe control state is stochastic due to the stochastic reward rIt entering through (12).\\n4\\nAdvantages of the Optimal Control View\\nThere are a number of potential beneﬁts in taking the optimal control view:\\n• It oﬀers a uniﬁed conceptual framework for adversarial machine learning;\\n• The optimal control literature provides eﬃcient solutions when the dynamics f is known and one can\\ntake the continuous limit to solve the diﬀerential equations [15];\\n• Reinforcement learning, either model-based with coarse system identiﬁcation or model-free policy it-\\neration, allows approximate optimal control when f is unknown, as long as the adversary can probe\\nthe dynamics [8,9];\\n• A generic defense strategy may be to limit the controllability the adversary has over the learner.\\n• I mention in passing that the optimal control view applies equally to machine teaching [27, 29], and\\nthus extends to the application of personalized education [22,24].\\nI need to point out some limitations:\\n• Having a uniﬁed optimal control view does not automatically produce eﬃcient solutions to the control\\nproblem (4). For adversarial machine learning applications the dynamics f is usually highly nonlinear\\nand complex.\\nFurthermore, in graybox and blackbox attack settings f is not fully known to the\\nattacker. They aﬀect the complexity in ﬁnding an optimal control.\\n• The adversarial learning setting is largely non-game theoretic, though there are exceptions [5,16].\\nThese problems call for future research from both machine learning and control communities.\\nAcknowledgments. I acknowledge funding NSF 1837132, 1545481, 1704117, 1623605, 1561512, and\\nthe MADLab AF Center of Excellence FA9550-18-1-0166.\\nReferences\\n[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In\\nThe Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), 2016.\\n[2] Michael Athans and Peter L Falb. Optimal control: An introduction to the theory and its applications.\\nCourier Corporation, 2013.\\n[3] Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 4th edition, 2017.\\n6\\n\\n[4] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.\\nCoRR, abs/1712.03141, 2017.\\n[5] Michael Br¨\\nuckner and Tobias Scheﬀer. Stackelberg games for adversarial prediction problems. In Pro-\\nceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,\\npages 547–555. ACM, 2011.\\n[6] S´\\nebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed\\nbandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.\\n[7] Qi-Zhi Cai, Min Du, Chang Liu, and Dawn Song.\\nCurriculum adversarial training.\\nIn The 27th\\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI), 2018.\\n[8] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on\\ngraph structured data. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1115–\\n1124, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\\n[9] Yang Fan, Fei Tian, Tao Qin, and Tie-Yan Liu. Learning to teach. In ICLR, 2018.\\n[10] Terry L Friesz. Dynamic optimization and diﬀerential games, volume 135. Springer Science & Business\\nMedia, 2010.\\n[11] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on\\nneural network policies. arXiv, 2017.\\n[12] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li. Ma-\\nnipulating machine learning: Poisoning attacks and countermeasures for regression learning. The 39th\\nIEEE Symposium on Security and Privacy, 2018.\\n[13] Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. Adversarial Machine\\nLearning. Cambridge University Press, 2018. in press.\\n[14] Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Xiaojin Zhu. Adversarial attacks on stochastic bandits.\\nIn Advances in Neural Information Processing Systems (NIPS), 2018.\\n[15] L. Lessard, X. Zhang, and X. Zhu. An Optimal Control Approach to Sequential Machine Teaching.\\nArXiv e-prints, October 2018.\\n[16] Bo Li and Yevgeniy Vorobeychik. Scalable Optimization of Randomized Operational Decisions in Ad-\\nversarial Classiﬁcation Settings. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the\\nEighteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 38 of Proceedings\\nof Machine Learning Research, pages 599–607, San Diego, California, USA, 09–12 May 2015. PMLR.\\n[17] Daniel Liberzon. Calculus of variations and optimal control theory: A concise introduction. Princeton\\nUniversity Press, 2011.\\n[18] Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg,\\nand Le Song. Iterative machine teaching. In International Conference on Machine Learning, pages\\n2149–2158, 2017.\\n[19] Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James M. Rehg, and Le Song. Towards black-box iterative\\nmachine teaching. In ICML, volume 80 of JMLR Workshop and Conference Proceedings, pages 3147–\\n3155. JMLR.org, 2018.\\n[20] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD\\ninternational conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.\\n7\\n\\n[21] Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine\\nlearners. In The Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.\\n[22] Kaustubh Patil, Xiaojin Zhu, Lukasz Kopec, and Bradley Love. Optimal teaching for limited-capacity\\nhuman learners. In Advances in Neural Information Processing Systems (NIPS), 2014.\\n[23] B. Recht. A Tour of Reinforcement Learning: The View from Continuous Control. ArXiv e-prints, June\\n2018.\\n[24] Ayon Sen, Purav Patel, Martina A. Rau, Blake Mason, Robert Nowak, Timothy T. Rogers, and Xiaojin\\nZhu. Machine beats human at sequencing visuals for perceptual-ﬂuency practice. In Educational Data\\nMining, 2018.\\n[25] Emanuel Todorov. Optimal control theory. Bayesian brain: probabilistic approaches to neural coding,\\npages 269–298, 2006.\\n[26] Yevgeniy Vorobeychik and Murat Kantarcioglu. Adversarial machine learning. Synthesis Lectures on\\nArtiﬁcial Intelligence and Machine Learning, 12(3):1–169, 2018.\\n[27] Xiaojin Zhu. Machine teaching: an inverse problem to machine learning and an approach toward optimal\\neducation. In The Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence (AAAI “Blue Sky” Senior\\nMember Presentation Track), 2015.\\n[28] Xiaojin Zhu, Ji Liu, and Manuel Lopes. No learner left behind: On the complexity of teaching multiple\\nlearners simultaneously. In The 26th International Joint Conference on Artiﬁcial Intelligence (IJCAI),\\n2017.\\n[29] Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Raﬀerty. An Overview of Machine Teaching.\\nArXiv e-prints, January 2018. https://arxiv.org/abs/1801.05927.\\n8\\n'},\n",
       " {'title': 'Minimax deviation strategies for machine learning and recognition with\\n  short learning samples',\n",
       "  'summary': 'The article is devoted to the problem of small learning samples in machine\\nlearning. The flaws of maximum likelihood learning and minimax learning are\\nlooked into and the concept of minimax deviation learning is introduced that is\\nfree of those flaws.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1707.04849v1',\n",
       "  'authors': ['Michail Schlesinger', 'Evgeniy Vodolazskiy'],\n",
       "  'text': 'arXiv:1707.04849v1  [cs.LG]  16 Jul 2017\\nMinimax deviation strategies for machine\\nlearning and recognition with short learning\\nsamples\\nSchlesinger M.I. and Vodolazskiy E.V.\\nAugust 31, 2018\\nAbstract\\nThe article is devoted to the problem of small learning samples in\\nmachine learning. The ﬂaws of maximum likelihood learning and min-\\nimax learning are looked into and the concept of minimax deviation\\nlearning is introduced that is free of those ﬂaws.\\n1\\nIntroduction\\nThe small learning sample problem has been around in machine learning\\nunder diﬀerent names during its whole life. The learning sample is used to\\ncompensate for the lack of knowledge about the recognized object when its\\nstatistical model is not completely known. Naturally, the longer the learning\\nsample is, the better is the subsequent recognition. However, when the learn-\\ning sample becomes too small (2, 3, 5 elements) an eﬀect of small samples\\nbecomes evident. In spite of the fact that any learning sample (even a very\\nsmall one) provides some additional information about the object, it may be\\nbetter to ignore the learning sample than to utilize it with the commonly\\nused methods.\\nExample 1. Let us consider an object that can be in one of two random\\nstates y = 1 and y = 2 with equal probabilities. In each state the object\\ngenerates two independent Gaussian random signals x1 and x2 with variances\\nequal 1. Mean values of signals depend on the state as it is shown on Fig.\\n1\\n\\n1. In the ﬁrst state the mean value is (2, 0). In the second state the mean\\nvalue depends on an unknown parameter θ and is (0, θ). Even if no learning\\nsample is given a minimax strategy can be used to make a decision about the\\nstate y. The minimax strategy ignores the second signal and makes decision\\ny∗= 1 when x1 > 1 and decision y∗= 2 when x1 ≤1.\\nx2\\nx1\\nb\\nb\\nθ\\nb\\n2\\n0\\ny∗= 1\\ny∗= 2\\np(x1, x2|y = 1)\\np(x1, x2|y = 2)\\nFigure 1: Example 1. (x1, x2) ∈R2 – signal, y ∈{1, 2} – state.\\nNow let us assume that there is a sample of signals generated by an\\nobject in the second state but with higher variance 16. A maximum likelihood\\nstrategy estimates the unknown parameter θ and then makes a decision about\\ny as if the estimated value of the parameter is its true value. Fig. 2 shows how\\nthe probability of a wrong decision (called the risk) depends on parameter θ\\nfor diﬀerent sizes of the learning sample. If the learning sample is suﬃciently\\nlong, the risk of maximum likelihood strategy may become arbitrarily close\\nto the minimum possible risk.\\nNaturally, when the length of the sample\\ndecreases the risk becomes worse and worse. Furthermore, when it becomes\\nas small as 3 or 2 elements the risk of the maximum likelihood strategy\\nbecomes worse than the risk of the minimax strategy that uses neither the\\nlearning sample nor the signal x2 at all. Hence, it is better to ignore available\\nadditional data about the recognized object than to try to make use of it in\\na conventional way. It demonstrates a serious theoretical ﬂaw of commonly\\nused methods, and deﬁnitely not that short samples are useless. Any learning\\nsample, no mater how long or short it is, provides some, may be not a lot\\ninformation about the recognized object and a reasonable method has to use\\nit.\\n2\\n\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nn = 1\\nn = 2\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nn = 3\\nn = 10\\nFigure 2: Example 1. Probability of a wrong decision (risk) for diﬀerent\\nsizes n of the learning sample. The curve R(qML, θ) is the risk of a maximum\\nlikelihood strategy. The curve R(qminmax, θ) is the risk of a minimax strategy.\\nThe curve min\\nq\\nR(q, θ) is the minimum possible risk for each model.\\nExample 2. This is a simple example that has been used by H.Robbins in\\nhis seminal article [5] where he initiated empirical Bayessian approach and\\nexplaned its main idea. An object can be in one of two possible states y = 1\\nand y = 2. In each state the object generates a univariate Gaussian signal\\nx with variance 1. The mean value of the generated signal depends on the\\nstate y so that\\np(x|y = 1) =\\n1\\n√\\n2πe−(x+1)2\\n2\\n,\\np(x|y = 2) =\\n1\\n√\\n2πe−(x−1)2\\n2\\n.\\nOnly a priori probabilities of states are unknown and θ is the probability\\nof the ﬁrst state so that p(y = 1) = θ and p(y = 2) = 1 −θ. A minimax\\nstrategy for such incomplete statistical model makes decision y∗based on\\nthe sign of the observed signal and ensures probability of correct recognition\\n0.84 independently of a priori probabilities of states.\\nLet not only a single object, but a collection of mutually independant\\nobjects be available for recognition. Each object is in its own hidden state and\\nis presented with its own signal. Let us also assume that the decision about\\n3\\n\\nx\\ny∗= 2\\ny∗= 1\\np(x|y = 1)\\np(x|y = 2)\\nb\\n1\\nb\\n−1\\nb\\nFigure 3: Example 2. x ∈R – signal, y ∈{1, 2} – state.\\neach object’s state does not have to be made immediately when the object\\nis observed and can be postponed until the whole collection is observed. In\\nthis case maximum likelihood estimations of a priori probabilities of states\\ncan be computed and then each object of the collection is recognized as if the\\nestimated values of probabilities were the true values. When the presented\\ncollection is suﬃciently long the probability of a wrong decision can be made\\nas close to the minimum as possible (Fig.4). However, when the collection is\\ntoo short, the probability of a wrong decision can be much worse than that\\nof the minimax strategy.\\nThe considered examples lead to a diﬃcult and up to now an unanswered\\nquestion. What should be done when a ﬁxed sample of 2-3 elements is given\\nand no additional elements can be obtained? Is it really the best way to\\nignore these data or is it possible to make use of them? We want to ﬁll up\\nthis gap between maximum likelihood and minimax strategies and develop\\na strategy that covers teh whole range of learning samples lengths including\\nzero length. However, this gap, and it is infact a gap, shows a theoretical\\nimperfection of the commonly used learning procedures, namely, of maximum\\nlikelihood learning. The short sample problem in whole follows from the fact\\nthat maximum likelihood learning as well as many other learning procedures\\nhave not been deduced from any explicit risk-oriented requirement to the\\nquality of post-learning recognition.\\nWe will formulate such risk-oriented\\nrequirements a priori and will see what type of learning procedures follow.\\n2\\nBasic deﬁnitions\\nDeﬁnition 1. An object is represented with a tuple\\n\\nX, Y, Θ, pXY : X × Y × Θ →R\\n\\x0b\\n4\\n\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nn = 1\\nn = 2\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nn = 5\\nn = 10\\nFigure 4: Example 2. Probability of a wrong decision (risk) for diﬀerent\\nsizes n of the learning sample.\\nThe curve R(qML, θ) shows the risk of a\\nmaximum likelihood strategy, R(qminmax, θ) is the risk of a minimax strategy,\\nmin\\nq\\nR(q, θ) is the minimal possible risk.\\nwhere\\nX is a ﬁnite set of signal values x ∈X;\\nY is a ﬁnite set of states y ∈Y ;\\nΘ is a ﬁnite set of models θ ∈Θ;\\npXY (x, y; θ) is a probability of a pair (x ∈X, y ∈Y ) for a model θ ∈Θ.\\nA signal x is an observable parameter of recognized object whereas a\\nstate y is its hidden parameter. A pair (x, y) is random and for each pair\\n(x ∈X, y ∈Y ) its probability pXY (x, y; θ) exists. However, this probability\\nis not known because it depends on an unknown model θ. As for the model\\nθ it is not random, it takes a ﬁxed but unknown value. Only the set Θ is\\nknown that the value θ belongs to.\\nLet z be some random data that depend on a model θ and take values\\nfrom a ﬁnite set Z. The data is speciﬁed with a tuple\\n\\nZ, pZ : Z × Θ →R\\n\\x0b\\nwhere pZ(z; θ) is a probability of data z ∈Z for model θ ∈Θ.\\n5\\n\\nDeﬁnition 2. A random data\\n\\nZ, pZ : Z ×Θ →R\\n\\x0b\\nthat depends on a model\\nis called a learning data for an object\\n\\nX, Y, Θ, pXY : X × Y × Θ →R\\n\\x0b\\nif\\npXY Z(x, y, z; θ) = pXY (x, y; θ) · pZ(z; θ) for all x ∈X, y ∈Y, z ∈Z, θ ∈Θ.\\nA learning sample ((xi, yi)|i = 1, 2, . . . , n) used for supervised learning is\\na special cases of learning data when\\nZ = (X × Y )n and pZ(z; θ) =\\nn\\nY\\ni=1\\npXY (xi, yi; θ).\\nA learning sample (xi|i = 1, 2, . . . , n) for unsupervised learning is another\\nspecial case of learning data when\\nZ = Xn and pZ(z; θ) =\\nn\\nY\\ni=1\\nX\\ny∈Y\\npXY (xi, y; θ).\\nAny expert knowledge about the true model is also learning data. One can\\neven consider the case when |Z| = 1 and therefore pZ(z; θ) = 1, which is\\nequivalent to the absence of any learning data at all. We do not restrict the\\nlearning data in any way except that for any ﬁxed model the learning data\\nz depend neither on the current signal x nor on the current state y so that\\npXY Z(x, y, z; θ) = pXY (x, y; θ) · pZ(z; θ) for all x ∈X, y ∈Y, z ∈Z, θ ∈Θ.\\nDeﬁnition 3. A non-negative function q : X×Y ×Z →R is called a strategy\\nif P\\ny∈Y q(y|x, z) = 1 for all x ∈X, z ∈Z.\\nValue q(y|x, z) of a strategy q : X × Y × Z →R is a probability of a\\nrandomized decision that the current state of an object is y, given the current\\nobserved signal x and the available learning data z. The set of all strategies\\nq : X × Y × Z →R is denoted Q.\\nLet ω : Y × Y be a loss function whose value ω(y, y′) is the loss of a\\ndecision y′ when the true state is y.\\nDeﬁnition 4. Risk R(q, θ) of a strategy q on a model θ is expected loss\\nR(q, θ) =\\nX\\nz∈Z\\nX\\nx∈X\\nX\\ny∈Y\\npXY (x, y; θ)pZ(z; θ)\\nX\\ny′∈Y\\nq(y′|x, z)ω(y, y′).\\n6\\n\\nLet us be reminded that throughout the paper the sets X, Y , Z and Θ\\nare assumed to be ﬁnite. This allows a much more transparent formulation\\nof main results. Allowing some of the sets to be inﬁnite would require ﬁner\\nmathematical tools and the results might be obscured by unnecessary tech-\\nnical details.\\n3\\nImproper and Bayesian strategies.\\nOne can see that the risk of a strategy depends not only on the strategy itself\\nbut also on the model that the strategy is applied to. Therefore, in a general\\ncase it is not possible to prefer some strategy q1 to another strategy q2. The\\nrisk of q1 may be better than the risk of q2 on some models and worse on the\\nothers. However, it is possible to prefer strategy q2 to strategy q1 if the risk\\nof q1 is greater than the risk of q2 on all models. In this case we will say that\\nq2 dominates q1 and q1 is dominated by q2.\\nDeﬁnition 5. A strategy q0 is called improper if a strategy q∗exists such\\nthat R(q0, θ) > R(q∗, θ)\\nfor all θ ∈Θ.\\nWe want to exclude all improper from consideration strategies and derive\\na common form of all the rest.\\nLet T denote the set of all non-negative\\nfunctions τ : Θ →R such that P\\nθ∈Θ\\nτ(θ) = 1. Functions of such type will be\\nreﬀerred to as weight functions.\\nDeﬁnition 6. A strategy q∗is called Bayesian if there exists a weight func-\\ntion τ ∈T such that\\nq∗= arg min\\nq∈Q\\nX\\nθ∈Θ\\nτ(θ)R(q, θ).\\nTheorem 1. Each strategy q0 ∈Q is either Bayesian or improper, but never\\nboth.\\nProof. For a given strategy q0 let us deﬁne a function F: T × Q →R,\\nF(τ, q) =\\nX\\nθ∈Θ\\nτ(θ)\\n\\x02\\nR(q, θ) −R(q0, θ)\\n\\x03\\n.\\n7\\n\\nAccording to Deﬁnition 4, for any ﬁxed θ the risk R(q, θ) is a linear function\\nof probabilities q(y|x, z). Consequently, for any ﬁxed τ the function F is also\\na linear function of probabilities q(y|x, z). Similarly, function F is a linear\\nfunction of weights τ(θ) for any ﬁxed strategy q. The set Q of strategies\\nand the set T of weight functions are both closed convex sets. Consequently,\\ndue to the known duality theorem [1, 2, 4] function F has a saddle point\\n(τ ∗∈T, q∗∈Q) such that\\nmax\\nτ∈T min\\nq∈Q F(τ, q) = F(τ ∗, q∗) = min\\nq∈Q max\\nτ∈T F(τ, q),\\nwhere\\nq∗= argmin\\nq∈Q\\nmax\\nτ∈T F(τ, q),\\nτ ∗= argmax\\nτ∈T\\nmin\\nq∈Q F(τ, q).\\nIt is obvious that F(τ, q0) = 0 for any τ ∈T. Therefore, the inequality\\nmin\\nq∈Q F(τ, q) ≤0 holds for every τ ∈T and, consequently,\\nmax\\nτ∈T min\\nq∈Q F(τ, q) = F(τ ∗, q∗) ≤0.\\nTherefore, there are two mutually exclusive cases: either F(τ ∗, q∗) < 0 or\\nF(τ ∗, q∗) = 0. In such way the proof of the theorem is reduced to proving\\nthe following four propositions:\\nProposition 1. If the strategy q0 is Bayessian then F(τ ∗, q∗) = 0.\\nProposition 2. If F(τ ∗, q∗) = 0 then the strategy q0 is Bayessian.\\nProposition 3. If the strategy q0 is improper then F(τ ∗, q∗) < 0.\\nProposition 4. If F(τ ∗, q∗) < 0 then the strategy q0 is improper.\\nProof of Proposition 1. If the strategy q0 is Bayessian then according to\\nDeﬁnition 6 a weight function τ 0 exists such that inequality\\nX\\nθ∈Θ\\nτ 0(θ)R(q, θ) ≥\\nX\\nθ∈Θ\\nτ 0(θ)R(q0, θ)\\nis valid for all q ∈Q. Consequently, for all q ∈Q the chain\\n0 ≤\\nX\\nθ∈Θ\\nτ 0(θ)[R(q, θ) −R(q0, θ)] = F(τ 0, q) ≤max\\nτ∈T F(τ, q)\\n8\\n\\nis also valid. Since all numbers maxτ∈T F(τ, q), q ∈Q, are not negative the\\nleast of them is also not negative and\\nmin\\nq∈Q max\\nτ∈T F(τ, q) = F(τ ∗, q∗) ≥0\\nFrom this inequality it follows that F(τ ∗, q∗) = 0 because a case F(τ ∗, q∗) > 0\\nis impossible.\\nProof of Proposition 2. Let F(τ ∗, q∗) = 0 then\\n0 = F(τ ∗, q∗) = max\\nτ∈T min\\nq∈Q F(τ, q) = min\\nq∈Q F(τ ∗, q) =\\n= min\\nq∈Q\\nX\\nθ∈Θ\\nτ ∗(θ)\\n\\x02\\nR(q, θ) −R(q0, θ)\\n\\x03\\n=\\n= min\\nq∈Q\\n\\x14 X\\nθ∈Θ\\nτ ∗(θ)R(q, θ)\\n\\x15\\n−\\nX\\nθ∈Θ\\nτ ∗(θ)R(q0, θ).\\nIt implies the equality\\nmin\\nq∈Q\\nX\\nθ∈Θ\\nτ ∗(θ)R(q, θ) =\\nX\\nθ∈Θ\\nτ ∗(θ)R(q0, θ)\\nand therefore,\\nq0 = arg min\\nq∈Q\\nX\\nθ∈Θ\\nτ ∗(θ)R(q, θ),\\nwhich means that q0 is Bayesian according to Deﬁnition 6.\\nProof of Proposition 3.\\nIf the strategy q0 is improper then according to\\nDeﬁnition 5 a strategy q1 exists such that inequality R(q1, θ) < R(q0, θ)\\nholds for all θ. The set of models is ﬁnite and therefore, a value ε < 0 exists\\nsuch that for any θ inequality R(q1, θ) −R(q0, θ) ≤ε holds and a chain\\n0 > ε ≥\\nX\\nθ∈Θ\\nτ(θ)[R(q1, θ) −R(q0, θ)] = F(τ, q1) ≥min\\nq∈Q F(τ, q)\\nis valid for any τ ∈T. Since all numbers minq∈Q F(τ, q), τ ∈T, are not\\ngreater then ε the greatest of them is also not greater then ε and\\nmax\\nτ∈T min\\nq∈Q F(τ, q) = F(τ ∗, q∗) ≤ε < 0.\\n9\\n\\nProof of Proposition 4. Let F(τ ∗, q∗) < 0 then\\nF(τ ∗, q∗) = min\\nq∈Q max\\nτ∈T F(τ, q) = max\\nτ∈T F(τ, q∗) =\\n= max\\nτ∈T\\nX\\nθ∈Θ\\nτ(θ)\\n\\x02\\nR(q∗, θ) −R(q0, θ)\\n\\x03\\n= max\\nθ∈Θ\\n\\x02\\nR(q∗, θ) −R(q0, θ)\\n\\x03\\nand therefore\\nmax\\nθ∈Θ\\n\\x02\\nR(q∗, θ) −R(q0, θ)\\n\\x03\\n< 0.\\nConsequently, the inequality R(q∗, θ) < R(q0, θ) holds for all models θ ∈Θ\\nand q0 is improper according to Deﬁnition 5.\\nThe theorem gives good reasons to reappraise lot of well-known methods\\nthat are commonly used as something self-evident.\\nLet us illustrate this\\ncriticism with two simple examples. The ﬁrst example considers a certain\\nmethod of recognition without learning and the second relates to maximum\\nlikelihood learning. In both examples the loss function is\\nω(y, y′) =\\n(\\n0,\\nif y = y′,\\n1,\\nif y ̸= y′.\\nExample 3. Let x be an image of a letter, y be its name and θ be a position\\nof the letter in a ﬁeld of vision. Let the function pXY : X × Y × Θ →R be\\nconstructively deﬁned so that probability pXY (x, y; θ) can be calculated for\\neach triple x, y, θ. In this case when an image x with an unknown position\\nθ is observed the decision y∗(x) about the name of the letter has to be of the\\nform\\ny∗(x) = argmax\\ny∈Y\\nX\\nθ∈Θ\\nτ(θ)pXY (x, y; θ).\\n(1)\\nTheorem 1 reveals a certain weakness of the commonly used form\\nargmax\\ny∈Y\\nmax\\nθ∈Θ pXY (x, y; θ).\\n(2)\\nThe strategy (2) could be represented in the form (1) if the weights τ(θ) in\\n(1) could be chosen individually for each observation x ∈X. However, each\\n10\\n\\nBayessian strategy is speciﬁed with its own weight function τ : Θ →R so\\nthat weights are assigned to elements of the set Θ, not of the set Θ × X.\\nAs a rule, the strategy (2) cannot be represented in the form (1) with ﬁxed\\nweights τ(θ) that do not depend on x. It means that the strategy (2) is not\\nBayessian and is dominated by some other strategy that for each position of\\nthe letter recognizes its name better then strategy (2).\\nExample 4. Let the sets X, Y and Θ be speciﬁed for the recognized object\\nas well as a function pXY : X × Y × Θ →R. Let the learning information\\nbe a random learning sample z = ((xi, yi)|i = 1, 2, . . . , n) such that\\npZ(z; θ) =\\nn\\nY\\ni=1\\npXY (xi, yi; θ).\\nThen the decision y∗about the current state y0 based on the current signal\\nx0 and available learning sample z has to be of the form\\ny∗= arg max\\ny0∈Y\\nX\\nθ∈Θ\\nτ(θ)\\nn\\nY\\ni=0\\np(xi, yi; θ)\\n(3)\\nfor some ﬁxed τ that does not depend on z. One can see that the commonly\\nused maximum likelihood strategy\\ny∗= arg max\\ny0 p(x0, y0; θML(z)),\\n(4)\\nθML(z) = arg max\\nθ∈Θ\\nn\\nY\\ni=1\\np(xi, yi; θ)\\ncan almost never be represented in the form (3) with constant weights and\\ntherefore is not Bayessian. It means that some other strategy exists that\\nmakes a decision about the current state based both on current signal and\\nlearning information and for each model makes it better than strategy (4).\\n4\\nA gap between maximum likelihood and\\nminimax strategies.\\nWe consider maximum likelihood and minimax strategies and specify a gap\\nbetween them.\\n11\\n\\nLet us deﬁne for each θ ∈Θ a strategy qopt(θ) = argminq∈Q R(q, θ) that\\nassigns a probability qopt(y|x, z; θ) for each triple (x, y, z).\\nThe strategy\\nqopt(θ) is the best possible strategy that should be used if a true model were\\nknown. Since the model is known no learning data are needed. For any ﬁxed\\nmodel θ a strategy q(θ) : X × Y × Z →R can be replaced with a strategy\\nqX(θ) : X × Y →R with the same risk. Probabilities q(y|x, z; θ) have to be\\ntransformed into probabilities qX(y|x; θ) according to expression\\nqX(y|x; θ) =\\nX\\nz∈Z\\npZ(z; θ)q(y|x, z; θ)\\nand so the chain\\nR(q, θ) =\\nX\\nz∈Z\\nX\\nx∈X\\nX\\ny∈Y\\npXY (x, y; θ)pZ(z; θ)\\nX\\ny′∈Y\\nq(y′|x, z; θ)ω(y, y′) =\\n=\\nX\\nx∈X\\nX\\ny∈Y\\npXY (x, y; θ)\\nX\\ny′∈Y\\nω(y, y′)\\nX\\nz∈Z\\npZ(z; θ)q(y′|x, z; θ) =\\n=\\nX\\nx∈X\\nX\\ny∈Y\\npXY (x, y; θ)\\nX\\ny′∈Y\\nqX(y′|x; θ)ω(y, y′) = R(qX, θ).\\nis valid for each model θ. Consequently, for each θ the equality\\nmin\\nq∈Q R(q, θ) = min\\nqX∈QX R(qX, θ)\\n(5)\\nis valid. The symbol QX in (5) designates a set of all strategies of the form\\nqX : X × Y →R that do not use the learning data.\\nDeﬁnition 7. A strategy qML : X × Y × Z →R is called a maximum\\nlikelihood strategy if for each triple (x, y, z) it speciﬁes a probability\\nqML(y|x, z) = qopt\\nX (x|y; θML(z)),\\nwhere qopt\\nX (θ) = argmin\\nqX∈QX\\nR(qX, θ) and θML(z) = argmax\\nθ∈Θ\\npZ(z; θ).\\nIn other words, maximum likelihood strategies use the learning data z to\\nestimate a model θ and make a decision that minimizes the expected loss\\nwith an assumption that the estimated model is the true model.\\n12\\n\\nAs it has been quoted for Examples 3 and 4, as a rule, maximum likelihood\\nstrategies cannot be represented in a form of a Bayessian strategy\\nqB = argmin\\nq∈Q\\nX\\nθ∈Θ\\nτ(θ)R(q, θ)\\nwith ﬁxed weights τ(θ) that do not depend on the learning data. In such\\ncases the maximum likelihood strategy qML may be dominated with another\\nstrategy of the form X × Y × Z →R. Minimax strategies are free of this\\nﬂaw.\\nDeﬁnition 8. Strategy argmin\\nq∈Q\\nmax\\nθ∈Θ R(q, θ) is called a minimax strategy.\\nTheorem 2. No minimax strategy is improper.\\nProof. Let us prove an equivalent statement that any improper strategy q0 is\\nnot minimax. Indeed, as far as q0 is improper another strategy q1 exists such\\nthat R(q1, θ) < R(q0, θ) for all θ. Therefore, maxθ R(q1, θ) < maxθ R(q0, θ)\\nand minq maxθ R(q, θ) < maxθ R(q0, θ) and q0 is not argminq maxθ R(q0, θ).\\nThough maximum likelihood strategy may be improper whereas minimax\\nstrategy is never improper the ﬁrst one has an essential advantage over the\\nsecond. There is a rather wide class of learning data such that the maximum\\nlikelihood strategy is in a sense consistent for any recognized object whereas\\nthere is a rather wide class of recognized objects such that the minimax strat-\\negy is not consistent for any learning data. Let us exactly formulate these\\nstatements and prove them.\\nLet z ∈Z be a random variable that depends on model θ and let for each\\nz ∈Z and θ ∈Θ a probabillity pZ(z; θ) be given. We will say that this\\ndependence is essential if for each two diﬀerent models θ1 ̸= θ2 a value z∗\\nexists such that pZ(z∗; θ1) ̸= pZ(z∗; θ2). Let zn = (zi|i = 1, 2, . . . , n) ∈Zn be\\na learning sample, pZn(zn; θ∗) = Qn\\ni=1 pZ(zi; θ∗) be a probability of a sample\\nand θML(zn) = argmaxθ pZn(zn; θ) be a maximum likelihood estimation of\\nthe model.\\nConsistency is a generally known property of maximum likelihood esti-\\nmate. In the considered case this property may be formulated in a simple\\n13\\n\\nway that the probability of inequality θML(zn) ̸= θ∗converges to zero when\\nn increases or, formally,\\nlim\\nn→∞\\nX\\nzn∈Zn\\nerr\\nn\\nY\\ni=1\\npZ(zi; θ∗) = 0\\n(6)\\nwhere\\nZn\\nerr = {zn ∈Zn|θML(zn) ̸= θ∗}.\\n(7)\\nThe consistency of a maximum likelihood estimations is a base for a proof of\\nthe following theorem about consistency of maximum likelihood strategy.\\nTheorem 3. Let z be random variable that takes values from a set Z ac-\\ncording to probability distribution pZ(z; θ) that essentially depends on θ;\\nlet n be a positive integer and zn = (zi|i = 1, 2, . . . , n) ∈Zn be a random\\nlearning sample with probability distribution pZn(zn; θ) = Qn\\ni=1 pZ(zi; θ);\\nlet qML\\nn\\n: X × Y × Zn →R be a maximum likelihood strategy for an object\\n⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.\\nThen\\nlim\\nn→∞max\\nθ∈Θ\\n\\x02\\nR(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n= 0.\\nProof. As far as a set Θ is ﬁnite the proof of the theorem is reduced to proof\\nof the equality\\nlim\\nn→∞\\n\\x02\\nR(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n= 0\\n(8)\\nfor any θ. The subsequent proof is based on equality (5), on equalities (6)\\nand (7) that express consistency of maximum likelihood estimates and on\\nequality\\nR(qML\\nn\\n, θ) =\\nX\\nzn∈Zn\\npZn(zn; θ) min\\nqX∈QX R(qX, θML(zn)),\\nwhere θML(zn) = argmax\\nθ∈Θ\\npZn(zn; θ),\\n14\\n\\nthat follows from Deﬁnition 7. The following chain is valid:\\nlim\\nn→∞[R(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)] = lim\\nn→∞[R(qML\\nn\\n, θ) −min\\nqX∈QX R(qX, θ)] =\\n= lim\\nn→∞[\\nX\\nzn∈Zn\\npZn(zn; θ) min\\nqX∈QX R(qX, θML(zn)) −min\\nqX∈QX R(qX, θ)]\\n= lim\\nn→∞\\nX\\nzn∈Zn\\npZn(zn; θ)[ min\\nqX∈QX R(qX, θML(zn)) −min\\nqX∈QX R(qX, θ)]\\n= lim\\nn→∞\\nX\\nzn∈Zn\\nerr\\npZn(zn; θ)[ min\\nqX∈QX R(qX, θML(zn)) −min\\nqX∈QX RX(qX, θ)]\\n≤lim\\nn→∞\\nX\\nzn∈Zn\\nerr\\npZn(zn; θ)[max\\ny∈Y max\\ny′∈Y w(y, y′) −min\\ny∈Y min\\ny′∈Y w(y, y′)]\\n= lim\\nn→∞{[max\\ny∈Y max\\ny′∈Y w(y, y′) −min\\ny∈Y min\\ny′∈Y w(y, y′)]\\nX\\nzn∈Zn\\nerr\\npZn(zn; θ)}\\n= [max\\ny∈Y max\\ny′∈Y w(y, y′) −min\\ny∈Y min\\ny′∈Y w(y, y′)] lim\\nn→∞\\nX\\nzn∈Zn\\nerr\\npZn(zn; θ) = 0.\\nIt follows from a chain that for any θ an inequality\\nlim\\nn→∞\\n\\x02\\nR(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n≤0\\nholds. The diﬀerence R(qML\\nn\\n, θ)−minq∈Q R(q, θ) is never negative and so (8)\\nis proved.\\nSo, with the increasing length of learning sample the risk function of max-\\nimum likelihood strategy becomes arbitrarily close to the minimum possible\\nrisk function. Minimax strategy has not this property. Moreover, for certain\\nclass of objects minimax strategies simply ignore the learning sample, no\\nmatter how long it is.\\nTheorem 4. Let for an object ⟨X, Y, Θ, pXY : X × Y × Θ →R⟩a pair (θ∗, q∗\\nX)\\nexists such that\\nq∗\\nX = argmin\\nqX∈QX\\nR(qX, θ∗),\\nθ∗= argmax\\nθ∈Θ\\nR(q∗\\nX, θ).\\nThen the inequality\\nmax\\nθ∈Θ R(q, θ) ≥max\\nθ∈Θ R(q∗\\nX, θ)\\n(9)\\nis valid for any learning data ⟨Z, pZ : Z × Θ →R⟩and any strategy q : X ×\\nY × Z →R.\\n15\\n\\nProof. For any strategy q ∈Q we have the chain\\nmax\\nθ∈Θ R(q, θ) ≥R(q, θ∗) ≥min\\nq∈Q R(q, θ∗) =\\n= min\\nqX∈QX R(qX, θ∗) = R(q∗\\nX, θ∗) = max\\nθ∈Θ R(q∗\\nX, θ).\\nThe theorem shows that for some objects the minimax approach is partic-\\nularly inappropriate because it enforces to ignore any learning data. There\\nis nothing unusual in conditions of the Theorem 4. Examples 1 and 2 in\\nIntroduction show just the cases when these conditions are satisﬁed.\\nSo, there is a following gap between maximum likelihood and minimax strate-\\ngies. Maximum likelihood strategy may be dominated with other strategy.\\nIn this case it can be improved and, consequently, it is not optimal from any\\npoint of view. However, for wide class of learning data maximum likelihood\\nstrategies are consistent and so their chortage does not become apparent\\nwhen learning sample of an arbitrary size may be obtained. Cases of learn-\\ning samples of ﬁxed sizes, especially, short samples form an area of improper\\napplication of maximum likelihood strategies. This area is not covered with\\nminimax strategies. Though minimax strategies are dominated with no strat-\\negy, for rather wide class of objects minimax requirement enforces to ignore\\nany learning sample, no matter how long it is.\\n5\\nMinimax deviation strategies.\\nThis section is aimed at developing a Bayesian consistent strategy that has\\nto ﬁll a gap between maximum likelihood and minimax strategies.\\nDeﬁnition 9. A strategy argmin\\nq∈Q\\nmax\\nθ∈Θ\\n\\x02\\nR(q, θ) −min\\nq′∈Q R(q′, θ)\\n\\x03\\nis called mini-\\nmax deviation strategy.\\nMinimax deviation strategies do not have the drawback of the minimax\\nstrategies. A theorem that is similar to Theorem 3 for maximum likelihood\\nstrategies is also valid for minimax deviation strategies.\\n16\\n\\nTheorem 5. Let z be random variable that takes values from a set Z ac-\\ncording to probability distribution pZ(z; θ) that essentially depends on θ;\\nlet n be a positive integer and zn = (zi|i = 1, 2, . . . , n) ∈Zn is a random\\nlearning sample with probability distribution pZn(zn; θ) = Qn\\ni=1 pZ(zi; θ);\\nlet q∗\\nn : X × Y × Zn →R be a minimax deviation strategy for an object\\n⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.\\nThen\\nlim\\nn→∞max\\nθ∈Θ\\n\\x02\\nR(q∗\\nn, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n= 0.\\n(10)\\nProof. The Theorem is a straighforward consequence of Deﬁnition 9 and\\nthe Theorem 3. Let qML\\nn\\nbe a maximum likelihood strategy for an object\\n⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.\\nIt follows from Deﬁnition 9 that\\nmax\\nθ∈Θ\\n\\x02\\nR(q∗\\nn, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n≤max\\nθ∈Θ\\n\\x02\\nR(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\nfor any n. It follows from Theorem 3 that\\nlim\\nn→∞max\\nθ∈Θ\\n\\x02\\nR(q∗\\nn, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n≤\\n≤lim\\nn→∞max\\nθ∈Θ\\n\\x02\\nR(qML\\nn\\n, θ) −min\\nq∈Q R(q, θ)\\n\\x03\\n= 0.\\nAs far as the diﬀerence [R(q∗\\nn, θ) −minq∈Q R(q, θ)\\n\\x03\\nis negative for no model\\nthe equality (10) is proved.\\nLet us note that the proof of the Theorem 10 shows not only a consistency\\nof minimax deviation strategy. It shows also that minimax deviation strat-\\negy converges to desired result not slower than maximum likelihood strategy.\\nSimilarly, one can show that this advantage of minimax deviation strategy\\nholds as compared with any consistent strategy and from this point of view\\nit is the best of all consistent strategies.\\nFollowing theorem states that minimax deviation strategies are also inap-\\npropriate for recognition of certain type of objects.\\n17\\n\\nTheorem 6. Let for an object ⟨X, Y, Θ, p : X × Y × Θ →R⟩a model θ∗and\\na strategy q∗\\nX exist such that\\nq∗\\nX = argmin\\nqX∈QX\\n[RX(qX, θ∗) −min\\nq′\\nX∈QX RX(q′\\nX, θ∗)],\\n(11)\\nθ∗= argmax\\nθ∈Θ\\n[RX(q∗\\nX, θ) −min\\nq′\\nX∈QX RX(q′\\nX, θ)].\\n(12)\\nThen the inequality\\nmax\\nθ∈Θ [R(q, θ) −min\\nqX∈QX R(qX, θ)] ≥max\\nθ∈Θ [R(q∗\\nX, θ) −min\\nqX∈QX R(qX, θ)]\\nholds for any learning data ⟨Z, pZ : Z × Θ →R⟩and any strategy q ∈Q.\\nProof. In fact, proof of the theorem does not diﬀer from the proof of the\\nTheorem 4.\\nHowever, the consequences of this theorem for minimax deviation strate-\\ngies are not so destructive as those of Theorem 4 for minimax strategies. In\\nfact, conditions (11) and (12) imply that a strategy q∗\\nX ∈QX exists that\\ndoes not use learning information and assures minimal possible risk for each\\nmodel,\\nR(q∗\\nX, θ) = min\\nqX∈QX R(qX, θ) for all θ ∈Θ.\\nIn this case, any learning data are needless and has to be omitted by any\\nstrategy.\\nEvidently, minimax deviation strategy is not improper and, consequently,\\nis Bayessian. The following theorem shows how the corresponding weight\\nfunction has to be obtained.\\nTheorem 7. Minimax deviation strategy\\nq∗= argmin\\nq∈Q\\nmax\\nθ∈Θ\\n\\x02\\nR(q, θ) −min\\nqX∈QX R(qX, θ)\\n\\x03\\nis a Bayesian strategy argmin\\nq∈Q\\nP\\nθ∈Θ\\nτ ∗(θ)R(q, θ) with respect to weight function\\nτ ∗= arg max\\nτ∈T\\n\"\\nmin\\nq∈Q\\nX\\nθ∈Θ\\nτ(θ)R(q, θ) −\\nX\\nθ∈Θ\\nτ(θ) min\\nqX∈QX R(qX, θ)\\n#\\n.\\n(13)\\n18\\n\\nProof. Let us deﬁne a function F : T × Q →R,\\nF(τ, q) =\\nX\\nθ∈Θ\\nτ(θ)R(q, θ) −\\nX\\nθ∈Θ\\nτ(θ) min\\nqX∈QX R(qX, θ)\\nand express q∗and τ ∗in terms of F,\\nq∗= argmin\\nq∈Q\\nmax\\nθ∈Θ\\n\\x02\\nR(q, θ) −min\\nqX∈QX R(qX, θ)\\n\\x03\\n= argmin\\nq∈Q\\nmax\\nτ∈T\\nX\\nθ∈Θ\\nτ(θ)\\n\\x02\\nR(q, θ) −min\\nqX∈QX R(qX, θ)\\n\\x03\\n= argmin\\nq∈Q\\nmax\\nτ∈T F(τ, q),\\nτ ∗= arg max\\nτ∈T min\\nq∈Q F(τ, q).\\nThe function F is a linear function of q for ﬁxed τ and a linear function of\\nτ for ﬁxed q and is deﬁned on a Cartesian product of two closed convex sets\\nT and Q. In such case a pair (τ ∗, q∗) is a saddle point [1, 2, 4],\\nmin\\nq∈Q max\\nτ∈T F(τ, q) = F(τ ∗, q∗) = max\\nτ∈T min\\nq∈Q F(τ, q),\\nthat implies F(τ ∗, q∗) = min\\nq∈Q F(τ ∗, q) and\\nq∗= arg min\\nq∈Q F(τ ∗, q) =\\n= arg min\\nq∈Q\\n\"X\\nθ∈Θ\\nτ ∗(θ)R(q, θ) −\\nX\\nθ∈Θ\\nτ ∗(θ) min\\nqX∈QX R(qX, θ)\\n#\\n=\\n= arg min\\nq∈Q\\nX\\nθ∈Θ\\nτ ∗(θ)R(q, θ).\\nIn such way developing minimax deviation strategy is reduced to calculat-\\ning weights τ(θ) of models that maximize concave function (13). In described\\nbelow experiments general purpose methods of non-smooth optimization [6]\\nwere used.\\n19\\n\\n6\\nExperiments\\nMinimax deviation strategies have been built for objects considered in In-\\ntroduction in Examples 1 and 2. Minimax deviation strategies have been\\ncompared with maximum likelihood and minimax strategies.\\nResults are\\npresented on Figures 5 and 6 that show risk R(q, θ) of the strategies as a\\nfunction of a model for several learning sample sizes. Figure 5 relates to\\nExample 1 and Figure 6 to Example 2.\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nn = 1\\nn = 2\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nθ\\nb\\n−6\\n−3\\n−0\\n3\\n6\\nb\\nb\\nb\\nb\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nn = 3\\nn = 10\\nFigure 5: Example 1. Probability of making a wrong decision for diﬀerent\\nsizes n of the learning sample. The dashed line shows the risk of a minimax\\ndeviation strategy. The curve R(qML, θ) is the risk of a maximum likelihood\\nstrategy. The curve R(qminmax, θ) is the risk of a minimax strategy. The\\ncurve min\\nq\\nR(q, θ) is the minimum possible risk for each model.\\n7\\nConclusion\\nThe paper analyzes the problem when for given object\\n\\nX, Y, Θ, pXY : X × Y × Θ →R\\n\\x0b\\n,\\nloss function w : Y × Y →R, learning data source\\n\\nZ, pZ : Z × Θ →R\\n\\x0b\\n,\\nobserved current signal x and available learning data z a decision y∗about\\n20\\n\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nn = 1\\nn = 2\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nθ\\nR(q, θ)\\nmin\\nq\\nR(q, θ)\\nR(qmin max, θ)\\nR(qML, θ)\\nb\\n0\\nb\\n0.5\\nb\\n1\\nn = 5\\nn = 10\\nFigure 6: Example 2. Probability of making a wrong decision for diﬀerent\\nsizes n of the learning sample. The dashed line shows the risk of a minimax\\ndeviation strategy. The curve R(qML, θ) is the risk of a maximum likelihood\\nstrategy. The curve R(qminmax, θ) is the risk of a minimax strategy. The\\ncurve min\\nq\\nR(q, θ) is the minimum possible risk for each model.\\ncurrent hidden state y has to be made. The wide class of commonly used\\nstrategies make the decision of a form\\ny∗= argmin\\ny′∈Y\\nX\\ny∈Y\\npXY (x, y; θest(z))w(y, y′)\\n(14)\\nwhere θest : Z →Θ is a reasonable estimating a model θ based on learning\\ndata z. It means that the learning data are used to choose a single best model\\nand the objects are recognized as if this best model equals the true model.\\nThe approach is acceptable if learning data are arbitrarily long learning sam-\\nples and estimator θest : Z →Θ is consistent. If the learning information has\\na ﬁxed format, for example, is a learning sample of limited size then the ap-\\nproach gives no guarantee for subsequent recognition. Indeed, the approach\\nis not deduced from any risk-oriented requirement. Reasonable requirement\\n21\\n\\nto the quality of post-learning recognition implies the decision of the form\\ny∗= argmin\\ny′∈Y\\nX\\nθ∈Θ\\nτ(θ)pZ(z; θ)\\nX\\ny∈Y\\npXY (x, y; θ)w(y, y′)\\n(15)\\nthat diﬀers from (14). Moreover, any decision that diﬀers from (15) can be\\nreplaced with a decision of the form (15) with the better recognition quality.\\nThere is nothing in decision (15) that could be treated as a selecting some best\\nmodel of the model set and so no question stands what estimator θest : Z →Θ\\nhas to be used. No model has to be selected, on the contrary, all models have\\nto take part in decision with their weights. It is essential that the weights do\\nnot depend on learning data, they are determined by requirement to searched\\nstrategy for concrete applied situation. The paper shows a way for computing\\nthese weights for minimax deviation strategy that is appropriate for learning\\nsamples of any length and in such way ﬁlls a gap between maximum likeli-\\nhood and minimax startegies.\\nMinimax deviation strategy is not at all a single strategy that is reason-\\nable in such or other application. Many other strategies are appropriate too,\\nfor example, a strategies of the form\\nargmin\\nq∈Q\\nmax\\nθ∈Θ\\nR(q, θ) −α(θ)\\nβ(θ)\\n(16)\\nwith predeﬁned numbers α(θ) and β(θ) > 0. Minimax strategy is a special\\ncase of (16) when α(θ) = 0, β(θ) = 1, minimax deviation strategy is a case\\nwhen α(θ) = minq∈Q R(q, θ), β(θ) = 1. A reasonable modiﬁcation of mini-\\nmax deviation strategy is a case when α(θ) = 0, β(θ) = minq∈Q R(q, θ). The\\nnumbers α(θ) may be risks of some already developed strategy and this is a\\ncase when the developer wants to check whether the better strategy is possi-\\nble. At last, numbers α(θ) may be simply desired values of risks in concrete\\napplied situation.\\nRequirements of the form (16) together with various loss functions deter-\\nmine various applied situations and obtained results show the way to cope\\nwith all them. It has become quite clear now that each strategy of the form\\n(16) may be represented in the form (15) because, obviously, no of them is\\nimproper. Obtained results imply unexpected conclusion that learning data\\n22\\n\\ntake part in a decision (15) in a uniﬁed form that depends neither on applied\\nsituation nor on recognized object. So, no question stands more how the\\nlearning data have to inﬂuence the decision about current state when the\\ncurrent signal is observed. Learning data inﬂuence the decision via and only\\nvia probabilities pZ; (z; θ), not via choise of some best model of the model\\nset.\\nReferences\\n[1] J.M. Borwein and A.S. Lewis. Convex Analysis and Nonlinear Optimiza-\\ntion. Springer Verlag, 2000.\\n[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambrige Univer-\\nsity Press, 2004.\\n[3] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classiﬁ-\\ncation. Wiley, 2000.\\n[4] J.-B. Hiriart-Urruty and C. Lemarechal. Fundamentals of Convex Anal-\\nysis. Springer Verlag, 2002.\\n[5] Herbert Robbins. Asymptotically Subminimax Solutions of Compound\\nStatistical Decision Problems. In Jerzy Neyman, editor, Proceedings of the\\nSecond Berkeley Symposium on Mathematical Statistics and Probability,\\npages 131–148. University of California Press, 1951.\\n[6] N.Z. Shor.\\nNondiﬀerentiable Optimization and Polynomial Problems.\\nNonconvex Optimization and Its Applications. Springer, 1998.\\n[7] Andrew R. Webb. Statistical Pattern Recognition. Wiley, 2002.\\n23\\n'}]"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc84e63-bc1d-4a4f-8eea-a315b9ce4100",
   "metadata": {},
   "source": [
    "## Step 2:  Processing and Embedding Generation\n",
    "\n",
    "1. **Document Creation**: The first step is to transform the text of the papers into Document objects, which are structures used by LangChain to manipulate and process textual content.\n",
    "2. **Text Splitting**: Once the text of the papers is encapsulated in Document objects, the next step is to segment them into smaller \"chunks.\" This is done using the RecursiveCharacterTextSplitter, a LangChain tool that divides the text into smaller blocks to facilitate subsequent processing. The chunk_size defines the maximu\n",
    "3. **Embedding Generation**: After the text segmentation, the next step is to convert these chunks into embeddings, which are vector representations of the text. This is accomplished using the HuggingFaceEmbeddings class, which leverages language models from Hugging Face to create these representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "064759c4-6e03-4653-8dd1-6dd845aa1ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = [Document(page_content=paper['text'], metadata={\"title\": paper['title']}) for paper in papers]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=400)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "4c5e2fdc-dbe1-4948-8ba3-8396ece5081a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings= HuggingFaceEmbeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25966f82-3b88-4067-86a8-9ebe143b535b",
   "metadata": {},
   "source": [
    "## Step 3: Vector Data Storage and Retrieval\n",
    "\n",
    "In this step, the pipeline stores the generated embeddings in a vector database and implements a similarity search function for specific queries.\n",
    "\n",
    "1. **Embedding Storage**: The previously segmented text chunks, which have been converted into embeddings, are stored in a vector database using Chroma.\n",
    "2. **Retriever Configuration**: Once the embeddings are stored in Chroma, the next step is to configure a \"retriever.\" This retriever is responsible for performing similarity searches within the vector database, allowing you to retrieve the most relevant embeddings based on a specific query.\n",
    "3. **Similarity Search Function**: The query_vectordb function is defined to perform similarity searches in the vector database. It accepts a query as input and returns the top k most similar results, where k is an adjustable parameter (with the default being 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "ff1486a1-1c3a-4c93-b6e3-6a7685ad0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "40d5dc9e-0632-4f61-bc9b-43bbb144a09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "d535ccd6-1f82-4372-b4d1-e5538fcc6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectordb(vectordb, query, k=10):\n",
    "    \"\"\"\n",
    "    Performs a similarity search in a vector database.\n",
    "    Parameters:\n",
    "    - vectordb: The vector database where the embeddings are stored.\n",
    "    - query: The query that will be used to search for the most similar embeddings in the database.\n",
    "    - k: The number of most similar results that should be returned (default is 10).\n",
    "    \"\"\"\n",
    "    return vectordb.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "239f6b6a-48b5-43be-968c-45ac12e52a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='works for machine learning lecture 6a overview of mini-batch gradient\\ndescent. Cited on, 14, 2012.\\n[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\\nmaker.\\nEﬃcient hyperparameter optimization for deep learning al-\\ngorithms using deterministic RBF surrogates.\\nIn Proceedings of the\\nThirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9,\\n2017, San Francisco, California, USA., pages 822–829, 2017.'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='works for machine learning lecture 6a overview of mini-batch gradient\\ndescent. Cited on, 14, 2012.\\n[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\\nmaker.\\nEﬃcient hyperparameter optimization for deep learning al-\\ngorithms using deterministic RBF surrogates.\\nIn Proceedings of the\\nThirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9,\\n2017, San Francisco, California, USA., pages 822–829, 2017.'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='works for machine learning lecture 6a overview of mini-batch gradient\\ndescent. Cited on, 14, 2012.\\n[40] Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoe-\\nmaker.\\nEﬃcient hyperparameter optimization for deep learning al-\\ngorithms using deterministic RBF surrogates.\\nIn Proceedings of the\\nThirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9,\\n2017, San Francisco, California, USA., pages 822–829, 2017.'),\n",
       " Document(metadata={'title': 'An Optimal Control View of Adversarial Machine Learning'}, page_content='would be empirical risk minimization with hinge loss ℓ() and a regularizer:\\nw1 = f(u0) ∈argminw\\nn\\nX\\ni=1\\nℓ(w, xi, yi) + λ∥w∥2.\\n(5)\\nThe batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf() if it knows the form (5), ℓ(), and the value of λ.\\n• The time horizon T = 1.\\n2'),\n",
       " Document(metadata={'title': 'An Optimal Control View of Adversarial Machine Learning'}, page_content='would be empirical risk minimization with hinge loss ℓ() and a regularizer:\\nw1 = f(u0) ∈argminw\\nn\\nX\\ni=1\\nℓ(w, xi, yi) + λ∥w∥2.\\n(5)\\nThe batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf() if it knows the form (5), ℓ(), and the value of λ.\\n• The time horizon T = 1.\\n2'),\n",
       " Document(metadata={'title': 'An Optimal Control View of Adversarial Machine Learning'}, page_content='would be empirical risk minimization with hinge loss ℓ() and a regularizer:\\nw1 = f(u0) ∈argminw\\nn\\nX\\ni=1\\nℓ(w, xi, yi) + λ∥w∥2.\\n(5)\\nThe batch SVM does not need an initial weight w0. The adversary has full knowledge of the dynamics\\nf() if it knows the form (5), ℓ(), and the value of λ.\\n• The time horizon T = 1.\\n2'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='described by the mathematical optimization problem of\\nmin\\nw∈K f(w)\\n(1.1)\\nThis is the problem that the lecture series focuses on, with particular em-\\nphasis on functions that arise in machine learning and have special structure\\nthat allows for eﬃcient algorithms.\\n1.1\\nExamples of optimization problems in machine\\nlearning\\n1.1.1\\nEmpirical Risk Minimization\\nMachine learning problems exhibit special structure. For example, one of\\nthe most basic optimization problems in supervised learning is that of ﬁtting\\na model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='described by the mathematical optimization problem of\\nmin\\nw∈K f(w)\\n(1.1)\\nThis is the problem that the lecture series focuses on, with particular em-\\nphasis on functions that arise in machine learning and have special structure\\nthat allows for eﬃcient algorithms.\\n1.1\\nExamples of optimization problems in machine\\nlearning\\n1.1.1\\nEmpirical Risk Minimization\\nMachine learning problems exhibit special structure. For example, one of\\nthe most basic optimization problems in supervised learning is that of ﬁtting\\na model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='described by the mathematical optimization problem of\\nmin\\nw∈K f(w)\\n(1.1)\\nThis is the problem that the lecture series focuses on, with particular em-\\nphasis on functions that arise in machine learning and have special structure\\nthat allows for eﬃcient algorithms.\\n1.1\\nExamples of optimization problems in machine\\nlearning\\n1.1.1\\nEmpirical Risk Minimization\\nMachine learning problems exhibit special structure. For example, one of\\nthe most basic optimization problems in supervised learning is that of ﬁtting\\na model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems'),\n",
       " Document(metadata={'title': 'Lecture Notes: Optimization for Machine Learning'}, page_content='a model to data, or examples, also known as the optimization problem of\\nEmpirical Risk Minimization (ERM). The special structure of the problems\\narising in such formulations is separability across diﬀerent examples into\\nindividual losses.\\nAn example of such formulation is the supervised learning paradigm of\\nlinear classiﬁcation. In this model, the learner is presented with positive and\\nnegative examples of a concept. Each example, denoted by ai, is represented\\nin Euclidean space by a d dimensional feature vector. For example, a com-\\nmon representation for emails in the spam-classiﬁcation problem are binary\\nvectors in Euclidean space, where the dimension of the space is the number of')]"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"LLM\"\n",
    "query_vectordb(vectordb, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e8faf-43be-44f5-bd21-94be9efb799c",
   "metadata": {},
   "source": [
    "## Chapter 2: Building a Prompt Flow for Generating Scientific Presentation Scripts\n",
    "\n",
    "<img src=\"img/text-generation2.png\" alt=\"image.png\"/>\n",
    "\n",
    "\n",
    "In this chapter, we will create a prompt flow with the goal of building a script for scientific presentations using GPT-3.5. To achieve this, we will follow a process in which each section of the script (such as the title, introduction, methodology, etc.) will be generated individually, using a specific \"Prompt Template\" for each part.\n",
    "\n",
    "\n",
    "1. Defining the Prompt Template::\n",
    "   - For each section of the script, such as the title, introduction, and conclusion, we define a \"Prompt Template\" in string format. This template contains the instructions that the GPT-3.5 model will follow to generate the desired text.\n",
    "   - For example, for the title, the template might be: :``` Generate a title for the presentation that is clear, concise, and reflects the content. Add a subtitle if needed. ```\n",
    "2. Executing Text Generation:\n",
    "    - We use a function called run_and_approve that takes the prompt template and a specific task, such as \"create a title\" or \"generate an introduction..\n",
    "    - This function executes the text generation based on the template, showing the result to the user for approval.\n",
    "3. Logging Metrics with Galileo::\n",
    "    - Each time the model successfully generates text and the user approves it, the metrics related to the generation process are logged and sent to Galileo, a tool used to monitor and evaluate the quality of the prompts and the model's responses.\n",
    "    - This allows for a detailed record of the model's performance in generating the different parts of the scientific presentation script..\n",
    "4. Creating Each Part of the Script:\n",
    "   - The process is repeated for each part of the script: title, introduction, methodology, results, conclusion, and references.\n",
    "   - For each of these sections, the model generates text according to the instructions in the template, and the user approves or rejects the result..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b928d0d-7e83-4f5d-b1d5-9792f28fd76a",
   "metadata": {},
   "source": [
    "## Step 0: Config Enviroment\n",
    "\n",
    "In this step, we log in using the Galileo and OpenAI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2d36fd47-d75d-4bf5-b082-f596a3d7498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "10b13463-62ba-4cb5-9ff5-89bf8253cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as diogo.vieira@hp.com.\n"
     ]
    }
   ],
   "source": [
    "os.environ['GALILEO_API_KEY'] = ''  # Substitua pela sua chave de API\n",
    "os.environ['GALILEO_CONSOLE_URL'] = 'https://console.hp.galileocloud.io/'\n",
    "GALILEO_PROJECT_NAME = 'Academic Script'\n",
    "config = pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "9c5d702b-82d1-4062-b93a-d08cb8656433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "import promptquality as pq\n",
    "from promptquality import NodeRow, NodeType\n",
    "import uuid\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Function that formats the contents of a list of documents into a single string.\n",
    "    Parameters:\n",
    "    - docs (List[Document]): List of Document objects, where each document contains a textual content.\n",
    "\n",
    "    Returns:\n",
    "    - str: A single string that joins the content of all documents, separated by two line breaks.\n",
    "    \n",
    "    \"\"\"\n",
    "    formatted_docs = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    return formatted_docs\n",
    "\n",
    "template = \"\"\"You are tasked with analyzing a scientific paper and responding to a series of steps or questions based on the paper's content. \n",
    "Your goal is to provide accurate, contextual responses for each step, drawing from the paper's information and your own knowledge when necessary.\n",
    "Here is the paper you will be analyzing:\n",
    "    {context}\n",
    "\n",
    "1. Read the step carefully.\n",
    "2. Search for relevant information in the paper that addresses the step.\n",
    "3. If the paper contains information directly related to the step, use that information to formulate your response.\n",
    "4. If the paper does not contain information directly related to the step, but the topic is related to the paper's content, use your own knowledge to provide a response that is consistent with the paper's context and subject matter.\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = {\n",
    "    \"context\": lambda inputs: format_docs(query_vectordb(vectordb, inputs['query'])), \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "a45bd45b-e070-4d08-b4c8-d8673e9cb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_template: Title: \"Machine Teaching: A Paradigm Shift in Education\"\n",
      "Subtitle: \"Exploring the Potential of Teaching Computers to Learn Tasks\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c34ce059c0140eab66baf540f88176d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/196d177e-873c-4acc-b90d-4350cb9c8bde/82483762-ec15-4c56-93f9-6abf386871f0?taskType=12\n",
      "introduction_template: Introduction:\n",
      "\n",
      "In the field of machine learning and artificial intelligence, the concept of generalization plays a crucial role in enabling computers to learn and adapt to various tasks. This topic is academically important as it forms the basis of how machines can be trained to solve complex problems and make decisions based on data. Practically, the ability to generalize allows for the development of intelligent systems that can perform tasks such as image recognition, language translation, and data classification.\n",
      "\n",
      "Previous research, including the work of Turing, has highlighted the importance of teaching computers how to learn to solve tasks rather than providing them with specific solutions. This approach forms the foundation of machine learning algorithms that aim to optimize performance by learning from examples.\n",
      "\n",
      "The research problem addressed in this paper revolves around the limitations of traditional algorithmic design schemes in solving complex tasks such as image identification and language translation. The specific objectives of the research include exploring the concept of generalization in machine learning, understanding the challenges associated with scaling algorithmic designs, and proposing alternative approaches to training computers to solve tasks.\n",
      "\n",
      "Hypothesis: Based on the discussion in the paper, it can be hypothesized that teaching computers to learn how to solve tasks through machine learning algorithms will lead to more scalable and efficient solutions compared to traditional algorithmic design approaches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c605c6d0e4e0460586fa092d33f571e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/6e4fe941-78f6-4990-a76d-229f512050ce/60b1568c-c4b2-44c9-b56a-3f4d09134e3a?taskType=12\n",
      "methodology_template: Methodology:\n",
      "\n",
      "Research Design:\n",
      "The research design in the paper is focused on optimization in the mean squared error sense. It involves finding a set of weights that best explains a set of observations. The mathematical program outlined in the paper aims to optimize a function that minimizes the squared differences between the predicted and actual values.\n",
      "\n",
      "Sample and Population Details:\n",
      "The paper does not explicitly mention the sample or population details. However, it refers to a set of observations, weights, and a matrix representing preferences for movies, indicating that the analysis is based on a specific dataset or sample related to movie preferences.\n",
      "\n",
      "Data Collection Methods:\n",
      "The paper does not specify the data collection methods used. It primarily focuses on mathematical optimization techniques for finding the best weights to explain the observations.\n",
      "\n",
      "Instruments Used for Data Collection:\n",
      "Since the paper is theoretical in nature and focuses on mathematical optimization, there are no specific instruments mentioned for data collection. The analysis is based on mathematical models and algorithms.\n",
      "\n",
      "Data Analysis Techniques:\n",
      "The primary data analysis technique discussed in the paper is optimization in the mean squared error sense. The goal is to minimize the squared differences between predicted and actual values by finding the optimal set of weights. Additionally, the paper mentions the use of estimators and Hessian matrices for analysis and optimization purposes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55bb02a345e4b48a8e38040f745db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/d2394703-9a44-45d9-9575-7578921bf759/6d929965-f226-45c9-b56d-a44048dd0d14?taskType=12\n",
      "results_template: Based on the provided excerpt from the paper, it seems to focus on hyperparameter optimization and variance reduction. However, the specific details related to generating results with visual aids like graphs and tables, initial interpretation of the data, or comparison with hypotheses are not explicitly mentioned in the excerpt. \n",
      "\n",
      "In order to generate a results section based on this paper, we can interpret the optimization goal stated in the mathematical program presented in the excerpt. The goal is to find a set of weights that explains the data points (Rd, bi) in the mean squared error sense. This optimization problem aims to minimize the sum of squared differences between the predicted values (a⊤i x) and the actual values (bi) for a given set of weights x.\n",
      "\n",
      "When presenting the results, one could include a table showing the optimized weights x along with corresponding predicted values and actual values. This table could provide insights into how well the weights explain the data points and how close the predicted values are to the actual values. Additionally, a graph could be created to visually represent the relationship between the predicted and actual values, possibly showing a trend of improvement in minimizing the mean squared error.\n",
      "\n",
      "In terms of initial interpretation, the results section could discuss how well the optimized weights performed in explaining the data points and whether the mean squared error was minimized effectively. This interpretation could provide insights into the effectiveness of the optimization approach in this specific context.\n",
      "\n",
      "In terms of comparison with hypotheses, if there were specific hypotheses or expectations regarding the optimization of weights to explain the data points, the results section could discuss how well the actual results align with these hypotheses. Any discrepancies or confirmations of hypotheses could be highlighted and discussed in this section.\n",
      "\n",
      "Overall, the results section could provide a comprehensive overview of the optimization process, the performance of the optimized weights, and any insights gained from comparing the results with initial hypotheses or expectations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6852495b82f419f89e2a0cd72fbe5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/ae5c13eb-7102-465c-8344-2e219c56a368/d592dc29-6c34-4b4d-ae59-94686f1d3413?taskType=12\n",
      "conclusion_template: The conclusion of the study involves synthesizing the results obtained from the analysis. The research problem addressed in the paper revolves around generalization error, regret, and their implications on statistical learning theory. The study contributes to a deeper understanding of the relationship between regret and generalization, highlighting how regret can imply generalization in learning from examples. The final reflection on the study's impact suggests that by considering regret and optimization error, one can gain insights into the convergence of decision-making strategies in statistical learning. Practical recommendations may include further exploration of the connections between regret and generalization in different learning scenarios to enhance predictive models and decision-making processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da96234798984afca712b7ac4248af17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/34e526a8-6799-42b5-b2a6-26e582ace815/aa638463-29b3-4970-ba3c-b66786a7d76b?taskType=12\n",
      "references_template: To generate the list of references for the study, we need to include all the sources cited in the presentation. However, since the provided text is an excerpt from a scientific paper and does not contain any specific citations or references, we are unable to generate a list of references based solely on the content provided.\n",
      "\n",
      "In a typical scientific paper, references are usually included at the end of the document in a separate section. These references would be formatted according to a specific style such as APA, MLA, or Chicago, depending on the guidelines provided by the publication or institution. If this paper were part of a larger document, we would need to refer to the complete document to gather the necessary information for generating the references.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve the result? (y/n):  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ba10eb50fb40419f892aaa71abce01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Calculating the metric perplexity requires credentials for OpenAI or Azure to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/4e3f3df7-098e-4d71-b123-959e682d02b2/895c2227-8310-4465-93e2-fb43f9810254?taskType=12\n",
      "Final Script:\n",
      " Title: \"Machine Teaching: A Paradigm Shift in Education\"\n",
      "Subtitle: \"Exploring the Potential of Teaching Computers to Learn Tasks\"\n",
      "\n",
      "Introduction:\n",
      "\n",
      "In the field of machine learning and artificial intelligence, the concept of generalization plays a crucial role in enabling computers to learn and adapt to various tasks. This topic is academically important as it forms the basis of how machines can be trained to solve complex problems and make decisions based on data. Practically, the ability to generalize allows for the development of intelligent systems that can perform tasks such as image recognition, language translation, and data classification.\n",
      "\n",
      "Previous research, including the work of Turing, has highlighted the importance of teaching computers how to learn to solve tasks rather than providing them with specific solutions. This approach forms the foundation of machine learning algorithms that aim to optimize performance by learning from examples.\n",
      "\n",
      "The research problem addressed in this paper revolves around the limitations of traditional algorithmic design schemes in solving complex tasks such as image identification and language translation. The specific objectives of the research include exploring the concept of generalization in machine learning, understanding the challenges associated with scaling algorithmic designs, and proposing alternative approaches to training computers to solve tasks.\n",
      "\n",
      "Hypothesis: Based on the discussion in the paper, it can be hypothesized that teaching computers to learn how to solve tasks through machine learning algorithms will lead to more scalable and efficient solutions compared to traditional algorithmic design approaches.\n",
      "\n",
      "Methodology:\n",
      "\n",
      "Research Design:\n",
      "The research design in the paper is focused on optimization in the mean squared error sense. It involves finding a set of weights that best explains a set of observations. The mathematical program outlined in the paper aims to optimize a function that minimizes the squared differences between the predicted and actual values.\n",
      "\n",
      "Sample and Population Details:\n",
      "The paper does not explicitly mention the sample or population details. However, it refers to a set of observations, weights, and a matrix representing preferences for movies, indicating that the analysis is based on a specific dataset or sample related to movie preferences.\n",
      "\n",
      "Data Collection Methods:\n",
      "The paper does not specify the data collection methods used. It primarily focuses on mathematical optimization techniques for finding the best weights to explain the observations.\n",
      "\n",
      "Instruments Used for Data Collection:\n",
      "Since the paper is theoretical in nature and focuses on mathematical optimization, there are no specific instruments mentioned for data collection. The analysis is based on mathematical models and algorithms.\n",
      "\n",
      "Data Analysis Techniques:\n",
      "The primary data analysis technique discussed in the paper is optimization in the mean squared error sense. The goal is to minimize the squared differences between predicted and actual values by finding the optimal set of weights. Additionally, the paper mentions the use of estimators and Hessian matrices for analysis and optimization purposes.\n",
      "\n",
      "Based on the provided excerpt from the paper, it seems to focus on hyperparameter optimization and variance reduction. However, the specific details related to generating results with visual aids like graphs and tables, initial interpretation of the data, or comparison with hypotheses are not explicitly mentioned in the excerpt. \n",
      "\n",
      "In order to generate a results section based on this paper, we can interpret the optimization goal stated in the mathematical program presented in the excerpt. The goal is to find a set of weights that explains the data points (Rd, bi) in the mean squared error sense. This optimization problem aims to minimize the sum of squared differences between the predicted values (a⊤i x) and the actual values (bi) for a given set of weights x.\n",
      "\n",
      "When presenting the results, one could include a table showing the optimized weights x along with corresponding predicted values and actual values. This table could provide insights into how well the weights explain the data points and how close the predicted values are to the actual values. Additionally, a graph could be created to visually represent the relationship between the predicted and actual values, possibly showing a trend of improvement in minimizing the mean squared error.\n",
      "\n",
      "In terms of initial interpretation, the results section could discuss how well the optimized weights performed in explaining the data points and whether the mean squared error was minimized effectively. This interpretation could provide insights into the effectiveness of the optimization approach in this specific context.\n",
      "\n",
      "In terms of comparison with hypotheses, if there were specific hypotheses or expectations regarding the optimization of weights to explain the data points, the results section could discuss how well the actual results align with these hypotheses. Any discrepancies or confirmations of hypotheses could be highlighted and discussed in this section.\n",
      "\n",
      "Overall, the results section could provide a comprehensive overview of the optimization process, the performance of the optimized weights, and any insights gained from comparing the results with initial hypotheses or expectations.\n",
      "\n",
      "The conclusion of the study involves synthesizing the results obtained from the analysis. The research problem addressed in the paper revolves around generalization error, regret, and their implications on statistical learning theory. The study contributes to a deeper understanding of the relationship between regret and generalization, highlighting how regret can imply generalization in learning from examples. The final reflection on the study's impact suggests that by considering regret and optimization error, one can gain insights into the convergence of decision-making strategies in statistical learning. Practical recommendations may include further exploration of the connections between regret and generalization in different learning scenarios to enhance predictive models and decision-making processes.\n",
      "\n",
      "To generate the list of references for the study, we need to include all the sources cited in the presentation. However, since the provided text is an excerpt from a scientific paper and does not contain any specific citations or references, we are unable to generate a list of references based solely on the content provided.\n",
      "\n",
      "In a typical scientific paper, references are usually included at the end of the document in a separate section. These references would be formatted according to a specific style such as APA, MLA, or Chicago, depending on the guidelines provided by the publication or institution. If this paper were part of a larger document, we would need to refer to the complete document to gather the necessary information for generating the references.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_and_approve(variable_name, template, question, vectordb, k=10):\n",
    "    \"\"\"\n",
    "    Function that executes a chain of prompts to generate text based on a template and a question,\n",
    "    with the option of manually approving the result.\n",
    "\n",
    "    Parameters:\n",
    "    - variable_name (str): Name of the variable to be used when printing the result.\n",
    "    - template (str): Prompt template that will be used as context to generate the text.\n",
    "    - question (str): Specific task that the model must perform, such as \"create a title\" or \"generate an introduction\".\n",
    "    - vectordb: Vector database used to search for documents related to the context.\n",
    "    - k (int, optional): Number of documents to be retrieved from the vector database. The default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text generated and approved by the user based on the template and the question.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Configure a new prompt_handler for each step\n",
    "        prompt_handler = pq.GalileoPromptCallback(\n",
    "            scorers=[\n",
    "                pq.Scorers.context_adherence_plus,\n",
    "                pq.Scorers.correctness,\n",
    "                pq.Scorers.prompt_perplexity\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Execute the chain in batch with the template as query and a specific question\n",
    "        result = chain.batch([{\"query\": template, \"question\": question}], config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        # Print the variable name and result\n",
    "        print(f\"{variable_name}: {result[0]}\")\n",
    "\n",
    "        approval = input(\"Approve the result? (y/n): \").strip().lower()\n",
    "        if approval == 'y':\n",
    "            # Finalize and send the prompt to Galileo\n",
    "            prompt_handler.finish()\n",
    "            return result[0]  # Return the approved result\n",
    "\n",
    "        print(\"Result not approved, generating again...\\n\")\n",
    "\n",
    "# Example execution for each cell\n",
    "\n",
    "\n",
    "title_template = \"\"\"\n",
    "Generate a title for the presentation that is clear, concise, and reflects the content. Add a subtitle if needed.\n",
    "\"\"\"\n",
    "title_result = run_and_approve(\"title_template\", title_template, question=\"create a title\", vectordb=vectordb)\n",
    "\n",
    "# Introduction\n",
    "introduction_template = \"\"\"\n",
    "Generate an introduction that includes:\n",
    "- Contextualization of the general theme.\n",
    "- Relevance of the topic, both academically and practically.\n",
    "- A brief literature review.\n",
    "- A clear definition of the research problem.\n",
    "- The specific objectives of the research.\n",
    "- Hypotheses (if applicable).\n",
    "\"\"\"\n",
    "introduction_result = run_and_approve(\"introduction_template\", introduction_template, question=\"generate an introduction\", vectordb=vectordb)\n",
    "\n",
    "# Methodology\n",
    "methodology_template = \"\"\"\n",
    "Generate the methodology section, including:\n",
    "- Research Design (e.g., experimental, descriptive, exploratory).\n",
    "- Sample and Population details.\n",
    "- Data Collection methods.\n",
    "- Instruments used for data collection.\n",
    "- Data Analysis techniques.\n",
    "\"\"\"\n",
    "methodology_result = run_and_approve(\"methodology_template\", methodology_template, question=\"generate the methodology\", vectordb=vectordb)\n",
    "\n",
    "#  Results\n",
    "results_template = \"\"\"\n",
    "Generate the results section, including:\n",
    "- Presentation of Data with visual aids like graphs and tables.\n",
    "- Initial Interpretation of the data.\n",
    "- Comparison with Hypotheses (if applicable).\n",
    "\"\"\"\n",
    "results_result = run_and_approve(\"results_template\", results_template, question=\"generate the results\", vectordb=vectordb)\n",
    "\n",
    "# Conclusion\n",
    "conclusion_template = \"\"\"\n",
    "Generate the conclusion of the study, including:\n",
    "- Synthesis of Results.\n",
    "- Response to the Research Problem.\n",
    "- Study Contributions.\n",
    "- Final Reflection on the study's impact or practical recommendations.\n",
    "\"\"\"\n",
    "conclusion_result = run_and_approve(\"conclusion_template\", conclusion_template, question=\"generate the conclusion\", vectordb=vectordb)\n",
    "\n",
    "# References\n",
    "references_template = \"\"\"\n",
    "Generate the list of references for the study, ensuring that:\n",
    "- All sources cited in the presentation are included.\n",
    "- The references are formatted according to a specific style (APA, MLA, Chicago).\n",
    "\"\"\"\n",
    "references_result = run_and_approve(\"references_template\", references_template, question=\"generate the references\", vectordb=vectordb)\n",
    "\n",
    "# Combine the results into a final script\n",
    "final_script = f\"{title_result}\\n\\n{introduction_result}\\n\\n{methodology_result}\\n\\n{results_result}\\n\\n{conclusion_result}\\n\\n{references_result}\"\n",
    "print(\"Final Script:\\n\", final_script)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
