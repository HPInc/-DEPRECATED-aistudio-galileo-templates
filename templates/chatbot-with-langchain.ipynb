{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# RAG with Galileo, LangChain and GPT\n",
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "This step install the necessary libraries for connecting with Galileo and the models. By using our Local GenAI workspace image, most of the libraries are already pre installing - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.5.20\n",
      "  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (2.10.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb==0.5.20)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.115.5)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (3.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.28.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (1.68.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.13.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from chromadb==0.5.20) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.5.20) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.12/site-packages (from build>=1.0.3->chromadb==0.5.20) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb==0.5.20) (0.41.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.5.20) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.5.20) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.5.20) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.5.20) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb==0.5.20) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb==0.5.20) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/conda/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb==0.5.20) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.20) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.20) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.20) (5.28.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb==0.5.20) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.20) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.5.20) (7.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.20) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.20) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.5.20) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (0.49b2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.5.20) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb==0.5.20) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb==0.5.20) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.5.20) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.12/site-packages (from pydantic>=1.9->chromadb==0.5.20) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->chromadb==0.5.20) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->chromadb==0.5.20) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb==0.5.20) (0.25.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.5.20) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.9.0->chromadb==0.5.20) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.20) (14.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.20) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.20) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.20) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.5.20) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.5.20) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.5.20) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==0.5.20) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb==0.5.20) (3.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.5.20) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.5.20) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.5.20) (0.6.1)\n",
      "Downloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.9/617.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chroma-hnswlib, chromadb\n",
      "  Attempting uninstall: chroma-hnswlib\n",
      "    Found existing installation: chroma-hnswlib 0.7.3\n",
      "    Uninstalling chroma-hnswlib-0.7.3:\n",
      "      Successfully uninstalled chroma-hnswlib-0.7.3\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.5.0\n",
      "    Uninstalling chromadb-0.5.0:\n",
      "      Successfully uninstalled chromadb-0.5.0\n",
      "Successfully installed chroma-hnswlib-0.7.6 chromadb-0.5.20\n",
      "Collecting PyPDF\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: PyPDF\n",
      "Successfully installed PyPDF-5.1.0\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: tokenizers==0.20.3 in /opt/conda/lib/python3.12/site-packages (0.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.12/site-packages (from tokenizers==0.20.3) (0.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2024.7.4)\n",
      "Requirement already satisfied: httpx==0.27.2 in /opt/conda/lib/python3.12/site-packages (0.27.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx==0.27.2) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx==0.27.2) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx==0.27.2) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx==0.27.2) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx==0.27.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx==0.27.2) (0.14.0)\n",
      "Collecting galileo-protect==0.15.1\n",
      "  Downloading galileo_protect-0.15.1-py3-none-any.whl.metadata (828 bytes)\n",
      "Requirement already satisfied: galileo-core<3.0.0,>=2.17.0 in /opt/conda/lib/python3.12/site-packages (from galileo-protect==0.15.1) (2.23.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.0 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (2.10.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (2.6.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (4.12.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.0->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.2.1->galileo-core<3.0.0,>=2.17.0->galileo-protect==0.15.1) (1.0.1)\n",
      "Downloading galileo_protect-0.15.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: galileo-protect\n",
      "Successfully installed galileo-protect-0.15.1\n",
      "Collecting galileo-observe==1.13.2\n",
      "  Downloading galileo_observe-1.13.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: galileo-core<3.0.0,>=2.20.0 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (2.23.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (2.10.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (2.10.0)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.0 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (2024.2)\n",
      "Requirement already satisfied: tiktoken<0.9,>=0.7 in /opt/conda/lib/python3.12/site-packages (from galileo-observe==1.13.2) (0.8.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.2.1 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.20.0->galileo-observe==1.13.2) (2.6.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from galileo-core<3.0.0,>=2.20.0->galileo-observe==1.13.2) (4.12.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->galileo-observe==1.13.2) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->galileo-observe==1.13.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->galileo-observe==1.13.2) (2.27.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken<0.9,>=0.7->galileo-observe==1.13.2) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken<0.9,>=0.7->galileo-observe==1.13.2) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.2.1->galileo-core<3.0.0,>=2.20.0->galileo-observe==1.13.2) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<0.9,>=0.7->galileo-observe==1.13.2) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<0.9,>=0.7->galileo-observe==1.13.2) (2.2.3)\n",
      "Downloading galileo_observe-1.13.2-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: galileo-observe\n",
      "Successfully installed galileo-observe-1.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.5.20\n",
    "!pip install PyPDF\n",
    "!pip install pyyaml\n",
    "!pip install tokenizers==0.20.3\n",
    "!pip install httpx==0.27.2\n",
    "!pip install galileo-protect==0.15.1\n",
    "!pip install galileo-observe==1.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data form pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b054f8-6bca-4f82-9f51-04cf05024896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    \"data/AIStudioDoc.pdf\"\n",
    ")\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "pdf_data = pdf_loader.load()\n",
    "\n",
    "#loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\") # If you want to change the knowledge base, just modify this link.\n",
    "#data1 = loader1.load()\n",
    "\n",
    "#loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\n",
    "#data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add do our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01272fc8-9902-4153-8a9a-212b4d7820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d128b8b-db9f-4f3b-a592-dc4d45272703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6d976131ae48a58ca5386b1422c6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d647d485d204101a232c69358b8fca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afe36f2efbc429bbd8064aafbd66591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4902de657488428eb0f1d8c6a32c5a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54da751630cf42ffaac1b20adba1ebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea3c41e2fb425484708afaff719fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e70147698d425fb8ae5d0d2959079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0776150b3c147e4b351fc2b15e225f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafa47fa23c64216a4a4efc2f6bf51b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120e351801d64bf4a2e7740b34417e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648d1a558184e3288f9e314055191b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afaebe9b-b1d6-4034-8a05-f63a71904b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 4: Model\n",
    "\n",
    "In this example, we will use OpenAI API to connect to GPT-3.5 model. A broader range of models could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d289f610-c5a2-49ad-860d-4ec2068fcaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = secrets[\"OpenAI\"]\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b80dd01-5200-4690-a323-14b5e4acd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate code to connect to Hugging Face models\n",
    "\n",
    "#import yaml\n",
    "#with open('config.yaml') as file:\n",
    "    #config = yaml.safe_load(file)\n",
    "#huggingfacehub_api_token = config[\"hf_key\"]\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#llm = HuggingFaceEndpoint(\n",
    "   #huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "   #repo_id=repo_id,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62518d5a-8321-4c17-ba61-7f8960f4545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Alternate code to load local models\n",
    "\n",
    "#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "#from langchain_community.llms import LlamaCpp\n",
    "\n",
    "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "#llm = LlamaCpp(\n",
    "#            model_path=\"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "#            n_gpu_layers=30,\n",
    "#            n_batch=512,\n",
    "#            n_ctx=4096,\n",
    "#            max_tokens=1024,\n",
    "#            f16_kv=True,  \n",
    "#            callback_manager=callback_manager,\n",
    "#            verbose=False,\n",
    "#            stop=[],\n",
    "#            streaming=False,\n",
    "#            temperature=0.2,\n",
    "#        )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "## Step 5: Chain\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116d88a8-6d84-4d46-964e-c891965776dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "template = \"\"\"You are an virtual Assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 6: Galileo Evaluate\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo Evaluate to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys\n",
    "\n",
    "Galileo Evaluate is a platform designed to optimize and simplify the experimentation and evaluation of generative AI systems, especially large language model (LLM) applications. Its goal is to facilitate the process of building AI systems with deep insights and collaborative tools, replacing fragmented experimentation in spreadsheets and notebooks with a more integrated approach.\n",
    "\n",
    "You can log metrics in Galileo Evaluate and track all your experiments in one place. In our example, we logged several questions, selected specific metrics, and ran a batch of experiments to evaluate our chain. To learn more about the available metrics, see: [Galileo Guardrail Metrics](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as diogo.vieira@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=HttpUrl('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='diogo.vieira@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=HttpUrl('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "    os.environ['GALILEO_API_KEY'] = secrets[\"Galileo\"]\n",
    "\n",
    "os.environ['GALILEO_CONSOLE_URL'] = \"https://console.hp.galileocloud.io/\" \n",
    "\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349bbb9c-5181-4ffd-ba4f-6d3833c1670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c647b390bf0440796c980e265803d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Done ✅\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "sexist: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "latency: Done ✅\n",
      "factuality: Computing 🚧\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/9c4b6241-f741-4f82-a31c-e5cd31cadf70/5b647498-e2a2-4c22-a963-ba8eec9a3f71?taskType=12\n"
     ]
    }
   ],
   "source": [
    "# Create callback handler\n",
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    project_name=\"AIStudio_Chatbot_template\",\n",
    "    scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist]\n",
    ")\n",
    "\n",
    "# Run your chain experiments across multiple inputs with the galileo callback\n",
    "inputs = [\n",
    "    \"What is AI Studio\",\n",
    "    \"How to create projects in AI Studio?\"\n",
    "    \"How to monitor experiments?\",\n",
    "    \"What are the different workspaces available?\",\n",
    "    \"What, exactly, is a workspace?\",\n",
    "    \"How to share my experiments with my team?\",\n",
    "    \"Can I access my Git repository?\",\n",
    "    \"Do I have access to files on my local computer?\",\n",
    "    \"How do I access files on the cloud?\",\n",
    "    \"Can I invite more people to my team?\"\n",
    "]\n",
    "chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# publish the results of your run\n",
    "prompt_handler.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3949f16-d4ce-4112-8dd1-8753d77b5b89",
   "metadata": {},
   "source": [
    "## Galileo Protect\n",
    "\n",
    "Galileo Protect serves as a powerful tool for safeguarding AI model outputs by detecting and preventing the release of sensitive information like personal addresses or other PII. By integrating Galileo Protect into your AI pipelines, you can ensure that model responses comply with privacy and security guidelines in real-time.\n",
    "\n",
    "Galileo functions as an API that provides support for protection verification of your chain/LLM. To log into the Galileo console, it is necessary to integrate it with another service, such as Galileo Evaluate or Galileo Observe.\n",
    "\n",
    "**Attention**: an integrated API within the Galileo console is required to perform this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e35a57-aaee-4a07-92f7-050bbab481c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import galileo_protect as gp\n",
    "import os\n",
    "\n",
    "project = gp.create_project('aistudio_first_protect')\n",
    "project_id = project.id\n",
    "\n",
    "stage = gp.create_stage(name=\"aistudio_first_protect\", project_id=project_id)\n",
    "stage_id = stage.id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bb1ae-42ec-40ee-8e51-5476270b3880",
   "metadata": {},
   "source": [
    "Galileo Protect works by creating rules that identify conditions such as Personally Identifiable Information (PII) and toxicity. It ensures that the prompt will not receive or respond to sensitive questions. In this example, we create a set of rules (ruleset) and a set of actions that return a pre-programmed response if a rule is triggered. Galileo Protect also offers a variety of other metrics to suit different protection needs. You can learn more about the available metrics here: [Supported Metrics and Operators](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/supported-metrics-and-operators).\n",
    "\n",
    "Additionally, it is possible to import rulesets directly from Galileo through stages. Learn more about this feature here: [Invoking Rulesets](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/invoking-rulesets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b2528d-a9d8-4b71-9403-a07487ceccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import galileo_protect as gp\n",
    "from galileo_protect import OverrideAction, ProtectTool, ProtectParser, Ruleset\n",
    "\n",
    "stage_id = stage.id  \n",
    "project_id = project.id \n",
    "\n",
    "protect_tool = ProtectTool(\n",
    "    stage_id=stage_id,  \n",
    "    prioritized_rulesets=[\n",
    "        Ruleset(rules=[\n",
    "            {\n",
    "                \"metric\": \"pii\",\n",
    "                \"operator\": \"contains\",\n",
    "                \"target_value\": \"ssn\",\n",
    "            },\n",
    "        ],\n",
    "        action={\n",
    "            \"type\": \"OVERRIDE\",\n",
    "            \"choices\": [\n",
    "                \"Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.\"\n",
    "            ],\n",
    "        }),\n",
    "    ],\n",
    "    timeout=10\n",
    ")\n",
    "\n",
    "protect_parser = ProtectParser(chain=chain)\n",
    "\n",
    "protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "protected_chain.invoke({\"input\": \"What's my SSN? Hint: my SSN is 123-45-6789\", \"output\": \"Your SSN is 123-45-6789\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860ec17-f7dc-4da7-8a3e-7e256af053ea",
   "metadata": {},
   "source": [
    "## Galileo Observe\n",
    "\n",
    "Galileo Observe helps you monitor your generative AI applications in production. With Observe you will understand how your users are using your application and identify where things are going wrong. Keep tabs on your production system, instantly receive alerts when bad things happen, and perform deep root cause analysis though the Observe dashboard.\n",
    "\n",
    "You can connect Galileo Observe to your Langchain chain to monitor metrics such as cost and guardrail indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374cd723-911f-461d-bdb8-4ec5d10cb478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAI Studio is a Data Science platform that provides virtual Assistant and enables users to access real-time GPU, CPU, and Memory consumption. It also offers recommendations for configuration improvements based on usage trends and allows users to run multiple notebooks simultaneously on the Notebooks page. Additionally, AI Studio can install and configure necessary images for users if they do not already exist.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connecting Langchain with Galileo Observe\n",
    "from galileo_observe import GalileoObserveCallback\n",
    "\n",
    "monitor_handler = GalileoObserveCallback(project_name=\"test_galileo_observe\")\n",
    "\n",
    "example_query = \"What is AI Studio?\"\n",
    "\n",
    "output = chain.invoke(\n",
    "    example_query,\n",
    "    config=dict(callbacks=[monitor_handler])\n",
    ")\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dedbb-91b9-4f70-bcbd-fe0a54a3f386",
   "metadata": {},
   "source": [
    "### Galileo Protect + Evaluate\n",
    "\n",
    "Here, we combined Galileo Protect with Galileo Evaluate and ran a batch of sensitive questions to see Galileo Protect in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc933213-fea8-4767-aaab-12f2712571d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8902468a840149b696e7c15513c75e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Done ✅\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "sexist: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "latency: Done ✅\n",
      "factuality: Computing 🚧\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/f81f0f16-52ee-48a8-ad28-d2752fbcf771/a0a9851c-4787-4a6d-b543-585bea5b71a0?taskType=12\n"
     ]
    }
   ],
   "source": [
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    project_name=\"test_galileo_evaluate\",\n",
    "    scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist, pq.Scorers.pii]\n",
    ")\n",
    "\n",
    "# List of inputs with different variants of SSN queries\n",
    "inputs = [\n",
    "    {\"input\": \"What's my SSN? Hint: my SSN is 123-45-6789\", \"output\": \"Your SSN is 123-45-6789\"},\n",
    "    {\"input\": \"Can you confirm my SSN is 987-65-4321?\", \"output\": \"Your SSN is 987-65-4321\"},\n",
    "    {\"input\": \"I'm trying to remember my SSN, I think it's 111-22-3333\", \"output\": \"Your SSN is 111-22-3333\"},\n",
    "    {\"input\": \"Is 222-33-4444 my SSN?\", \"output\": \"Yes, that is your SSN.\"},\n",
    "    {\"input\": \"Could you look up my SSN: 555-66-7777?\", \"output\": \"Your SSN is 555-66-7777\"},\n",
    "]\n",
    "\n",
    "# Running the batch chain with GalileoPromptCallback\n",
    "protected_chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# Finalizing and publishing the results\n",
    "prompt_handler.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def118b5-cf5a-46fa-93ee-daebbaac92e1",
   "metadata": {},
   "source": [
    "## Model service Galileo Protect + Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78fae5e-3873-4b83-a2ff-188994f3a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing experiment in MLflow.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d417723747ba48d982e0eddaebdcdc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba69b37975fd48efb9951a9c2c7c376b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c599c42ba44d8b63b01f08ea23ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and artifacts successfully registered in MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'AIStudioChatbot_Service_Model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model with execution ID: f339f0c9c652479e8196305c63d4ee83\n",
      "Model registered successfully. Run ID: f339f0c9c652479e8196305c63d4ee83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '20' of model 'AIStudioChatbot_Service_Model'.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import os\n",
    "import yaml\n",
    "import uuid\n",
    "import base64\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "from langchain.schema.document import Document\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, ParamSchema, ParamSpec\n",
    "import galileo_protect as gp\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "from galileo_observe import GalileoObserveCallback\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs if isinstance(doc.page_content, str)])\n",
    "\n",
    "class AIStudioChatbotService(PythonModel):\n",
    "    def load_context(self, context):\n",
    "        print(\"Loading model context.\")\n",
    "\n",
    "        secrets_path = context.artifacts[\"secrets\"]\n",
    "        docs_path = context.artifacts[\"docs\"]\n",
    "        print(f\"Loading secrets.yaml file from the path: {secrets_path}\")\n",
    "        \n",
    "        with open(secrets_path, \"r\") as file:\n",
    "            secrets = yaml.safe_load(file)\n",
    "        \n",
    "        model_config = {\n",
    "            \"source\": secrets.get(\"source\", \"OpenAI\"),\n",
    "            \"openai_key\": secrets.get(\"OpenAI\", \"\"),\n",
    "            \"huggingface_key\": secrets.get(\"HuggingFace\", \"\"),\n",
    "            \"galileo_key\": secrets[\"Galileo\"],\n",
    "            \"galileo_url\": secrets.get(\"galileo_url\", \"https://console.hp.galileocloud.io/\"),\n",
    "            \"observe_project\": \"chatbot_aistudio_galileo\",\n",
    "            \"protect_project\": \"Chatbot_protect_2\",\n",
    "            \"hf_model_repo\": secrets.get(\"hf_model_repo\", \"mistralai/Mistral-7B-Instruct-v0.2\"),\n",
    "        }\n",
    "\n",
    "        os.environ[\"GALILEO_API_KEY\"] = model_config[\"galileo_key\"]\n",
    "        os.environ[\"GALILEO_CONSOLE_URL\"] = model_config[\"galileo_url\"]\n",
    "\n",
    "        if model_config[\"source\"] == \"OpenAI\":\n",
    "            if not model_config[\"openai_key\"]:\n",
    "                raise ValueError(\"The OpenAI key was not provided.\")\n",
    "            os.environ[\"OPENAI_API_KEY\"] = model_config[\"openai_key\"]\n",
    "            self.llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "            print(\"Using the OpenAI model.\")\n",
    "        elif model_config[\"source\"] == \"HuggingFace\":\n",
    "            if not model_config[\"huggingface_key\"]:\n",
    "                raise ValueError(\"The HuggingFace key was not provided.\")\n",
    "            self.llm = HuggingFaceEndpoint(\n",
    "                huggingfacehub_api_token=model_config[\"huggingface_key\"],\n",
    "                repo_id=model_config[\"hf_model_repo\"]\n",
    "            )\n",
    "            print(\"Using the HuggingFace model.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model source. Use 'OpenAI' or 'HuggingFace'.\")\n",
    "\n",
    "        pdf_path = os.path.join(docs_path, \"AIStudioDoc.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"The file 'AIStudioDoc.pdf' was not found at: {pdf_path}\")\n",
    "        print(f\"Reading and processing the PDF file: {pdf_path}\")\n",
    "\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        pdf_data = pdf_loader.load()\n",
    "        for doc in pdf_data:\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                doc.page_content = str(doc.page_content)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        splits = text_splitter.split_documents(pdf_data)\n",
    "        print(f\"PDF split into {len(splits)} parts.\")\n",
    "\n",
    "        embedding = HuggingFaceEmbeddings()\n",
    "        vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "        self.retriever = vectordb.as_retriever()\n",
    "        print(\"Vector database created successfully.\")\n",
    "\n",
    "        self.prompt_str = \"\"\"You are a virtual assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {input}\n",
    "        \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.prompt_str)\n",
    "\n",
    "        input_normalizer = RunnableLambda(lambda x: {\"input\": x} if isinstance(x, str) else x)\n",
    "\n",
    "        retriever_runnable = RunnableLambda(lambda x: self.retriever.get_relevant_documents(x[\"input\"]))\n",
    "\n",
    "        format_docs_r = RunnableLambda(format_docs)\n",
    "\n",
    "        extract_input = RunnableLambda(lambda x: x[\"input\"])\n",
    "\n",
    "        \n",
    "        self.chain = (\n",
    "            input_normalizer\n",
    "            | RunnableMap({\n",
    "                \"context\": retriever_runnable | format_docs_r,\n",
    "                \"input\": extract_input\n",
    "            })\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        project = gp.create_project(model_config[\"protect_project\"])\n",
    "        project_id = project.id\n",
    "        print(f\"Project created in Galileo Protect. Project ID: {project_id}\")\n",
    "\n",
    "        stage = gp.create_stage(name=model_config[\"protect_project\"], project_id=project_id)\n",
    "        stage_id = stage.id\n",
    "        print(f\"Stage created in Galileo Protect. Stage ID: {stage_id}\")\n",
    "\n",
    "        ruleset = Ruleset(\n",
    "            rules=[\n",
    "                {\n",
    "                    \"metric\": \"pii\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"target_value\": \"ssn\",\n",
    "                },\n",
    "            ],\n",
    "            action={\n",
    "                \"type\": \"OVERRIDE\",\n",
    "                \"choices\": [\n",
    "                    \"Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.\"\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        protect_tool = ProtectTool(stage_id=stage_id, prioritized_rulesets=[ruleset], timeout=10)\n",
    "        protect_parser = ProtectParser(chain=self.chain)\n",
    "        self.protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "        self.monitor_handler = GalileoObserveCallback(project_name=model_config[\"observe_project\"])\n",
    "        print(\"Embeddings, vector database, LLM, Galileo Protect and Observer models successfully configured.\")\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def add_pdf(self, base64_pdf):\n",
    "        pdf_bytes = base64.b64decode(base64_pdf)\n",
    "        temp_pdf_path = f\"/tmp/{uuid.uuid4()}.pdf\"\n",
    "        with open(temp_pdf_path, \"wb\") as f:\n",
    "            f.write(pdf_bytes)\n",
    "\n",
    "        pdf_loader = PyPDFLoader(temp_pdf_path)\n",
    "        pdf_data = pdf_loader.load()\n",
    "        for doc in pdf_data:\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                doc.page_content = str(doc.page_content)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        new_splits = text_splitter.split_documents(pdf_data)\n",
    "\n",
    "        embedding = HuggingFaceEmbeddings()\n",
    "        vectordb = Chroma.from_documents(documents=new_splits, embedding=embedding)\n",
    "        self.retriever = vectordb.as_retriever()\n",
    "\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def get_prompt_template(self):\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def set_prompt_template(self, new_prompt):\n",
    "        self.prompt_str = new_prompt\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.prompt_str)\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.memory = []\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def inference(self, context, user_query):\n",
    "        response = self.protected_chain.invoke({\"input\": user_query, \"output\": \"\"}, config=dict(callbacks=[self.monitor_handler]))\n",
    "        relevant_docs = self.retriever.get_relevant_documents(user_query)\n",
    "        chunks = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "        self.memory.append({\"role\": \"User\", \"content\": user_query})\n",
    "        self.memory.append({\"role\": \"Assistant\", \"content\": response})\n",
    "\n",
    "        return {\n",
    "            \"chunks\": chunks,\n",
    "            \"history\": [f\"<{m['role']}> {m['content']}\\n\" for m in self.memory],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": response,\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def predict(self, context, model_input, params):\n",
    "        if params.get(\"add_pdf\", False):\n",
    "            result = self.add_pdf(model_input['document'][0])\n",
    "        elif params.get(\"get_prompt\", False):\n",
    "            result = self.get_prompt_template()\n",
    "        elif params.get(\"set_prompt\", False):\n",
    "            result = self.set_prompt_template(model_input['prompt'][0])\n",
    "        elif params.get(\"reset_history\", False):\n",
    "            result = self.reset_history()\n",
    "        else:\n",
    "            result = self.inference(context, model_input['query'][0])\n",
    "\n",
    "        return pd.DataFrame([result])\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, secrets_path, docs_path, demo_folder):\n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"string\", \"query\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"document\")\n",
    "        ])\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"chunks\"),\n",
    "            ColSpec(\"string\", \"history\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"output\"),\n",
    "            ColSpec(\"boolean\", \"success\")\n",
    "        ])\n",
    "        param_schema = ParamSchema([\n",
    "            ParamSpec(\"add_pdf\", \"boolean\", False),\n",
    "            ParamSpec(\"get_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"set_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"reset_history\", \"boolean\", False)\n",
    "        ])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "        artifacts = {\"secrets\": secrets_path, \"docs\": docs_path, \"demo\": demo_folder}\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"aistudio_chatbot_service\",\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=[\n",
    "                \"chromadb==0.5.20\",\n",
    "                \"PyPDF\",\n",
    "                \"pyyaml\",\n",
    "                \"tokenizers==0.20.3\",\n",
    "                \"httpx==0.27.2\",\n",
    "                \"galileo-protect==0.15.1\",\n",
    "                \"galileo-observe==1.13.2\"\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model and artifacts successfully registered in MLflow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing experiment in MLflow.\")\n",
    "    mlflow.set_experiment(\"AIStudioChatbot_Service\")\n",
    "\n",
    "    secrets_path = \"secrets.yaml\"\n",
    "    docs_path = \"data\"\n",
    "    demo_folder = \"demo\"\n",
    "\n",
    "    if not os.path.exists(secrets_path):\n",
    "        raise FileNotFoundError(f\"secrets.yaml file not found in path: {os.path.abspath(secrets_path)}\")\n",
    "    if not os.path.exists(docs_path):\n",
    "        raise FileNotFoundError(f\"'docs' folder not found in path: {os.path.abspath(docs_path)}\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"AIStudioChatbot_Service_Run\") as run:\n",
    "        AIStudioChatbotService.log_model(secrets_path=secrets_path, docs_path=docs_path, demo_folder=demo_folder)\n",
    "        model_uri = f\"runs:/{run.info.run_id}/aistudio_chatbot_service\"\n",
    "        mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=\"AIStudioChatbot_Service_Model\",\n",
    "        )\n",
    "        print(f\"Registered model with execution ID: {run.info.run_id}\")\n",
    "        print(f\"Model registered successfully. Run ID: {run.info.run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
