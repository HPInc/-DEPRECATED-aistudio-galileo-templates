{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb696e77-2b43-4599-91ee-32e70fe81af6",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "In this step, we are installing the libraries allowed for our project, which involve the use of LangChain, integration with Huggingface models, OpenAI, in addition to the storage of embeddings using ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baacc9ef-f902-4ff5-9a29-19159516c9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting promptquality\n",
      "  Downloading promptquality-0.62.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting galileo-core<3.0.0,>=2.13.0 (from promptquality)\n",
      "  Downloading galileo_core-2.13.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting importlib-metadata<9.0,>=8.0.0 (from promptquality)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting numpy>=1.25.0 (from promptquality)\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m282.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from promptquality) (2.9.1)\n",
      "Collecting pydantic-settings<3.0.0,>=2.1.0 (from promptquality)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from promptquality) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from promptquality) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from promptquality) (4.65.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from galileo-core<3.0.0,>=2.13.0->promptquality) (0.27.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from galileo-core<3.0.0,>=2.13.0->promptquality) (4.12.2)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<9.0,>=8.0.0->promptquality)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->promptquality) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->promptquality) (2.23.3)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.1.0->promptquality)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->promptquality) (2023.7.22)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.13.0->promptquality) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.13.0->promptquality) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.13.0->promptquality) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.13.0->promptquality) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->galileo-core<3.0.0,>=2.13.0->promptquality) (1.2.2)\n",
      "Downloading promptquality-0.62.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m881.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading galileo_core-2.13.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m821.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: zipp, python-dotenv, numpy, importlib-metadata, pydantic-settings, galileo-core, promptquality\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.19.2\n",
      "    Uninstalling zipp-3.19.2:\n",
      "      Successfully uninstalled zipp-3.19.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.11.0\n",
      "    Uninstalling importlib-metadata-6.11.0:\n",
      "      Successfully uninstalled importlib-metadata-6.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.8.2 requires numpy<2,>=1.21, but you have numpy 2.1.1 which is incompatible.\n",
      "mlflow 2.6.0 requires importlib-metadata!=4.7.0,<7,>=3.7.0, but you have importlib-metadata 8.5.0 which is incompatible.\n",
      "mlflow 2.6.0 requires numpy<2, but you have numpy 2.1.1 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.1.1 which is incompatible.\n",
      "pandas 2.2.0 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.1.1 which is incompatible.\n",
      "scikit-learn 1.4.0 requires numpy<2.0,>=1.19.5, but you have numpy 2.1.1 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.1.1 which is incompatible.\n",
      "sentence-transformers 3.1.0 requires numpy<2.0.0, but you have numpy 2.1.1 which is incompatible.\n",
      "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 2.1.1 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed galileo-core-2.13.0 importlib-metadata-8.5.0 numpy-2.1.1 promptquality-0.62.1 pydantic-settings-2.5.2 python-dotenv-1.0.1 zipp-3.20.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.24)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.121)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m232.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.8/446.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: numpy, multidict, frozenlist, async-timeout, aiohappyeyeballs, yarl, aiosignal, aiohttp, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.6.0 requires importlib-metadata!=4.7.0,<7,>=3.7.0, but you have importlib-metadata 8.5.0 which is incompatible.\n",
      "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 langchain-0.3.0 langchain-text-splitters-0.3.0 multidict-6.1.0 numpy-1.26.4 yarl-1.11.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.3.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.1.121)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (2.9.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (2.23.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.2.2)\n",
      "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.0 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /opt/conda/lib/python3.10/site-packages (from langchain-openai) (0.3.0)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
      "  Downloading openai-1.45.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (0.1.121)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (2.9.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.117->langchain-core<0.4,>=0.3->langchain-openai) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.2)\n",
      "Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m249.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.45.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m932.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, tiktoken, openai, langchain-openai\n",
      "Successfully installed jiter-0.5.0 langchain-openai-0.2.0 openai-1.45.1 tiktoken-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.9.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.6.6-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.19.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m333.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.64.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.3.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.23.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.9.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m603.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m473.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m319.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m325.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m245.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m269.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-3.6.6-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m979.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m657.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m924.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m825.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=35ea841567186ef4fc8c5b7bf909578682050b5aba9a7cf78e16228a8015f563\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, shellingham, pyproject_hooks, opentelemetry-util-http, opentelemetry-proto, mdurl, importlib-metadata, humanfriendly, httptools, googleapis-common-protos, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, markdown-it-py, coloredlogs, build, rich, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, typer, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.5.0\n",
      "    Uninstalling importlib_metadata-8.5.0:\n",
      "      Successfully uninstalled importlib_metadata-8.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.6.0 requires importlib-metadata!=4.7.0,<7,>=3.7.0, but you have importlib-metadata 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.114.2 googleapis-common-protos-1.65.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-8.4.0 kubernetes-30.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 posthog-3.6.6 pypika-0.48.9 pyproject_hooks-1.1.0 rich-13.8.1 shellingham-1.5.4 starlette-0.38.5 typer-0.12.5 uvicorn-0.30.6 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface\n",
    "!pip install promptquality\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain-openai\n",
    "!pip install chromadb\n",
    "%pip install --upgrade --quiet  GitPython\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f134258-bbd5-42c6-bab4-fa4361ba650d",
   "metadata": {},
   "source": [
    "## Local Environment\n",
    "If you want to run models locally using Llama-cpp, just execute the following cell:\n",
    "\n",
    "Learn more at: https://python.langchain.com/docs/integrations/llms/llamacpp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e584445d-a833-4a29-afd8-f23ea0d2dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pip\n",
    "\n",
    "# os.environ[\"CUDACXX\"] = \"/usr/local/cuda-12/bin/nvcc\"\n",
    "# os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
    "# os.environ[\"FORCE_CMAKE\"] = \"1\"\n",
    "        \n",
    "# pip.main([\"install\", \"llama-cpp-python==0.2.55\", \"numpy==1.23\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221d332-9889-4d45-a9d3-fb9b66c7c1e4",
   "metadata": {},
   "source": [
    "## Step 1: Cloning the Repository and Extracting Code from Jupyter Notebooks\n",
    "In this step, we clone a GitHub repository, search for Jupyter Notebook files (*.ipynb*), and extract both code and context from these notebooks. The code performs the following operations:\n",
    "\n",
    "-  **Cloning a GitHub Repository**:\n",
    "We begin by cloning the desired repository from GitHub using the function clone_repo.\n",
    "- Function clone_repo(*repo_url*, *clone_dir=\"./temp_repo\"*):\n",
    "  - **Objective**: This function clones a GitHub repository into a specified directory.\n",
    "  - **Process**:\n",
    "      - The function first checks if the target directory exists. If not, it creates the directory.\n",
    "      - It then uses the git.Repo.clone_from method from the git Python module to clone the repository.\n",
    "      - A confirmation message is printed to show where the repository has been cloned.\n",
    "  - **Input**:\n",
    "      - **repo_url**: The URL of the GitHub repository to be cloned.\n",
    "      - **clone_dir**: The directory where the repository will be stored (default is ./temp_repo).\n",
    "\n",
    "- **Locating All Notebooks in the Directory**:\n",
    "Once the repository is cloned, we proceed to find all Jupyter Notebook files (.ipynb) within the cloned directory.\n",
    "    - **Function** find_all_notebooks(directory):\n",
    "    - **Objective**: This function recursively searches through the directory and identifies all files with the .ipynb extension.\n",
    "    - **Process**: It uses os.walk() to traverse through the specified directory, listing all files and subdirectories.\n",
    "For each file ending with .ipynb, the function adds the full file path to a list of notebooks.\n",
    "\n",
    "- **Extracting Code and Context From Notebooks**:\n",
    "  After locating the notebooks, the next step is to extract both the code and any markdown context from each notebook.\n",
    "  - **Function** *extract_code_and_context(notebook_path)*\n",
    "  - **Objective**: This function reads a notebook and extracts the code cells and any corresponding markdown context.\n",
    "\n",
    "- **Process**:\n",
    "  - The notebook is opened using the nbformat.read function.\n",
    "  - The function iterates through each cell of the notebook:\n",
    "  - If the cell is of type markdown, it extracts the content of the markdown cell as context.\n",
    "  - If the cell is of type code, it creates a dictionary with the following fields:\n",
    "    - **ID**: A unique identifier for the code snippet, generated using uuid.uuid4().\n",
    "    - **Embedding**: Initially set to None (embeddings will be generated later).\n",
    "    - **Code**: The code content of the cell.\n",
    "    - **Filename**: The name of the notebook file.\n",
    "    - **Context**: The markdown context associated with the code (if any).\n",
    "The extracted code and context are appended to a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9126f1-d6c1-4db5-aae0-02b32ffb2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import nbformat\n",
    "import uuid\n",
    "\n",
    "# Function to clone GitHub repository\n",
    "def clone_repo(repo_url, clone_dir=\"./temp_repo\"):\n",
    "    if not os.path.exists(clone_dir):\n",
    "        os.makedirs(clone_dir)\n",
    "    git.Repo.clone_from(repo_url, clone_dir)\n",
    "    print(f\"Repository cloned in: {clone_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a7b454-45ce-438d-884a-6abadbd23633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all .ipynb notebooks in a directory\n",
    "def find_all_notebooks(directory):\n",
    "    notebooks = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                notebooks.append(os.path.join(root, file))\n",
    "    return notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad35464-7bce-4490-ad56-4f79d5057f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract code and context from notebooks\n",
    "def extract_code_and_context(notebook_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    extracted_data = []\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            context = ''.join(cell['source'])\n",
    "        elif cell['cell_type'] == 'code':\n",
    "            cell_data = {\n",
    "                \"id\": str(uuid.uuid4()),  \n",
    "                \"embedding\": None,        \n",
    "                \"code\": ''.join(cell['source']),\n",
    "                \"filename\": os.path.basename(notebook_path),\n",
    "                \"context\": context if 'context' in locals() else ''\n",
    "            }\n",
    "            extracted_data.append(cell_data)\n",
    "\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b64e206-6188-412d-8108-f87acc46ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned in: ./temp_repo\n"
     ]
    }
   ],
   "source": [
    "# Cloning the repository and performing the extraction\n",
    "repo_url = \"https://github.com/sergiopaniego/RAG_local_tutorial.git\"  #your repository\n",
    "clone_repo(repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c0dac3-b4d1-45f3-bbec-d6010ca469ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from: ./temp_repo/example_rag.ipynb\n",
      "Extracting data from: ./temp_repo/github_repo_rag.ipynb\n",
      "Extracting data from: ./temp_repo/llm_chat_memory.ipynb\n",
      "Extracting data from: ./temp_repo/whisper_rag.ipynb\n",
      "Extracting data from: ./temp_repo/youtube_rag.ipynb\n",
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "# Locate and process notebooks\n",
    "clone_dir = \"./temp_repo\"\n",
    "notebooks = find_all_notebooks(clone_dir)\n",
    "\n",
    "all_extracted_data = []\n",
    "\n",
    "for notebook in notebooks:\n",
    "    print(f\"Extracting data from: {notebook}\")\n",
    "    extracted_data = extract_code_and_context(notebook)  \n",
    "    all_extracted_data.extend(extracted_data)\n",
    "\n",
    "print(\"Extraction completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581caf79-09e5-4e76-b17f-a2fa957a2982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '06a1bf36-1943-4fa4-bedc-6b0c9b6a7775',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Install the requirements\\n\\nIf an error is raised related to docarray, refer to this solution: https://stackoverflow.com/questions/76880224/error-using-using-docarrayinmemorysearch-in-langchain-could-not-import-docarray'},\n",
       " {'id': 'e27724b3-3607-427c-833b-9745b630d7d1',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Select the LLM model to use\\n\\nThe model must be downloaded locally to be used, so if you want to run llama3, you should run:\\n\\n```\\n\\nollama pull llama3\\n\\n```\\n\\nCheck the list of models available for Ollama here: https://ollama.com/library'},\n",
       " {'id': 'c335cbc6-421b-40e9-acf0-042ac2b87dbc',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)\\n\\nmodel.invoke(\"Give me an inspirational quote\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# We instanciate the LLM model and the Embedding model'},\n",
       " {'id': 'f7671a48-7252-49ca-9bc4-7dd3771a5521',\n",
       "  'embedding': None,\n",
       "  'code': 'model.invoke(\"Waht is 2+2?\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# We instanciate the LLM model and the Embedding model'},\n",
       " {'id': '822093d3-820f-4621-a7da-a0c8892feadd',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()\\nresponse_from_model = model.invoke(\"Give me an inspirational quote\")\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '## Using a parser provided by LangChain, we can transform the LLM output to something more suitable to be read'},\n",
       " {'id': 'abbe1e2b-0e2f-4a63-b684-52329a77403c',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)\\nprompt.format(context=\"Here is some context\", question=\"Here is a question\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': \"# We generate the template for the conversation with the instruct-based LLM\\n\\nWe can create a template to structure the conversation effectively.\\n\\nThis template allows us to provide some general context to the Language Learning Model (LLM), which will be utilized for every prompt. This ensures that the model has a consistent background understanding for all interactions.\\n\\nAdditionally, we can include specific context relevant to the particular prompt. This helps the model understand the immediate scenario or topic before addressing the actual question. Following this specific context, we then present the actual question we want the model to answer.\\n\\nBy using this approach, we enhance the model's ability to generate accurate and relevant responses based on both the general and specific contexts provided.\"},\n",
       " {'id': 'fd7c51ed-5a5b-4323-9dea-32d83f99e64f',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What\\'s your name?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The model can answer prompts based on the context:'},\n",
       " {'id': 'c1b184e0-5ca8-4fa8-8025-4bf002e23521',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What\\'s my age?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': \"But it can't answer what is not provided as context:\"},\n",
       " {'id': '5cf257aa-d559-40ed-a0ce-529743e1a050',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What is 2+2?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'Even previously known info!'},\n",
       " {'id': '91be7c28-7974-42a9-89b4-c2cfd8f3a48b',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import PyPDFLoader\\n\\n\\nloader = PyPDFLoader(\"./files/teaching.pdf\")\\npages = loader.load_and_split()\\n#pages = loader.load()\\npages',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Load an example PDF to do Retrieval Augmented Generation (RAG)\\n\\nFor the example, you can select your own PDF.'},\n",
       " {'id': '1b2bacc3-41b5-41de-a0ee-1b43909ac042',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(pages)[:5]\\n\\npages',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Load an example PDF to do Retrieval Augmented Generation (RAG)\\n\\nFor the example, you can select your own PDF.'},\n",
       " {'id': '905eb49e-560f-40f5-ba31-bf78332111d8',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Store the PDF in a vector space.\\n\\nFrom Langchain docs:\\n\\n`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\\n\\nThe execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example.'},\n",
       " {'id': '58d7ecba-3c39-402c-bef9-728b963cf5aa',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()\\nretriever.invoke(\"artificial intelligence\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Create retriever of vectors that are similar to be used as context'},\n",
       " {'id': '94f27a05-c12c-4ae9-8fc0-8e531f1866f9',\n",
       "  'embedding': None,\n",
       "  'code': '# Assuming retriever is an instance of a retriever class and has a method to retrieve context\\nretrieved_context = retriever.invoke(\"artificial intelligence\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Generate conversate with the document to extract the details'},\n",
       " {'id': '9e56cdb2-dcda-4038-8695-61359f783a8c',\n",
       "  'embedding': None,\n",
       "  'code': 'questions = [\\n    \"What are his research interests?\",\\n    \"Does he have teaching experience?\",\\n    \"Does he know about Tensorflow?\"\\n]\\n\\nfor question in questions:\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n\\n    print(f\"Question: {question}\")\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Generate conversate with the document to extract the details'},\n",
       " {'id': '86859e77-2594-4d3c-bf56-e3f54848537d',\n",
       "  'embedding': None,\n",
       "  'code': 'while True:\\n    print(\"Say \\'exit\\' or \\'quit\\' to exit the loop\")\\n    question = input(\\'User question: \\')\\n    print(f\"Question: {question}\")\\n    if question.lower() in [\"exit\", \"quit\"]:\\n        print(\"Exiting the conversation. Goodbye!\")\\n        break\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': '# Loop to ask-answer questions continously'},\n",
       " {'id': '49e92e04-2ebe-4cf5-8696-ee1bacd58f89',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install llama_index\\n!pip3 install llama-index-readers-github\\n!pip3 install llama-index-embeddings-langchain\\n!pip3 install llama-index-llms-ollama',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': '#### Install dependencies'},\n",
       " {'id': '0c6bcb8a-8c7b-4744-90f3-4969822c5a6f',\n",
       "  'embedding': None,\n",
       "  'code': '# This is due to the fact that we use asyncio.loop_until_complete in\\n# the DiscordReader. Since the Jupyter kernel itself runs on\\n# an event loop, we need to add some help with nesting\\nimport nest_asyncio\\n\\nnest_asyncio.apply()',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': '#### This cell prevents the kernel to collapse when downloading the GitHub repo content'},\n",
       " {'id': 'ff640b33-2736-4ec5-b141-6697731974df',\n",
       "  'embedding': None,\n",
       "  'code': 'GITHUB_ACCESS_TOKEN=\"GITHUB_ACCESS_TOKEN\"',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': \"#### Let's download the repo code!\\n\\nIn this case, we target this particular repo but you can change the code to target other repos.\"},\n",
       " {'id': 'c41cb7fc-4b67-41a3-8181-44f8136396d1',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.readers.github import GithubRepositoryReader, GithubClient\\n\\ndef initialize_github_client(github_token):\\n    return GithubClient(github_token)\\n\\ngithub_client = initialize_github_client(GITHUB_ACCESS_TOKEN)\\n\\nloader = GithubRepositoryReader(\\n            github_client,\\n            owner=\\'sergiopaniego\\', # CHANGE\\n            repo=\\'RAG_local_tutorial\\', # CHANGE\\n            filter_file_extensions=(\\n                [\".ipynb\"],\\n                GithubRepositoryReader.FilterType.INCLUDE,\\n            ),\\n            verbose=False,\\n            concurrent_requests=5,\\n        )\\n\\ndocs = loader.load_data(branch=\"main\")',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': \"#### Let's download the repo code!\\n\\nIn this case, we target this particular repo but you can change the code to target other repos.\"},\n",
       " {'id': '25ea1673-3d24-4f79-abcf-eeaf370c6c76',\n",
       "  'embedding': None,\n",
       "  'code': \"from llama_index.embeddings.langchain import LangchainEmbedding\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\\nembed_model = LangchainEmbedding(embeddings)\",\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': '#### Load the embedding model'},\n",
       " {'id': '5a7d6dda-e591-4b0d-9ec5-fd39dbe991d8',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.core import Settings\\nfrom llama_index.core import VectorStoreIndex\\n\\n# Create vector store and upload indexed data\\nSettings.embed_model = embed_model # Set the embedding model to be used\\nindex = VectorStoreIndex.from_documents(docs)',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': '#### Store the code into VectorStoreIndex using the loaded embedding model'},\n",
       " {'id': '61389f05-0fff-40a0-9936-867346088c52',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.llms.ollama import Ollama\\nfrom llama_index.core import Settings\\n\\n# Select the LLM model. You can change the model name below.\\nllm = Ollama(model=\"llama3\", request_timeout=500.0) \\n\\n# Generate a query engine from the previosuly created vector store index\\nSettings.llm = llm # Set the LLM model to be used\\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=4)',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': '### Load the LLM model to make the requests'},\n",
       " {'id': 'f1d039c5-3a6d-4989-8eb7-c419266b1a4b',\n",
       "  'embedding': None,\n",
       "  'code': \"from llama_index.core.prompts.base import PromptTemplate\\n\\nresponse = query_engine.query('What is this repository about?')\\nprint(response)\",\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': \"#### Let's chat with the code! :)\"},\n",
       " {'id': '9ec24435-f5be-462e-8c18-1e6ba32d71cc',\n",
       "  'embedding': None,\n",
       "  'code': 'MODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '#### Select the baseline model'},\n",
       " {'id': 'f64922ac-6e8b-4e43-8240-822fc057abba',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)\\n\\nmodel.invoke(\"Give me an inspirational quote\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '#### Select the baseline model'},\n",
       " {'id': 'a81dda2f-8b55-41fd-8fc6-59c20cede070',\n",
       "  'embedding': None,\n",
       "  'code': \"from langchain.callbacks import get_openai_callback\\ndef count_tokens(chain, query):\\n    with get_openai_callback() as cb:\\n        result = chain.run(query)\\n        print(f'Spent a total of {cb.total_tokens} tokens')\\n\\n    return result\",\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '#### Select the baseline model'},\n",
       " {'id': '69d38531-753d-41b4-a8f8-e2f15e835b58',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains import ConversationChain\\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\\n\\n\\nconversation_buf = ConversationChain(llm=model,verbose=True, memory=ConversationBufferMemory())\\n\\ncount_tokens(conversation_buf,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 1: ConversationBufferMemory\\n\\nRetains previous iterations'},\n",
       " {'id': '7bc3b2ac-4532-44a2-8920-c2200da71833',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_buf,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 1: ConversationBufferMemory\\n\\nRetains previous iterations'},\n",
       " {'id': '095082e1-805d-4f6b-b1a0-024fc11d8bea',\n",
       "  'embedding': None,\n",
       "  'code': 'print(conversation_buf.memory.buffer)',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 1: ConversationBufferMemory\\n\\nRetains previous iterations'},\n",
       " {'id': '5844e179-752e-4f14-9078-18b07ef875d4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationSummaryMemory\\nmemory = ConversationSummaryMemory(llm=model)\\nconversation_sum = ConversationChain(llm=model, memory=memory,verbose=True)\\n',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 2: ConversationSummaryMemory\\n\\nRetains previous iterations summary. It asks the model for a summary of the input and them stores that.'},\n",
       " {'id': 'e6a1d292-f783-4ad5-8715-e2b902827bd9',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 2: ConversationSummaryMemory\\n\\nRetains previous iterations summary. It asks the model for a summary of the input and them stores that.'},\n",
       " {'id': '1eeea1cd-a527-47dc-baca-2f55ab983024',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 2: ConversationSummaryMemory\\n\\nRetains previous iterations summary. It asks the model for a summary of the input and them stores that.'},\n",
       " {'id': '177b59e3-e5ca-4f5d-87aa-1513e96494bb',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 2: ConversationSummaryMemory\\n\\nRetains previous iterations summary. It asks the model for a summary of the input and them stores that.'},\n",
       " {'id': 'f787148f-dbf0-4de6-924b-10a1a014beb8',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationBufferWindowMemory\\nconversation_bufw = ConversationChain(llm=model, memory=ConversationBufferWindowMemory(k=1),verbose=True)',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 3: ConversationBufferWindowMemory\\n\\n\\nRemembers k previous interations'},\n",
       " {'id': '69232b92-77cb-4429-b6e5-612536b61e39',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 3: ConversationBufferWindowMemory\\n\\n\\nRemembers k previous interations'},\n",
       " {'id': 'df8a4042-15aa-451e-95f8-317d6842a2e9',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 3: ConversationBufferWindowMemory\\n\\n\\nRemembers k previous interations'},\n",
       " {'id': '76ef7920-e415-442d-a954-a48e70d30616',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 3: ConversationBufferWindowMemory\\n\\n\\nRemembers k previous interations'},\n",
       " {'id': '4972fc1d-a49b-41fb-9648-13fd2a13af10',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\\nconversation_sumbuf = ConversationChain(llm=model, memory=ConversationSummaryBufferMemory(llm=model),verbose=True)\\n',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 4: ConversationSummaryBufferMemory\\n\\nCombination of ConversationSummaryMemory and ConversationBufferWindowMemory\\n'},\n",
       " {'id': '99f87630-9634-4d70-94b2-e4c60f43b8cf',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 4: ConversationSummaryBufferMemory\\n\\nCombination of ConversationSummaryMemory and ConversationBufferWindowMemory\\n'},\n",
       " {'id': '647b12b2-6c61-4a69-ade3-6828c14bab85',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 4: ConversationSummaryBufferMemory\\n\\nCombination of ConversationSummaryMemory and ConversationBufferWindowMemory\\n'},\n",
       " {'id': 'e90f49de-c399-43a5-8328-fa91d77d259b',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': '## Type 4: ConversationSummaryBufferMemory\\n\\nCombination of ConversationSummaryMemory and ConversationBufferWindowMemory\\n'},\n",
       " {'id': '4b424873-bc0a-4041-b5f3-ea89809626a3',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '## First, we install Whisper and its dependencies'},\n",
       " {'id': '3dd6c81f-1930-4efe-b4e9-b6313cf617ab',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install openai-whisper',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '## First, we install Whisper and its dependencies'},\n",
       " {'id': '199b7275-d3c3-43a8-9053-71ed1fd7b684',\n",
       "  'embedding': None,\n",
       "  'code': '# on Ubuntu or Debian\\n#sudo apt update && sudo apt install ffmpeg\\n\\n# on Arch Linux\\n#sudo pacman -S ffmpeg\\n\\n# on MacOS using Homebrew (https://brew.sh/)\\n!brew install ffmpeg\\n\\n# on Windows using Chocolatey (https://chocolatey.org/)\\n#choco install ffmpeg\\n\\n# on Windows using Scoop (https://scoop.sh/)\\n#scoop install ffmpeg',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': \"### We install ffmpeg version depending on the platform where we'll be running the example\"},\n",
       " {'id': '5205138b-fe71-44f3-b9e8-13cf4654123e',\n",
       "  'embedding': None,\n",
       "  'code': 'import whisper\\n\\nwhisper_model = whisper.load_model(\"base\")\\n\\ndef transcribe_audio(audio_path):\\n    result = whisper_model.transcribe(audio_path)\\n    return result[\\'text\\']\\n\\n# https://commons.wikimedia.org/wiki/File:Audio_Kevin_Folta.wav\\n# Audio example\\naudio_path = \"./files/Audio_Kevin_Folta.wav\" # CHANGE THIS FILE\\n\\n# Transcribir el audio\\ntranscribed_text = transcribe_audio(audio_path)\\n\\nprint(transcribed_text)\\n\\nwith open(\"./files/whisper_transcription.txt\", \"a\") as file:\\n    file.write(transcribed_text)\\n',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '## Given an example audio, we transcribe it to text so we can use it.\\n\\nThe example audio should be downloaded locally. Change the file name with your own example.'},\n",
       " {'id': '094a13ae-e6d1-4275-b2b8-86759b0bf66a',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We instantiate the model and the embeddings'},\n",
       " {'id': '1b957e7b-ad53-45db-931f-319062f07aae',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We instantiate the model and the embeddings'},\n",
       " {'id': '71f0ae1c-a968-40d6-b39a-df3d21b6b4e7',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./files/whisper_transcription.txt\")\\ntext_documents = loader.load()\\ntext_documents',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We load the transcription previously saved using TextLoader'},\n",
       " {'id': 'bda06ad1-f5dd-49fb-9cdd-93495e1e42e0',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(text_documents)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We explit the document into chunks'},\n",
       " {'id': '818c79f2-fbaf-4fc7-b657-fc420a749913',\n",
       "  'embedding': None,\n",
       "  'code': 'text_documents',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We explit the document into chunks'},\n",
       " {'id': '86adc0e3-517e-495f-ae5f-3476e47771c4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# Store the PDF in a vector space.\\n\\nFrom Langchain docs:\\n\\n`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\\n\\nThe execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example.'},\n",
       " {'id': 'be639937-1755-4750-b25b-da6f3db3eec9',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# Store the PDF in a vector space.\\n\\nFrom Langchain docs:\\n\\n`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\\n\\nThe execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example.'},\n",
       " {'id': '1641a03a-3c97-4f32-b9d5-0631e8dbad40',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# Generate the conversation template'},\n",
       " {'id': 'c9c4a521-4f05-4331-9b3a-c7aff952fa3e',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We instantiate the parser'},\n",
       " {'id': '280a934c-9f78-4ef2-970a-706207627bcd',\n",
       "  'embedding': None,\n",
       "  'code': 'while True:\\n    print(\"Say \\'exit\\' or \\'quit\\' to exit the loop\")\\n    question = input(\\'User question: \\')\\n    print(f\"Question: {question}\")\\n    if question.lower() in [\"exit\", \"quit\"]:\\n        print(\"Exiting the conversation. Goodbye!\")\\n        break\\n    retrieved_context = retriever.invoke(question)\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We can now extract the information from the audio!'},\n",
       " {'id': '98dca43e-3f57-4ddd-ae06-32f7ea4e19ab',\n",
       "  'embedding': None,\n",
       "  'code': '',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': '# We can now extract the information from the audio!'},\n",
       " {'id': '8f9b02a3-df36-437e-8538-ae760fae7d8e',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '### Install the dependencies'},\n",
       " {'id': '1b8f0093-cc71-4df3-ad12-5438a4cf7ea5',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install youtube_transcript_api',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '### Install the dependencies'},\n",
       " {'id': '8d489f2e-6fa5-4952-9b87-3636ae2af1c4',\n",
       "  'embedding': None,\n",
       "  'code': 'from youtube_transcript_api import YouTubeTranscriptApi\\n\\nsrt = YouTubeTranscriptApi.get_transcript(\"pxiP-HJLCx0\") # CHANGE THE ID OF THE VIDEO\\n\\nwith open(\"./files/youtube_transcription.txt\", \"a\") as file:\\n    for i in srt:\\n        file.write(i[\\'text\\'])',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': \"# Let's download an example transcript from a YT video\\n\\nYou can change the id of the video to download other video transcriptions.\\nWe save the contect to a file\"},\n",
       " {'id': 'a2dbbf60-5958-4378-993d-9d942a8bef80',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We instantiate the model and the embeddings'},\n",
       " {'id': '1662a2d5-7960-427b-a597-d63f8fe6b7ee',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We instantiate the model and the embeddings'},\n",
       " {'id': '54ef2e3e-8e4f-40c6-b3c3-860eca9429e7',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./files/youtube_transcription.txt\")\\ntext_documents = loader.load()\\ntext_documents',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We load the transcription previously saved using TextLoader'},\n",
       " {'id': '30eda0ee-7191-4760-b35a-56aa977a838e',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(text_documents)[:5]',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We explit the document into chunks'},\n",
       " {'id': '16c6baca-e145-4a93-aee0-5a07279dbd9f',\n",
       "  'embedding': None,\n",
       "  'code': 'text_documents',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We explit the document into chunks'},\n",
       " {'id': 'c843c07c-ffe7-4932-b4f7-d3cfeb3b23d1',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# Store the PDF in a vector space.\\n\\nFrom Langchain docs:\\n\\n`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\\n\\nThe execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example.'},\n",
       " {'id': 'd5d99d34-5b43-4120-8639-17e57be92cff',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# Store the PDF in a vector space.\\n\\nFrom Langchain docs:\\n\\n`DocArrayInMemorySearch is a document index provided by Docarray that stores documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server.`\\n\\nThe execution time of the following block depends on the complexity and longitude of the PDF provided. Try to keep it small and simple for the example.'},\n",
       " {'id': 'b7573242-22c6-4e31-a315-71903c475b7d',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We instantiate the parser'},\n",
       " {'id': 'e7d43995-2417-4f73-b013-00568a9bdd6a',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)\\nprompt.format(context=\"Here is some context\", question=\"Here is a question\")',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# Generate the conversation template'},\n",
       " {'id': '60b96685-6a3b-49cc-a156-8e652f7f090b',\n",
       "  'embedding': None,\n",
       "  'code': 'retrieved_context = retriever.invoke(\"laptop\")\\nquestions = [\\n    \"Which is the best laptop for students?\",\\n    \"How much is a laptop worth?\",\\n    \"Make a summary of the video\"\\n]\\n\\nfor question in questions:\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n\\n    print(f\"Question: {question}\")\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': '# We can now extract the information from the video!'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7714591-6f9f-448f-a759-9bbff0ddd1b7",
   "metadata": {},
   "source": [
    "## Step 2: Generate metadata with llm  🔢\n",
    "\n",
    "In this step, we use a language model (LLM) to generate descriptions and explanatory metadata for each extracted code snippet. The code performs the following operations:\n",
    "\n",
    "-  We define a prompt template that contains placeholders for the code snippet, the file name, and an optional context. The goal is for the model to provide a clear and concise explanation of what the code does, based on these three pieces of information.\n",
    "\n",
    "-  A PromptTemplate object is created from this template, allowing it to be used in conjunction with the language model.\n",
    "\n",
    "-  We use the OpenAI LLM, authenticated with an API key, to process the information and generate responses.\n",
    "\n",
    "- The function update_context_with_llm iterates through the data structure containing the extracted code, runs the language model for each item, and replaces the original context field with the explanation generated by the AI.\n",
    "\n",
    "- Finally, the data structure is updated with the new explanations, which are stored in the context field.\n",
    "\n",
    "-  The ultimate goal is to enrich the original data structure by providing clear explanations for each code snippet, making it easier to understand and use the information later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b30b2a9-59cd-4e71-b21e-2e61826ac357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a313f79-571e-4fec-b38c-ec94bb46551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\" #your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b18b4d-ff7f-44e0-a975-2477d3cabe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will receive three pieces of information: a code snippet, a file name, and an optional context. Based on this information, explain in a clear, summarized and concise way what the code snippet is doing.\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "File name:\n",
    "{filename}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Describe what the code above does.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad693e1-eb5b-482e-95a4-5d83a6f0aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213104ca-f28c-4d8b-b0db-579a6d0fc68e",
   "metadata": {},
   "source": [
    "### Generate metadata with llm local\n",
    "\n",
    "If you happen to be using a local model with LlamaCPP to generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d28bb47-0beb-40e6-85ac-c7797ba49412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# llm_local = LlamaCpp(\n",
    "            # model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            # n_gpu_layers=64,\n",
    "            # n_batch=512,\n",
    "            # n_ctx=4096,\n",
    "            # max_tokens=1024,\n",
    "            # f16_kv=True,  \n",
    "            # callback_manager=callback_manager,\n",
    "            # verbose=False,\n",
    "            # stop=[],\n",
    "            # streaming=False,\n",
    "            # temperature=0.4,\n",
    "        # )\n",
    "\n",
    "# llm_chain = prompt | llm_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85ba720e-cea4-4535-805a-b20e7f717bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_context_with_llm(data_structure):\n",
    "    updated_structure = []\n",
    "    for item in data_structure:\n",
    "        code = item['code']\n",
    "        filename = item['filename']\n",
    "        context = item['context']\n",
    "    \n",
    "        # Run LLM to generate code explanation\n",
    "        response = llm_chain.invoke({\n",
    "            \"code\": code, \n",
    "            \"filename\": filename, \n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        # Update the item with the LLM response\n",
    "        item['context'] = response.strip()\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e8379a-6dc8-4f99-9f42-648ec2cbd629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_data = update_context_with_llm(all_extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "311822ae-8d0b-4077-87ab-d2fed0b2f4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '06a1bf36-1943-4fa4-bedc-6b0c9b6a7775',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet installs several packages using pip3 in order to run a file named \"example_rag.ipynb\". One of the packages, \"docarray\", may cause an error, so the code also includes a solution to address this potential issue.'},\n",
       " {'id': 'e27724b3-3607-427c-833b-9745b630d7d1',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is setting a variable, MODEL, to a specific value. The file name is example_rag.ipynb. The context provides instructions on how to select and use the LLM model, and also includes a link to a website where a list of available models can be found. The code is specifically selecting the llama3 model and the context provides a command to download the model if it is not already available locally.'},\n",
       " {'id': 'c335cbc6-421b-40e9-acf0-042ac2b87dbc',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)\\n\\nmodel.invoke(\"Give me an inspirational quote\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is importing two classes, Ollama and OllamaEmbeddings, from the langchain_community package. It then creates objects of these classes, passing in a model parameter called MODEL. Finally, it uses the model to invoke a method that returns an inspirational quote. The file name is \"example_rag.ipynb\" and it appears that the code is being used as an example in a Jupyter notebook, possibly for demonstrating how to use the Ollama and OllamaEmbeddings classes. The context mentions instantiating the LLM and Embedding models, which suggests that the code is being used to create and use these models.'},\n",
       " {'id': 'f7671a48-7252-49ca-9bc4-7dd3771a5521',\n",
       "  'embedding': None,\n",
       "  'code': 'model.invoke(\"Waht is 2+2?\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is invoking a model to answer the question \"What is 2+2?\". The file name is \"example_rag.ipynb\". This code is part of a larger context where the LLM model and Embedding model are being instantiated.'},\n",
       " {'id': '822093d3-820f-4621-a7da-a0c8892feadd',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()\\nresponse_from_model = model.invoke(\"Give me an inspirational quote\")\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a StrOutputParser from the langchain_core.output_parsers module. It then creates an instance of the StrOutputParser and uses it to parse a response received from a model. The parsed response is then printed. The file name is \"example_rag.ipynb\" and the context suggests that this code is being used to transform the output of a model provided by LangChain into a more readable format.'},\n",
       " {'id': 'abbe1e2b-0e2f-4a63-b684-52329a77403c',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)\\nprompt.format(context=\"Here is some context\", question=\"Here is a question\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': \"The code above imports a PromptTemplate from the langchain module and assigns it to a variable. It then creates a template string that includes a context and question, and assigns it to another variable. It then utilizes the PromptTemplate to format the context and question within the template string, creating a structured prompt for the Language Learning Model (LLM). This process can be repeated for multiple prompts, ensuring consistency in the model's background understanding and providing specific context for each prompt.\"},\n",
       " {'id': 'fd7c51ed-5a5b-4323-9dea-32d83f99e64f',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What\\'s your name?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code above is formatting a prompt with the context \"My parents named me Sergio\" and a question \"What\\'s your name?\". It then invokes a model to generate a response based on the formatted prompt. The response is then parsed using a parser and the resulting parsed response is printed. The file name is \"example_rag.ipynb\" and it suggests that the code is an example of how to use a retrieval-augmented generation (RAG) model to answer prompts.'},\n",
       " {'id': 'c1b184e0-5ca8-4fa8-8025-4bf002e23521',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What\\'s my age?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet uses a prompt and context to ask a question and receive a response from a model. It then uses a parser to format the response and prints the parsed response. The file name is \"example_rag.ipynb\" and the optional context is \"My parents named me Sergio\".'},\n",
       " {'id': '5cf257aa-d559-40ed-a0ce-529743e1a050',\n",
       "  'embedding': None,\n",
       "  'code': 'formatted_prompt = prompt.format(context=\"My parents named me Sergio\", question=\"What is 2+2?\")\\nresponse_from_model = model.invoke(formatted_prompt)\\nparsed_response = parser.parse(response_from_model)\\nprint(parsed_response)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is formatting a prompt with a context and question, then using a model to invoke it and parsing the response. Finally, the parsed response is printed. The file name is \"example_rag.ipynb\" and the optional context is \"Even previously known info!\".'},\n",
       " {'id': '91be7c28-7974-42a9-89b4-c2cfd8f3a48b',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import PyPDFLoader\\n\\n\\nloader = PyPDFLoader(\"./files/teaching.pdf\")\\npages = loader.load_and_split()\\n#pages = loader.load()\\npages',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a module called \"PyPDFLoader\" from a package called \"langchain_community.document_loaders\". It then uses the \"PyPDFLoader\" to load a PDF file called \"teaching.pdf\" from a specified location. The \"load_and_split()\" function is then used to retrieve and split the pages of the PDF file. Finally, the code assigns the split pages to a variable called \"pages\". The file name is \"example_rag.ipynb\" and the context suggests that this code is being used to load an example PDF for the purpose of performing Retrieval Augmented Generation (RAG). The user is instructed to select their own PDF file for the example.'},\n",
       " {'id': '1b2bacc3-41b5-41de-a0ee-1b43909ac042',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(pages)[:5]\\n\\npages',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a module called \"RecursiveCharacterTextSplitter\" from a package called \"langchain.text_splitter\". It then creates an instance of this class and sets the chunk size and overlap values. Finally, it uses this text splitter to split a given document into smaller chunks, with a maximum of 5 chunks being returned. The file name is \"example_rag.ipynb\" and the context indicates that this code is being used to perform Retrieval Augmented Generation (RAG) on a PDF document. The user is prompted to select their own PDF for the example.'},\n",
       " {'id': '905eb49e-560f-40f5-ba31-bf78332111d8',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet imports a module called \"DocArrayInMemorySearch\" from the \"langchain_community.vectorstores\" package. It then creates an instance of this document index using the \"from_documents\" method, which takes in two arguments: \"text_documents\" and \"embeddings\". The file name is \"example_rag.ipynb\" and the context indicates that the code is used to store a PDF in a vector space. The code uses the DocArrayInMemorySearch index to store the PDF document in memory, which is a good option for smaller datasets. The execution time of the code depends on the complexity and length of the PDF provided.'},\n",
       " {'id': '58d7ecba-3c39-402c-bef9-728b963cf5aa',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()\\nretriever.invoke(\"artificial intelligence\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code creates a retriever object that can be used to retrieve vector representations of data. The file name is \"example_rag.ipynb\" and the code is being used to create a retriever for vectors that are similar to be used as context.'},\n",
       " {'id': '94f27a05-c12c-4ae9-8fc0-8e531f1866f9',\n",
       "  'embedding': None,\n",
       "  'code': '# Assuming retriever is an instance of a retriever class and has a method to retrieve context\\nretrieved_context = retriever.invoke(\"artificial intelligence\")',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code snippet is using an instance of the \"retriever\" class to invoke a method that will retrieve context related to artificial intelligence. This context is then stored in a variable called \"retrieved_context\". The code is likely part of a larger program or project, as it references a file named \"example_rag.ipynb\" and mentions generating conversation or extracting details from a document.'},\n",
       " {'id': '9e56cdb2-dcda-4038-8695-61359f783a8c',\n",
       "  'embedding': None,\n",
       "  'code': 'questions = [\\n    \"What are his research interests?\",\\n    \"Does he have teaching experience?\",\\n    \"Does he know about Tensorflow?\"\\n]\\n\\nfor question in questions:\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n\\n    print(f\"Question: {question}\")\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code above creates a list of questions and uses a for loop to iterate through each question. It then formats the prompt using a retrieved context and the current question, which is then used to invoke a model and obtain a response. The response is then parsed and printed alongside the original question. This process is repeated for each question in the list. The code is likely part of a larger program or notebook titled \"example_rag\" that generates conversation with a document to extract details.'},\n",
       " {'id': '86859e77-2594-4d3c-bf56-e3f54848537d',\n",
       "  'embedding': None,\n",
       "  'code': 'while True:\\n    print(\"Say \\'exit\\' or \\'quit\\' to exit the loop\")\\n    question = input(\\'User question: \\')\\n    print(f\"Question: {question}\")\\n    if question.lower() in [\"exit\", \"quit\"]:\\n        print(\"Exiting the conversation. Goodbye!\")\\n        break\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'example_rag.ipynb',\n",
       "  'context': 'The code above creates a loop that continuously prompts the user to input a question. The program will print the question and then check if the user input matches either \"exit\" or \"quit\". If it does, the loop will exit and the program will print a goodbye message. If not, the program will format the user input and use it as a prompt for a model, which will then generate a response. The response will be parsed and printed as an answer. This process will continue until the user inputs \"exit\" or \"quit\". The file name is \"example_rag.ipynb\" and it likely contains code related to the Retriever-Reader-Generator (RAG) model. The optional context is a comment explaining the purpose of the loop.'},\n",
       " {'id': '49e92e04-2ebe-4cf5-8696-ee1bacd58f89',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install llama_index\\n!pip3 install llama-index-readers-github\\n!pip3 install llama-index-embeddings-langchain\\n!pip3 install llama-index-llms-ollama',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is installing four different packages using the pip3 command. These packages are called llama_index, llama-index-readers-github, llama-index-embeddings-langchain, and llama-index-llms-ollama. The file name associated with this code is github_repo_rag.ipynb. This code is likely part of a project or notebook that is related to GitHub repositories. The context provided suggests that this code is part of a larger process of installing dependencies.'},\n",
       " {'id': '0c6bcb8a-8c7b-4744-90f3-4969822c5a6f',\n",
       "  'embedding': None,\n",
       "  'code': '# This is due to the fact that we use asyncio.loop_until_complete in\\n# the DiscordReader. Since the Jupyter kernel itself runs on\\n# an event loop, we need to add some help with nesting\\nimport nest_asyncio\\n\\nnest_asyncio.apply()',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is using the nest_asyncio library to help with nesting event loops in the DiscordReader. This is necessary because the Jupyter kernel itself runs on an event loop. The code is being applied to the github_repo_rag.ipynb file and is preventing the kernel from collapsing when downloading content from a GitHub repository.'},\n",
       " {'id': 'ff640b33-2736-4ec5-b141-6697731974df',\n",
       "  'embedding': None,\n",
       "  'code': 'GITHUB_ACCESS_TOKEN=\"GITHUB_ACCESS_TOKEN\"',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is setting a variable called GITHUB_ACCESS_TOKEN and assigning it the value \"GITHUB_ACCESS_TOKEN\". This variable is most likely used for authentication purposes. The file name is github_repo_rag.ipynb, which suggests that it is a Jupyter notebook file. The context indicates that the code is used to download a specific repository\\'s code, and it can be modified to target other repositories.'},\n",
       " {'id': 'c41cb7fc-4b67-41a3-8181-44f8136396d1',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.readers.github import GithubRepositoryReader, GithubClient\\n\\ndef initialize_github_client(github_token):\\n    return GithubClient(github_token)\\n\\ngithub_client = initialize_github_client(GITHUB_ACCESS_TOKEN)\\n\\nloader = GithubRepositoryReader(\\n            github_client,\\n            owner=\\'sergiopaniego\\', # CHANGE\\n            repo=\\'RAG_local_tutorial\\', # CHANGE\\n            filter_file_extensions=(\\n                [\".ipynb\"],\\n                GithubRepositoryReader.FilterType.INCLUDE,\\n            ),\\n            verbose=False,\\n            concurrent_requests=5,\\n        )\\n\\ndocs = loader.load_data(branch=\"main\")',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet imports the GithubRepositoryReader and GithubClient classes from the llama_index.readers.github module. It then defines a function named initialize_github_client that takes in a github_token and returns an instance of the GithubClient class. The code then initializes a GithubClient instance named github_client using the initialize_github_client function and passing in the GITHUB_ACCESS_TOKEN. \\n\\nNext, the code creates an instance of the GithubRepositoryReader class named loader. This instance uses the github_client to read data from a specific Github repository belonging to the owner \"sergiopaniego\" and with the name \"RAG_local_tutorial\". It also specifies to only include files with the .ipynb extension and sets the number of concurrent requests to 5. \\n\\nFinally, the code uses the loader instance to load data from the \"main\" branch of the specified repository and stores it in a variable named docs. The file name associated with this code snippet is \"github_repo_rag.ipynb\" and it appears to be used to download code from the specified Github repository.'},\n",
       " {'id': '25ea1673-3d24-4f79-abcf-eeaf370c6c76',\n",
       "  'embedding': None,\n",
       "  'code': \"from llama_index.embeddings.langchain import LangchainEmbedding\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\\nembed_model = LangchainEmbedding(embeddings)\",\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is importing two different embedding libraries, LangchainEmbedding and HuggingFaceEmbeddings, and creating an instance of the HuggingFaceEmbeddings model using the \"BAAI/bge-base-en-v1.5\" model. The file name, \"github_repo_rag.ipynb\", suggests that this code may be used in a GitHub repository for a project related to the RAG (Retrieval-Augmented Generation) model.'},\n",
       " {'id': '5a7d6dda-e591-4b0d-9ec5-fd39dbe991d8',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.core import Settings\\nfrom llama_index.core import VectorStoreIndex\\n\\n# Create vector store and upload indexed data\\nSettings.embed_model = embed_model # Set the embedding model to be used\\nindex = VectorStoreIndex.from_documents(docs)',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code is importing the Settings and VectorStoreIndex modules from the llama_index.core package. It then sets the embedding model to be used and creates a VectorStoreIndex object, which is used to store the indexed data. The file name is github_repo_rag.ipynb and the context is specifying that the code is being used to store the code into VectorStoreIndex using a previously loaded embedding model.'},\n",
       " {'id': '61389f05-0fff-40a0-9936-867346088c52',\n",
       "  'embedding': None,\n",
       "  'code': 'from llama_index.llms.ollama import Ollama\\nfrom llama_index.core import Settings\\n\\n# Select the LLM model. You can change the model name below.\\nllm = Ollama(model=\"llama3\", request_timeout=500.0) \\n\\n# Generate a query engine from the previosuly created vector store index\\nSettings.llm = llm # Set the LLM model to be used\\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=4)',\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is importing the Ollama class from the llama_index.llms.ollama module and the Settings class from the llama_index.core module. It then creates an instance of the Ollama class, specifying the model name and request timeout. Next, the code sets the LLM model to be used and generates a query engine from the previously created vector store index. The file name is github_repo_rag.ipynb and the context is loading the LLM model to make requests.'},\n",
       " {'id': 'f1d039c5-3a6d-4989-8eb7-c419266b1a4b',\n",
       "  'embedding': None,\n",
       "  'code': \"from llama_index.core.prompts.base import PromptTemplate\\n\\nresponse = query_engine.query('What is this repository about?')\\nprint(response)\",\n",
       "  'filename': 'github_repo_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a PromptTemplate class from the base module in the llama_index.core.prompts package. It then uses the query_engine to retrieve a response to the question \"What is this repository about?\" and prints the response. The file name is likely associated with a Jupyter notebook file used for interactive coding and documentation. The optional context suggests that the code may be used for conversational or interactive purposes.'},\n",
       " {'id': '9ec24435-f5be-462e-8c18-1e6ba32d71cc',\n",
       "  'embedding': None,\n",
       "  'code': 'MODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code above defines a variable named \"MODEL\" with the value \"llama3\", which is a reference to a model available on the website https://ollama.com/library/llama3. The file name associated with this code snippet is \"llm_chat_memory.ipynb\". This code is likely being used to select a baseline model for a chatbot or natural language processing project.'},\n",
       " {'id': 'f64922ac-6e8b-4e43-8240-822fc057abba',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)\\n\\nmodel.invoke(\"Give me an inspirational quote\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet imports the Ollama class from the langchain_community.llms module and the OllamaEmbeddings class from the langchain_community.embeddings module. It then creates an instance of the Ollama class, passing in the MODEL parameter, and another instance of the OllamaEmbeddings class, also passing in the MODEL parameter. Finally, it calls the invoke() method on the model instance, passing in the string \"Give me an inspirational quote\" as a parameter. The file name is likely associated with a Jupyter notebook used for chatting with the Ollama model and storing chat history.'},\n",
       " {'id': 'a81dda2f-8b55-41fd-8fc6-59c20cede070',\n",
       "  'embedding': None,\n",
       "  'code': \"from langchain.callbacks import get_openai_callback\\ndef count_tokens(chain, query):\\n    with get_openai_callback() as cb:\\n        result = chain.run(query)\\n        print(f'Spent a total of {cb.total_tokens} tokens')\\n\\n    return result\",\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is importing a function called \"get_openai_callback\" from a module named \"langchain.callbacks\". It then defines a function called \"count_tokens\" that takes in two parameters, \"chain\" and \"query\". Within this function, it uses the \"get_openai_callback\" function to create a callback object and then uses this object to run a query on a language model. After the query is run, the total number of tokens used by the callback is printed. The function then returns the result of the query. The file name accompanying this code is \"llm_chat_memory.ipynb\" and the context suggests that this code is being used to select a baseline model.'},\n",
       " {'id': '69d38531-753d-41b4-a8f8-e2f15e835b58',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains import ConversationChain\\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\\n\\n\\nconversation_buf = ConversationChain(llm=model,verbose=True, memory=ConversationBufferMemory())\\n\\ncount_tokens(conversation_buf,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet imports the ConversationChain and ConversationBufferMemory modules from the langchain.chains and langchain.chains.conversation.memory packages, respectively. It then creates an instance of the ConversationChain class with a specified model and enables verbose mode. The memory argument is set to ConversationBufferMemory, which is a type of memory that stores previous conversation iterations. Finally, the count_tokens function is called with the conversation_buf instance and a string as arguments. This code is likely part of a larger notebook file named llm_chat_memory.ipynb, which deals with language chain and conversation memory.'},\n",
       " {'id': '7bc3b2ac-4532-44a2-8920-c2200da71833',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_buf,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is a function called \"count_tokens\" that takes in two arguments: a conversation buffer (a collection of messages) and a specific phrase \"It is a dell xps 17\". The function counts the number of times the phrase appears in the conversation buffer. The file name is \"llm_chat_memory.ipynb\" and the context provided is related to a type of memory called \"ConversationBufferMemory\" that stores previous iterations. This suggests that the code is likely part of a larger program or system that is designed to store and analyze conversations, specifically counting the occurrence of a given phrase.'},\n",
       " {'id': '095082e1-805d-4f6b-b1a0-024fc11d8bea',\n",
       "  'embedding': None,\n",
       "  'code': 'print(conversation_buf.memory.buffer)',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet prints the content of a memory buffer called \"conversation_buf\". The file name is \"llm_chat_memory.ipynb\" and the context indicates that this buffer is used to store previous conversations.'},\n",
       " {'id': '5844e179-752e-4f14-9078-18b07ef875d4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationSummaryMemory\\nmemory = ConversationSummaryMemory(llm=model)\\nconversation_sum = ConversationChain(llm=model, memory=memory,verbose=True)\\n',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code above imports a ConversationSummaryMemory class from the langchain.chains.conversation.memory module and assigns it to the variable \"memory\". It then creates a ConversationChain object, passing in the imported class, a model, and a verbose parameter. The file name is likely for a Jupyter Notebook where the code is being executed. The context provides additional information about the purpose of the ConversationSummaryMemory class, stating that it stores summaries of previous conversations by asking the model for a summary of the input and then storing it. This code is likely used for natural language processing tasks.'},\n",
       " {'id': 'e6a1d292-f783-4ad5-8715-e2b902827bd9',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet uses the function \"count_tokens\" to count the number of tokens in a conversation summary, which is provided as the first argument. The second argument is a string of text, \"Hi, i am not able to start my dell laptop\", which is the input for the conversation summary. The third argument is an optional context, which provides information about the type of conversation summary being used. This code is likely part of a larger program, stored in the file \"llm_chat_memory.ipynb\", which is used to retain previous iterations of conversation summaries. This allows the program to ask the model for a summary of the input and store it for future use.'},\n",
       " {'id': '1eeea1cd-a527-47dc-baca-2f55ab983024',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is a function called \"count_tokens\" that takes in a conversation summary and a string \"It is a dell xps 17\" as arguments. The purpose of this function is to count the number of tokens (words, numbers, punctuation) in the given conversation summary. The file name associated with this code is \"llm_chat_memory.ipynb\" and it is used for the Type 2 ConversationSummaryMemory, which is a method of retaining previous iterations of conversation summaries. This allows the model to continuously improve its summary generation by storing and using previous summaries. The optional context refers to the type of conversation summary memory and the purpose of the code snippet within that context.'},\n",
       " {'id': '177b59e3-e5ca-4f5d-87aa-1513e96494bb',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sum,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is a function called \"count_tokens\" that takes in two parameters: \"conversation_sum\" and \"How do i start it up\". It is used in a file named \"llm_chat_memory.ipynb\", which is related to a type of conversation summary memory. The function counts the number of tokens in the conversation summary and stores it in the memory. The context suggests that this is part of a larger model that uses previous iterations of the summary to generate a new one based on the input.'},\n",
       " {'id': 'f787148f-dbf0-4de6-924b-10a1a014beb8',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationBufferWindowMemory\\nconversation_bufw = ConversationChain(llm=model, memory=ConversationBufferWindowMemory(k=1),verbose=True)',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code creates a conversation buffer window memory that remembers the previous interactions in a conversation. It imports the necessary module and defines the memory with a specified value of k, which determines the number of interactions to be remembered. The code also sets up a conversation chain, using a language model and the conversation buffer window memory. The file name suggests that this code may be used for a chatbot, as indicated by \"chat\" and \"memory\" in the file name.'},\n",
       " {'id': '69232b92-77cb-4429-b6e5-612536b61e39',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is a function called \"count_tokens\" that takes in two arguments: a conversation buffer and a string of text. It counts the number of tokens (words) in the given text and stores it in a variable. This function is likely used in the file \"llm_chat_memory.ipynb\" as part of a type 3 conversation buffer window memory, which remembers the k previous interactions in a conversation. This function may be used to keep track of the number of words in each interaction in order to analyze the conversation.'},\n",
       " {'id': 'df8a4042-15aa-451e-95f8-317d6842a2e9',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code is calling the function \"count_tokens\" with two arguments: a conversation buffer called \"conversation_bufw\" and a string \"It is a dell xps 17\". It is also specifying the context as a conversation buffer window memory that remembers a specified number of previous interactions. This function likely counts the number of tokens (individual words or symbols) in the given string within the specified memory context.'},\n",
       " {'id': '76ef7920-e415-442d-a954-a48e70d30616',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_bufw,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code is creating a function called \"count_tokens\" that takes in two inputs: a variable named \"conversation_bufw\" and a string. It then counts the number of tokens (individual words or phrases) in the string and returns the count. This function is likely used in the file \"llm_chat_memory.ipynb\" which is related to a conversation buffer window memory feature. This feature remembers the previous k interactions in a conversation.'},\n",
       " {'id': '4972fc1d-a49b-41fb-9648-13fd2a13af10',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\\nconversation_sumbuf = ConversationChain(llm=model, memory=ConversationSummaryBufferMemory(llm=model),verbose=True)\\n',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code imports a class called ConversationSummaryBufferMemory from the langchain.chains.conversation.memory module. Then, it creates an instance of the ConversationChain class, passing in a model and an instance of the ConversationSummaryBufferMemory class. The verbose parameter is set to True. This code is likely part of a larger program that uses language models to generate conversational responses. The file name suggests that it is a Jupyter notebook and the context indicates that it is a type of memory used for storing conversation summaries and buffers.'},\n",
       " {'id': '99f87630-9634-4d70-94b2-e4c60f43b8cf',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"Hi, i am not able to start my dell laptop\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet counts the number of tokens (individual words or symbols) in a given conversation summary buffer, which contains the phrase \"Hi, i am not able to start my dell laptop\". This is likely part of a larger program called \"llm_chat_memory.ipynb\" which is used to store and manage conversation summaries in memory. This specific code is a combination of two other types of memory, ConversationSummaryMemory and ConversationBufferWindowMemory.'},\n",
       " {'id': '647b12b2-6c61-4a69-ade3-6828c14bab85',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"It is a dell xps 17\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet is a function called count_tokens that takes in two parameters, conversation_sumbuf and \"It is a dell xps 17\". The purpose of the code is to count the number of tokens in the given text string and return the count. \\n\\nThe file name is llm_chat_memory.ipynb, which suggests that the code snippet is likely part of a larger program or project related to chat or conversation memory. \\n\\nBased on the context provided, it appears that the code snippet is part of a type 4 memory system called ConversationSummaryBufferMemory. This type is a combination of two other types of memory, ConversationSummaryMemory and ConversationBufferWindowMemory, which likely work together to store and organize conversation summaries and buffer windows.'},\n",
       " {'id': 'e90f49de-c399-43a5-8328-fa91d77d259b',\n",
       "  'embedding': None,\n",
       "  'code': 'count_tokens(conversation_sumbuf,\"How do i start it up\")',\n",
       "  'filename': 'llm_chat_memory.ipynb',\n",
       "  'context': 'The code snippet in the file \"llm_chat_memory.ipynb\" is a function called \"count_tokens\" that takes in two arguments - \"conversation_sumbuf\" and \"How do i start it up\". It is part of a type 4 memory system called \"ConversationSummaryBufferMemory\" which is a combination of \"ConversationSummaryMemory\" and \"ConversationBufferWindowMemory\". The purpose of this code is to count the number of tokens (words or phrases) in a conversation summary using a buffer window. This allows for efficient storage and retrieval of conversation information.'},\n",
       " {'id': '4b424873-bc0a-4041-b5f3-ea89809626a3',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is installing various packages and dependencies related to the language processing tool Whisper. It first installs the langchain package, then langchain_pinecone, and then langchain with the optional docarray feature. It also installs docarray and pypdf. The file name is \"whisper_rag.ipynb\" and the context suggests that this code is being used to set up Whisper for use.'},\n",
       " {'id': '3dd6c81f-1930-4efe-b4e9-b6313cf617ab',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install openai-whisper',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet installs the openai-whisper package using pip3 and its dependencies. The file name is whisper_rag.ipynb, suggesting that it is a Jupyter notebook file related to the whisper package. The optional context mentions that the code is installing Whisper for the first time. Therefore, the code is likely setting up the environment for using the whisper package.'},\n",
       " {'id': '199b7275-d3c3-43a8-9053-71ed1fd7b684',\n",
       "  'embedding': None,\n",
       "  'code': '# on Ubuntu or Debian\\n#sudo apt update && sudo apt install ffmpeg\\n\\n# on Arch Linux\\n#sudo pacman -S ffmpeg\\n\\n# on MacOS using Homebrew (https://brew.sh/)\\n!brew install ffmpeg\\n\\n# on Windows using Chocolatey (https://chocolatey.org/)\\n#choco install ffmpeg\\n\\n# on Windows using Scoop (https://scoop.sh/)\\n#scoop install ffmpeg',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is installing the ffmpeg software on different platforms depending on the operating system. The file name is \"whisper_rag.ipynb\" and it is not clear what it contains without further context.'},\n",
       " {'id': '5205138b-fe71-44f3-b9e8-13cf4654123e',\n",
       "  'embedding': None,\n",
       "  'code': 'import whisper\\n\\nwhisper_model = whisper.load_model(\"base\")\\n\\ndef transcribe_audio(audio_path):\\n    result = whisper_model.transcribe(audio_path)\\n    return result[\\'text\\']\\n\\n# https://commons.wikimedia.org/wiki/File:Audio_Kevin_Folta.wav\\n# Audio example\\naudio_path = \"./files/Audio_Kevin_Folta.wav\" # CHANGE THIS FILE\\n\\n# Transcribir el audio\\ntranscribed_text = transcribe_audio(audio_path)\\n\\nprint(transcribed_text)\\n\\nwith open(\"./files/whisper_transcription.txt\", \"a\") as file:\\n    file.write(transcribed_text)\\n',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is importing the module \"whisper\" and loading a pre-trained model called \"base\". It then defines a function called \"transcribe_audio\" which uses the loaded model to transcribe an audio file and return the resulting text. The code provides an example audio file path and calls the function to transcribe it, printing the result and also appending it to a text file. The file name is specified as \"whisper_rag.ipynb\" and the context states that the code is used to transcribe an example audio file to text for further use.'},\n",
       " {'id': '094a13ae-e6d1-4275-b2b8-86759b0bf66a',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet defines the variable \"MODEL\" with the value \"llama3\" and provides a link for reference. The file name is \"whisper_rag.ipynb\". The context mentions instantiating the model and embeddings. This code likely refers to implementing a language model called \"llama3\" using the file \"whisper_rag.ipynb\" and loading the necessary components for the model to function properly. This may be part of a larger project or program that utilizes the llama3 language model.'},\n",
       " {'id': '1b957e7b-ad53-45db-931f-319062f07aae',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code above imports the Ollama model and Ollama embeddings from the langchain_community library. It then instantiates the Ollama model and embeddings using a specified MODEL. This code is likely used for language modeling and embedding tasks. The file name, whisper_rag.ipynb, suggests that this code may be used for analyzing and processing text data.'},\n",
       " {'id': '71f0ae1c-a968-40d6-b39a-df3d21b6b4e7',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./files/whisper_transcription.txt\")\\ntext_documents = loader.load()\\ntext_documents',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a function called TextLoader from a module called langchain_community.document_loaders. Then, it creates a variable called loader and assigns it the result of calling the TextLoader function with the file path \"./files/whisper_transcription.txt\" as an argument. Next, it calls the load method on the loader variable, which loads the text documents from the specified file path and assigns them to a variable called text_documents. Finally, it prints the value of the text_documents variable. The file name is whisper_rag.ipynb and the context suggests that this code is being used to load a transcription from a text file.'},\n",
       " {'id': 'bda06ad1-f5dd-49fb-9cdd-93495e1e42e0',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(text_documents)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is importing the RecursiveCharacterTextSplitter function from the langchain module. It then creates an instance of the function with specific chunk size and overlap parameters. The function is then used to split a document into smaller chunks, which are stored in a variable called text_documents. The file name is \"whisper_rag.ipynb\" and the context suggests that the code is splitting a document into smaller chunks for further processing.'},\n",
       " {'id': '818c79f2-fbaf-4fc7-b657-fc420a749913',\n",
       "  'embedding': None,\n",
       "  'code': 'text_documents',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is creating a variable called \"text_documents\" and assigning it a value, likely a collection of text documents. This variable is stored in a file named \"whisper_rag.ipynb\". The optional context suggests that the code is splitting the document into smaller parts called chunks. Overall, the code is likely manipulating and organizing text documents into manageable pieces.'},\n",
       " {'id': '86adc0e3-517e-495f-ae5f-3476e47771c4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet imports a package called langchain_community.vectorstores and uses it to create a document index called DocArrayInMemorySearch. This index stores documents in memory and is useful for small datasets. The file name is \"whisper_rag.ipynb\" and the code is used to store a PDF in a vector space. The code execution time depends on the complexity and length of the PDF file.'},\n",
       " {'id': 'be639937-1755-4750-b25b-da6f3db3eec9',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet creates a document index using the DocArrayInMemorySearch function and stores it in a variable named \"retriever\". This index is used to store a PDF file in a vector space. The file name is \"whisper_rag.ipynb\". The context provides additional information about the function and suggests keeping the PDF simple and small for faster execution time.'},\n",
       " {'id': '1641a03a-3c97-4f32-b9d5-0631e8dbad40',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code imports the \"PromptTemplate\" class from the \"langchain.prompts\" module and assigns it to the variable \"template\". The template variable contains a string with a question and a context placeholder. Then, it creates a new instance of the PromptTemplate class using the template string. The file name is \"whisper_rag.ipynb\" and the optional context, which is not provided in the given code, would be used to fill in the placeholder in the template string. Overall, the code is creating a conversation template to prompt the user for a question and provide a context for the question.'},\n",
       " {'id': 'c9c4a521-4f05-4331-9b3a-c7aff952fa3e',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a class called StrOutputParser from a module called langchain_core.output_parsers. It then creates an instance of this class and assigns it to a variable called parser. The file name is \"whisper_rag.ipynb\". The context mentions instantiating the parser, indicating that the code is creating an object of the StrOutputParser class, likely for the purpose of parsing text output.'},\n",
       " {'id': '280a934c-9f78-4ef2-970a-706207627bcd',\n",
       "  'embedding': None,\n",
       "  'code': 'while True:\\n    print(\"Say \\'exit\\' or \\'quit\\' to exit the loop\")\\n    question = input(\\'User question: \\')\\n    print(f\"Question: {question}\")\\n    if question.lower() in [\"exit\", \"quit\"]:\\n        print(\"Exiting the conversation. Goodbye!\")\\n        break\\n    retrieved_context = retriever.invoke(question)\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is a loop that continuously prompts the user to enter a question. If the user enters \"exit\" or \"quit\", the loop will break and the conversation will end. Otherwise, the code will retrieve context from a retriever, format a prompt using the retrieved context and the user\\'s question, invoke a model to generate a response, parse the response, and print it as an answer. This process will continue until the user exits the loop. The file name indicates that the code is likely part of a notebook file. The optional context suggests that the code is used to extract information from audio files.'},\n",
       " {'id': '98dca43e-3f57-4ddd-ae06-32f7ea4e19ab',\n",
       "  'embedding': None,\n",
       "  'code': '',\n",
       "  'filename': 'whisper_rag.ipynb',\n",
       "  'context': 'The code snippet is extracting information from an audio file. The file name is \"whisper_rag.ipynb\". The context suggests that the code is able to extract specific information from the audio file.'},\n",
       " {'id': '8f9b02a3-df36-437e-8538-ae760fae7d8e',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is installing five different packages using the pip3 command. These packages are langchain, langchain_pinecone, docarray, and pypdf. The code is being executed in the file named \"youtube_rag.ipynb\". The purpose of this code is to install the necessary dependencies for a project related to YouTube and the RAG (RetrievAndGenerate) model.'},\n",
       " {'id': '1b8f0093-cc71-4df3-ad12-5438a4cf7ea5',\n",
       "  'embedding': None,\n",
       "  'code': '!pip3 install youtube_transcript_api',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet installs the YouTube Transcript API library using pip3. The file name suggests that this code is being used in a Jupyter notebook for interacting with YouTube videos. The context suggests that this code is part of a larger section for installing dependencies and setting up the environment for using the YouTube Transcript API.'},\n",
       " {'id': '8d489f2e-6fa5-4952-9b87-3636ae2af1c4',\n",
       "  'embedding': None,\n",
       "  'code': 'from youtube_transcript_api import YouTubeTranscriptApi\\n\\nsrt = YouTubeTranscriptApi.get_transcript(\"pxiP-HJLCx0\") # CHANGE THE ID OF THE VIDEO\\n\\nwith open(\"./files/youtube_transcription.txt\", \"a\") as file:\\n    for i in srt:\\n        file.write(i[\\'text\\'])',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet uses the YouTubeTranscriptApi library to retrieve the transcript of a YouTube video with the given ID and stores it in the variable \"srt\". It then opens a file named \"youtube_transcription.txt\" and appends the text from each element in the \"srt\" list to the file. The file name \"youtube_rag.ipynb\" suggests that this code is part of a Jupyter notebook used for downloading and storing YouTube transcripts. The optional context clarifies that the video ID can be changed to download different transcripts and that the code saves the transcript to a file.'},\n",
       " {'id': 'a2dbbf60-5958-4378-993d-9d942a8bef80',\n",
       "  'embedding': None,\n",
       "  'code': '#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x7b\"\\n#MODEL = \"gemma:7b\"\\n#MODEL = \"llama2\"\\nMODEL = \"llama3\" # https://ollama.com/library/llama3',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet declares a variable named MODEL and assigns it different values such as \"gpt-3.5-turbo\", \"mixtral:8x7b\", \"gemma:7b\", and \"llama3\". The file name is \"youtube_rag.ipynb\". The context mentions instantiating the model and embeddings, indicating that the code is likely used for natural language processing tasks such as text generation or question-answering on YouTube data. The comment \"#MODEL = \"llama3\" # https://ollama.com/library/llama3\" provides a link to a website for further reference on the \"llama3\" model.'},\n",
       " {'id': '1662a2d5-7960-427b-a597-d63f8fe6b7ee',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=MODEL)\\nembeddings = OllamaEmbeddings(model=MODEL)',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code above imports the Ollama and OllamaEmbeddings modules from the langchain_community package. It then creates an instance of the Ollama model and an instance of the OllamaEmbeddings model, both using a specified model (MODEL). The file name is likely related to using these models for analyzing and generating text from YouTube comments, as indicated by the context comment.'},\n",
       " {'id': '54ef2e3e-8e4f-40c6-b3c3-860eca9429e7',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./files/youtube_transcription.txt\")\\ntext_documents = loader.load()\\ntext_documents',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet imports a module called TextLoader from a package named langchain_community.document_loaders. It then creates an instance of the TextLoader class, passing in the file path to a file named \"youtube_transcription.txt\". Next, it calls the load() method on the loader instance, which returns a collection of text documents. Finally, the code prints out the text documents. This code is likely used to load and access a transcription file saved from a YouTube video in the Jupyter Notebook file named \"youtube_rag.ipynb\".'},\n",
       " {'id': '30eda0ee-7191-4760-b35a-56aa977a838e',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\\ntext_documents = text_splitter.split_documents(text_documents)[:5]',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is importing the RecursiveCharacterTextSplitter function from the langchain.text_splitter module. It then creates an instance of this function, setting the chunk size to 100 and overlap to 20. This function is used to split text documents into smaller chunks. The code then uses this function to split a list of text documents, limiting it to the first 5 chunks. The file name is \"youtube_rag.ipynb\" and this code is being used to split documents into smaller chunks for further processing.'},\n",
       " {'id': '16c6baca-e145-4a93-aee0-5a07279dbd9f',\n",
       "  'embedding': None,\n",
       "  'code': 'text_documents',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is creating a function or variable called \"text_documents\" that is used to split a document into smaller chunks. The file name associated with this code is \"youtube_rag.ipynb\". The optional context suggests that this code is being used in a larger project or program related to YouTube.'},\n",
       " {'id': 'c843c07c-ffe7-4932-b4f7-d3cfeb3b23d1',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_documents(text_documents, embedding=embeddings)',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a package called \"langchain_community.vectorstores\" and then creating an object called \"DocArrayInMemorySearch\" from that package. This object is used to store documents in memory and is particularly useful for small datasets. The file name, \"youtube_rag.ipynb,\" suggests that this code may be used in a project related to YouTube. The context explains that the code is used to store a PDF in a vector space, and this process may take longer depending on the complexity and length of the PDF. The comment in the context also suggests that this code is being used as an example and encourages keeping the PDF simple for faster execution.'},\n",
       " {'id': 'd5d99d34-5b43-4120-8639-17e57be92cff',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectorstore.as_retriever()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is creating a document index using the Docarray library and storing it in memory. This index is used to retrieve information from a vector space. The context of this code suggests that it is being used to store a PDF file in a vector space, potentially for the purpose of language processing or analysis. The file name suggests that this code may be used in conjunction with YouTube, possibly to retrieve information from YouTube videos and store it in a vector space. The code is optimized to work with small and simple PDFs for faster execution.'},\n",
       " {'id': 'b7573242-22c6-4e31-a315-71903c475b7d',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.output_parsers import StrOutputParser\\n\\nparser = StrOutputParser()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is importing the \"StrOutputParser\" class from the \"langchain_core.output_parsers\" module. It then creates an instance of the \"StrOutputParser\" class and assigns it to the variable \"parser\". This code is most likely used for parsing output data from a language model. The file name is \"youtube_rag.ipynb\", which suggests that the code is being used in a Jupyter notebook for working with YouTube content. The comment \"# We instantiate the parser\" further confirms that the code is instantiating the parser, likely for further data manipulation or analysis.'},\n",
       " {'id': 'e7d43995-2417-4f73-b013-00568a9bdd6a',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import PromptTemplate\\n\\ntemplate = \"\"\"\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, answer with \"I don\\'t know\".\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)\\nprompt.format(context=\"Here is some context\", question=\"Here is a question\")',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is importing a prompt template from the \"langchain\" library and assigning it to a variable. It then creates a template string with a context and question placeholder. The template is then used to create a prompt object, which can be formatted with a specific context and question. The file name is associated with a YouTube conversation template.'},\n",
       " {'id': '60b96685-6a3b-49cc-a156-8e652f7f090b',\n",
       "  'embedding': None,\n",
       "  'code': 'retrieved_context = retriever.invoke(\"laptop\")\\nquestions = [\\n    \"Which is the best laptop for students?\",\\n    \"How much is a laptop worth?\",\\n    \"Make a summary of the video\"\\n]\\n\\nfor question in questions:\\n    formatted_prompt = prompt.format(context=retrieved_context, question=question)\\n    response_from_model = model.invoke(formatted_prompt)\\n    parsed_response = parser.parse(response_from_model)\\n\\n    print(f\"Question: {question}\")\\n    print(f\"Answer: {parsed_response}\")\\n    print()',\n",
       "  'filename': 'youtube_rag.ipynb',\n",
       "  'context': 'The code snippet is performing a natural language processing task using a RAG (Retrieval-Augmented Generation) model. It first retrieves a context (likely a video or article) related to the topic \"laptop\" using a \"retriever\" function. Then, it creates a list of questions and uses them to generate prompts with the retrieved context. The prompts are then passed to the RAG model, which generates a response. This response is then parsed using a parser, and the question and answer are printed for each question. The file name indicates that this code is likely being run in a Jupyter Notebook, and the optional context is a comment explaining the purpose of the code.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436a808-52a4-406c-8f2b-d3a4283fffc9",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings and Structure Data\n",
    "\n",
    "In this step, we use an embeddings model to generate embedding vectors for the context extracted from each code snippet. The code performs the following operations:\n",
    "\n",
    "**HuggingFace Embeddings**: We use the HuggingFace embeddings model \"all-MiniLM-L6-v2\" to generate vectors that semantically represent the context of the code snippets.\n",
    "\n",
    "**Function** *update_embeddings*: This function iterates through the previously extracted data structure. For each item:\n",
    "\n",
    "- Generates an embedding vector from the context field using the embed_query method of the embeddings model.\n",
    "- Updates the item in the data structure, inserting the new embedding vector into the embedding field.\n",
    "Conversion to DataFrame: After updating the data structure with the embeddings, we use the to_dataframe_row function to convert the list of code snippets and their respective metadata into a format suitable for a Pandas DataFrame.\n",
    "\n",
    "Each item in the data structure is converted into a dictionary containing:\n",
    "\n",
    "- **ID**: A unique identifier for the code snippet.\n",
    "- **Embeddings**: The embedding vector generated for the context.\n",
    "- **Code**: The extracted code.\n",
    "- **Metadata**: Additional metadata, such as the filename and updated context.\n",
    "  \n",
    "The list of dictionaries is then converted into a DataFrame.\n",
    "\n",
    "Creating the DataFrame: The to_dataframe_row function organizes this data, and Pandas is used to create a DataFrame, facilitating the manipulation and future use of the data with the results stored in a DataFrame for easy visualization and further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9be8588b-6138-468a-9a00-32d0191f9a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-09-17 13:21:14.542555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-17 13:21:16.520573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da802273f61645f694e289ecaaebdae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1143196dce43eeab969e5bd63b810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e749d4a5b9584e9fbbdc326a9e6c531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f24ce3746c34df7bd8252e19ac80309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0ff295d65a4c2b9e96bfe16c73fbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6627709c301b4866940904c51abaa60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978268d2f3564fc39ec10e67f2fc3eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d7cf6ad96a4f89bfd745b66132b729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33864edf977342e1b5e32722f7868b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57e9280067c448d90bf18ba63897884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad311e82337c49e495cfa5ebf4066fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "528c975c-bd47-476e-9791-73feafc3b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(data_structure):\n",
    "    updated_structure = []\n",
    "    for item in data_structure:\n",
    "        context = item['context']\n",
    "\n",
    "        # Generate the embedding for the context\n",
    "        embedding_vector = embeddings.embed_query(context)\n",
    "\n",
    "        # Update the item with the new embedding\n",
    "        item['embedding'] = embedding_vector\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe046de9-f2e4-48ac-afa1-bf4d401edf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_structure = update_embeddings(all_extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccb3ca2e-2508-4efc-a9ae-d41feaf977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_dataframe_row(embedded_snippets: list):\n",
    "    \"\"\"\n",
    "    Helper function to convert a list of embedded snippets into a dataframe row\n",
    "    in dictionary format.\n",
    "\n",
    "    Args:\n",
    "        embedded_snippets: List of dictionaries containing Snippets to be converted\n",
    "\n",
    "    Returns:\n",
    "        List of Dictionaries suitable for conversion to a DataFrame\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for snippet in embedded_snippets:\n",
    "        output = {\n",
    "            \"ids\": snippet['id'],\n",
    "            \"embeddings\": snippet['embedding'],\n",
    "            \"code\": snippet['code'],\n",
    "            \"metadatas\": {\n",
    "                \"filenames\": snippet['filename'],\n",
    "                \"context\": snippet['context'],\n",
    "            },\n",
    "        }\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff1903e6-e135-4a19-a3e1-4c0add9f981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = to_dataframe_row(updated_structure)\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "405a0859-89a0-4ca5-a2c9-c2f2e752c82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>code</th>\n",
       "      <th>metadatas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06a1bf36-1943-4fa4-bedc-6b0c9b6a7775</td>\n",
       "      <td>[-0.012226501479744911, 0.07351084053516388, -...</td>\n",
       "      <td>!pip3 install langchain\\n!pip3 install langcha...</td>\n",
       "      <td>{'filenames': 'example_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e27724b3-3607-427c-833b-9745b630d7d1</td>\n",
       "      <td>[-0.04303930699825287, 0.00578521890565753, -0...</td>\n",
       "      <td>#MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x...</td>\n",
       "      <td>{'filenames': 'example_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c335cbc6-421b-40e9-acf0-042ac2b87dbc</td>\n",
       "      <td>[-0.1125277429819107, 0.00823664665222168, -0....</td>\n",
       "      <td>from langchain_community.llms import Ollama\\nf...</td>\n",
       "      <td>{'filenames': 'example_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f7671a48-7252-49ca-9bc4-7dd3771a5521</td>\n",
       "      <td>[-0.035975854843854904, 0.019312957301735878, ...</td>\n",
       "      <td>model.invoke(\"Waht is 2+2?\")</td>\n",
       "      <td>{'filenames': 'example_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>822093d3-820f-4621-a7da-a0c8892feadd</td>\n",
       "      <td>[-0.07964743673801422, 0.06151750311255455, -0...</td>\n",
       "      <td>from langchain_core.output_parsers import StrO...</td>\n",
       "      <td>{'filenames': 'example_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>c843c07c-ffe7-4932-b4f7-d3cfeb3b23d1</td>\n",
       "      <td>[-0.0798792690038681, 0.033993981778621674, -0...</td>\n",
       "      <td>from langchain_community.vectorstores import D...</td>\n",
       "      <td>{'filenames': 'youtube_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>d5d99d34-5b43-4120-8639-17e57be92cff</td>\n",
       "      <td>[-0.06140899658203125, 0.058361735194921494, -...</td>\n",
       "      <td>retriever = vectorstore.as_retriever()</td>\n",
       "      <td>{'filenames': 'youtube_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>b7573242-22c6-4e31-a315-71903c475b7d</td>\n",
       "      <td>[-0.05073198303580284, -0.021852439269423485, ...</td>\n",
       "      <td>from langchain_core.output_parsers import StrO...</td>\n",
       "      <td>{'filenames': 'youtube_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>e7d43995-2417-4f73-b013-00568a9bdd6a</td>\n",
       "      <td>[-0.0349389873445034, 0.0597563199698925, -0.0...</td>\n",
       "      <td>from langchain.prompts import PromptTemplate\\n...</td>\n",
       "      <td>{'filenames': 'youtube_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>60b96685-6a3b-49cc-a156-8e652f7f090b</td>\n",
       "      <td>[-0.07848169654607773, 0.0859607681632042, -0....</td>\n",
       "      <td>retrieved_context = retriever.invoke(\"laptop\")...</td>\n",
       "      <td>{'filenames': 'youtube_rag.ipynb', 'context': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ids  \\\n",
       "0   06a1bf36-1943-4fa4-bedc-6b0c9b6a7775   \n",
       "1   e27724b3-3607-427c-833b-9745b630d7d1   \n",
       "2   c335cbc6-421b-40e9-acf0-042ac2b87dbc   \n",
       "3   f7671a48-7252-49ca-9bc4-7dd3771a5521   \n",
       "4   822093d3-820f-4621-a7da-a0c8892feadd   \n",
       "..                                   ...   \n",
       "65  c843c07c-ffe7-4932-b4f7-d3cfeb3b23d1   \n",
       "66  d5d99d34-5b43-4120-8639-17e57be92cff   \n",
       "67  b7573242-22c6-4e31-a315-71903c475b7d   \n",
       "68  e7d43995-2417-4f73-b013-00568a9bdd6a   \n",
       "69  60b96685-6a3b-49cc-a156-8e652f7f090b   \n",
       "\n",
       "                                           embeddings  \\\n",
       "0   [-0.012226501479744911, 0.07351084053516388, -...   \n",
       "1   [-0.04303930699825287, 0.00578521890565753, -0...   \n",
       "2   [-0.1125277429819107, 0.00823664665222168, -0....   \n",
       "3   [-0.035975854843854904, 0.019312957301735878, ...   \n",
       "4   [-0.07964743673801422, 0.06151750311255455, -0...   \n",
       "..                                                ...   \n",
       "65  [-0.0798792690038681, 0.033993981778621674, -0...   \n",
       "66  [-0.06140899658203125, 0.058361735194921494, -...   \n",
       "67  [-0.05073198303580284, -0.021852439269423485, ...   \n",
       "68  [-0.0349389873445034, 0.0597563199698925, -0.0...   \n",
       "69  [-0.07848169654607773, 0.0859607681632042, -0....   \n",
       "\n",
       "                                                 code  \\\n",
       "0   !pip3 install langchain\\n!pip3 install langcha...   \n",
       "1   #MODEL = \"gpt-3.5-turbo\"\\n#MODEL = \"mixtral:8x...   \n",
       "2   from langchain_community.llms import Ollama\\nf...   \n",
       "3                        model.invoke(\"Waht is 2+2?\")   \n",
       "4   from langchain_core.output_parsers import StrO...   \n",
       "..                                                ...   \n",
       "65  from langchain_community.vectorstores import D...   \n",
       "66             retriever = vectorstore.as_retriever()   \n",
       "67  from langchain_core.output_parsers import StrO...   \n",
       "68  from langchain.prompts import PromptTemplate\\n...   \n",
       "69  retrieved_context = retriever.invoke(\"laptop\")...   \n",
       "\n",
       "                                            metadatas  \n",
       "0   {'filenames': 'example_rag.ipynb', 'context': ...  \n",
       "1   {'filenames': 'example_rag.ipynb', 'context': ...  \n",
       "2   {'filenames': 'example_rag.ipynb', 'context': ...  \n",
       "3   {'filenames': 'example_rag.ipynb', 'context': ...  \n",
       "4   {'filenames': 'example_rag.ipynb', 'context': ...  \n",
       "..                                                ...  \n",
       "65  {'filenames': 'youtube_rag.ipynb', 'context': ...  \n",
       "66  {'filenames': 'youtube_rag.ipynb', 'context': ...  \n",
       "67  {'filenames': 'youtube_rag.ipynb', 'context': ...  \n",
       "68  {'filenames': 'youtube_rag.ipynb', 'context': ...  \n",
       "69  {'filenames': 'youtube_rag.ipynb', 'context': ...  \n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43d8f8-09ed-49c7-8504-2574a78bfb5a",
   "metadata": {},
   "source": [
    "## Step 4: Store and Query Documents in ChromaDB 🔗🏦\n",
    "\n",
    "In this step, we use ChromaDB, a vector database system, to store code snippets and their respective metadata. We also implement a function to retrieve documents based on queries. The code performs the following operations:\n",
    "\n",
    "####  Connection and Collection Creation\n",
    "- **ChromaDB Client**: A ChromaDB client is initialized to interact with the database.\n",
    "- **Collection Creation or Retrieval**: The collection named \"my_collection\" is created (or retrieved, if it already exists) within the ChromaDB database. Collections are used to store documents and their corresponding embeddings.\n",
    "#### Inserting Documents\n",
    "- **Data Extraction**: The following fields are extracted from the DataFrame and converted into lists:\n",
    "   - **ids**: A list of unique identifiers for each document (code snippet).\n",
    "   - **documents**: A list of code snippets.\n",
    "   - **metadatas**: A list of metadata associated with each document, such as the filename and context.\n",
    "   - **embeddings_list**: A list of embedding vectors previously generated for the context of each code snippet.\n",
    "- **Inserting into ChromaDB**: The upsert method is used to insert or update the documents, ids, metadata, and embeddings in the created collection.\n",
    "#### Querying Documents\n",
    "- **Query**: After adding the documents to the collection, a query is performed. The code searches for documents related to the query text \"!pip install\", returning the 5 most relevant results.\n",
    "#### *retriever* **Function*\n",
    "- **Document Retrieval**: The retriever function is implemented to query the collection. It takes a query string, the collection, and the number of results to return (top_n) as parameters.\n",
    "  - **Query in ChromaDB**: The function executes a query in the collection using the provided string.\n",
    "  - **Creating Document Objects**: For each result returned, the function creates a Document instance containing the page content (code snippet) and its metadata.\n",
    "  - **Returning Documents**: The function returns a list of Document objects that contain the page content and metadata for easy retrieval and future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b02ae488-1639-48f9-b988-30651042ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully!!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "ids = df[\"ids\"].tolist()\n",
    "documents = df[\"code\"].tolist()\n",
    "metadatas = df[\"metadatas\"].tolist()\n",
    "embeddings_list = df[\"embeddings\"].tolist()\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings_list  \n",
    ")\n",
    "\n",
    "print(\"Documents added successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ea2e862-d333-40a7-bea8-befb20570f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in the collection: 70\n"
     ]
    }
   ],
   "source": [
    "document_count = collection.count()\n",
    "print(f\"Total documents in the collection: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "989c4fee-50ae-4a13-bea4-7ae479723f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:24<00:00, 3.39MiB/s]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"!pip install\"],\n",
    "    n_results=5,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22ef3dc4-9154-420e-823f-60d0548b20ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['8f9b02a3-df36-437e-8538-ae760fae7d8e',\n",
       "   '3dd6c81f-1930-4efe-b4e9-b6313cf617ab',\n",
       "   '06a1bf36-1943-4fa4-bedc-6b0c9b6a7775',\n",
       "   '49e92e04-2ebe-4cf5-8696-ee1bacd58f89',\n",
       "   '1b8f0093-cc71-4df3-ad12-5438a4cf7ea5']],\n",
       " 'distances': [[1.1053783893585205,\n",
       "   1.1907209157943726,\n",
       "   1.1907271146774292,\n",
       "   1.1952283382415771,\n",
       "   1.2564711570739746]],\n",
       " 'metadatas': [[{'context': 'The code snippet is installing five different packages using the pip3 command. These packages are langchain, langchain_pinecone, docarray, and pypdf. The code is being executed in the file named \"youtube_rag.ipynb\". The purpose of this code is to install the necessary dependencies for a project related to YouTube and the RAG (RetrievAndGenerate) model.',\n",
       "    'filenames': 'youtube_rag.ipynb'},\n",
       "   {'context': 'The code snippet installs the openai-whisper package using pip3 and its dependencies. The file name is whisper_rag.ipynb, suggesting that it is a Jupyter notebook file related to the whisper package. The optional context mentions that the code is installing Whisper for the first time. Therefore, the code is likely setting up the environment for using the whisper package.',\n",
       "    'filenames': 'whisper_rag.ipynb'},\n",
       "   {'context': 'The code snippet installs several packages using pip3 in order to run a file named \"example_rag.ipynb\". One of the packages, \"docarray\", may cause an error, so the code also includes a solution to address this potential issue.',\n",
       "    'filenames': 'example_rag.ipynb'},\n",
       "   {'context': 'The code snippet is installing four different packages using the pip3 command. These packages are called llama_index, llama-index-readers-github, llama-index-embeddings-langchain, and llama-index-llms-ollama. The file name associated with this code is github_repo_rag.ipynb. This code is likely part of a project or notebook that is related to GitHub repositories. The context provided suggests that this code is part of a larger process of installing dependencies.',\n",
       "    'filenames': 'github_repo_rag.ipynb'},\n",
       "   {'context': 'The code snippet installs the YouTube Transcript API library using pip3. The file name suggests that this code is being used in a Jupyter notebook for interacting with YouTube videos. The context suggests that this code is part of a larger section for installing dependencies and setting up the environment for using the YouTube Transcript API.',\n",
       "    'filenames': 'youtube_rag.ipynb'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "   '!pip3 install openai-whisper',\n",
       "   '!pip3 install langchain\\n!pip3 install langchain_pinecone\\n!pip3 install langchain[docarray]\\n!pip3 install docarray\\n!pip3 install pypdf',\n",
       "   '!pip3 install llama_index\\n!pip3 install llama-index-readers-github\\n!pip3 install llama-index-embeddings-langchain\\n!pip3 install llama-index-llms-ollama',\n",
       "   '!pip3 install youtube_transcript_api']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ea4d5b-5d97-4906-8f96-9c73569bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def retriever(query: str, collection, top_n: int = 10) -> List[Document]:\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_n\n",
    "    )\n",
    "    \n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=str(results['documents'][i]),\n",
    "            metadata=results['metadatas'][i] if isinstance(results['metadatas'][i], dict) else results['metadatas'][i][0]  # Corrigir o metadado se for uma lista\n",
    "        )\n",
    "        for i in range(len(results['documents']))\n",
    "    ]\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2ac5-1ad2-4125-ad23-03708db54075",
   "metadata": {},
   "source": [
    "## Step 5: Chain 🦜⛓️\n",
    "\n",
    "In this step, we use a flow to automatically generate Python code based on a provided context and question. The code performs the following:\n",
    "\n",
    "#### Function *format_docs(docs: List[Document]) -> str:*\n",
    "- **Purpose**: This function formats a list of documents docs into a single string by concatenating the content of each document (doc.page_content) with two line breaks (\\n\\n) between them. This ensures that the context used in code generation is organized and readable.\n",
    "\n",
    "#### Language Model and Processing Chain:\n",
    "- **ChatOpenAI**: A language model from OpenAI is used to generate responses based on the provided prompt.\n",
    "- The **chain**processes data using the following components:\n",
    "  - **Context**: The context is formatted using the *format_docs* function, which calls the retriever function to fetch relevant context from the document base.\n",
    "  - **Question**: The question is passed directly through the chain to process the prompt.\n",
    "  - **Model**: The model generates the code based on the template and the provided data.\n",
    "  - **Output Parser**: The output is processed with StrOutputParser to ensure the return is a clean string.\n",
    "\n",
    "#### Function *clean_and_print_code(result: str)*:\n",
    "- Purpose: This function takes the generated code string from the model and removes any formatting markers (e.g., ```python). After cleaning, the code is printed in a clean format, ready for execution.\n",
    "\n",
    "#### Interaction with Galileo:\n",
    "- The *promptquality* library is used to evaluate the quality of the generated prompts.\n",
    "- **Galileo Callback**: A custom callback is configured using the Galileo API Key, where the following evaluation scopes are set:\n",
    "   - **Context Adherence**: Evaluates whether the generated code aligns with the provided context.\n",
    "   - **Correctness**: Checks the factual accuracy of the generated code.\n",
    "   - **Prompt Perplexity**: Measures the complexity of the prompt, useful for evaluating its clarity.\n",
    " \n",
    "#### Chain Execution:\n",
    "- A set of inputs containing the query and the question is provided to run the chain. The system generates code based on questions like \"How can I use audio in RAG?\" and \"create code audio with RAG\" using the vector base.\n",
    "\n",
    "#### Results Publishing:\n",
    "- The Galileo callback finalizes and publishes the results, recording the evaluation of each run of the code generation chain.\n",
    "\n",
    "#### Function *create_new_code_cell_from_output(output)*:\n",
    " - Purpose: This function dynamically creates a new code cell in the Jupyter Notebook from the generated output. It handles different output formats such as strings or dictionaries (if the output contains JSON) and inserts the resulting code into the next code cell in the notebook.\n",
    "\n",
    "\n",
    "#### Processing the results: \n",
    "- After the chain execution, the function iterates over each generated result, attempts to parse it as JSON, and creates a new code cell in the notebook from the output. If the result is not JSON, it treats the output as a code string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2fe1793-7624-44f9-8878-fc861b83dfd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI() \n",
    "\n",
    "chain = {\n",
    "    \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b31ca-f9ae-4c7d-b732-76568d00fdf6",
   "metadata": {},
   "source": [
    "### Local Model\n",
    "Cell to run a local model using LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531fbad-3fee-4005-8d1d-9e573901e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_docs(docs: List[Document]) -> str:\n",
    "    # return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "# Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# model = llm_local() \n",
    "\n",
    "# chain = {\n",
    "    # \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    # \"question\": RunnablePassthrough()\n",
    "# } | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9790269-881b-4dab-9d8d-0e18a4fa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_print_code(result: str):\n",
    "    clean_code = result.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    print(clean_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7dbbab73-a108-4652-8ec9-80cef16727fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as diogo.vieira@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=Url('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='diogo.vieira@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=Url('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "os.environ['GALILEO_API_KEY'] = \"htMRukWlQyvOEDMnAUYQUTQnEZL6_3ubALGkhn6ph70\" #your api Key\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "pq.login(galileo_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11037204-855f-4149-980e-782dc9b34766",
   "metadata": {},
   "source": [
    "### Information Parameter 💡\n",
    "\n",
    "**Query**: A query is generally used to retrieve information, such as documents or code snippets, from a database or retrieval system, like a vector database or an embeddings database. In this case, the query is likely being used to search for code snippets related to the specific request, such as the creation of an LLM model and an embedding model.\n",
    "\n",
    "**Question**: The question represents the specific task you are asking the language model to perform. This involves generating code based on the context retrieved by the query. The question is sent to the LLM to generate the appropriate response or code based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf3441bd-a570-4010-8af8-2f4ed3f30d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439f1c8f1484411ba67baa0db57def22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "prompt_perplexity: Failed ❌, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "latency: Done ✅\n",
      "groundedness: Failed ❌, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/81833064-c547-4e3c-b0f1-e99da1f7250c/3094a823-e59a-42ce-a09d-9f61f2deef0e?taskType=12\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Code\n",
    "\n",
    "\n",
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    scorers=[\n",
    "        pq.Scorers.context_adherence_plus,  # groundedness\n",
    "        pq.Scorers.correctness,             # factuality\n",
    "        pq.Scorers.prompt_perplexity        # perplexity \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example of inputs to run the chain\n",
    "inputs = [\n",
    "    {\"query\": \"instantiate the LLM model and the Embedding model\", \"question\": \"create code llm model and the embedding model\"},\n",
    "\n",
    "]\n",
    "#How to create a vector bank?\n",
    "#create code a chromadb vector database\n",
    "\n",
    "results = chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# Publish run results\n",
    "prompt_handler.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9185a95d-6a9a-428a-8074-d394246d73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "def create_new_code_cell_from_output(output):\n",
    "    \"\"\"\n",
    "    Creates a new code cell in Jupyter Notebook from an output,\n",
    "    dealing with different output formats.\n",
    "\n",
    "    Args:\n",
    "        output: The output to be inserted into the new cell. It can be a string, a dictionary\n",
    "                or another type of object.\n",
    "    \"\"\"\n",
    "\n",
    "    shell = get_ipython()\n",
    "\n",
    "    if isinstance(output, dict):\n",
    "        code = output['cells'][0]['source']\n",
    "        code = ''.join(code)\n",
    "    else:\n",
    "        code = str(output)\n",
    "\n",
    "    clean_code = code.strip()\n",
    "\n",
    "    shell.set_next_input(clean_code, replace=False)\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        output = json.loads(result)\n",
    "        create_new_code_cell_from_output(output)\n",
    "    except json.JSONDecodeError:\n",
    "        # If it's not JSON, just treat it as a string of code\n",
    "        create_new_code_cell_from_output(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88207b-e1ba-4b92-9fbc-b9c587c9e04b",
   "metadata": {},
   "source": [
    "### LLM generated code here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8060fb-f95e-4b6f-871d-4ed927013807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:summarization]",
   "language": "python",
   "name": "conda-env-summarization-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
