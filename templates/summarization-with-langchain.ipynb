{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# Summarization of transcripts with Langchain\n",
    "\n",
    "In this example, we intend to create a summarizer for long transcripts. The main goal is to break the original transcript into different chunks based on context - i.e. using an unsupervised approach to identify the different topics throughout the transcript (somehow similarly to Topic Modelling) - and summarize each of these chunks. in the end, the different summaries are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "Most of the libraries that are necessary for the development of this example are built-in on the GenAI workspace, available in AI Studio. More specific libraries to handle the type of input will be added here. In this case, we are giving support to transcripts in the webvtt format, used to store transcripts, which require the webvtt-py library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8fcd-58ec-4eb9-a9bc-8c93d632f284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install webvtt-py\n",
    "!pip install pandas\n",
    "!pip install promptquality==0.64.2\n",
    "!pip install httpx==0.27.2\n",
    "!pip install galileo-protect==0.15.1\n",
    "!pip install galileo-observe==1.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff65e3-dbdf-457a-8746-66c453182f26",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a24f4-01f4-4a33-8124-7d7601ced6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from the transcript\n",
    "\n",
    "At first, we need to read the data from the transcript. As our transcript is in the .vtt format, we use a library called webvtt-py to read the content. As the text is a trancript of audio/video, it is organized in small chunks of conversation, each containing a sequential id, the time of the start and end of the chunk, and the text content (often in the form speaker:content).\n",
    "\n",
    "From this data, we expect to extract the actual content,  while keeping reference to the other metadata - for this reason, we are loading all the data into a Pandas dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16a213-9f92-4c75-93ff-66adc3133cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for caption in webvtt.read('data/I_have_a_dream.vtt'):\n",
    "    line = caption.text.split(\":\")\n",
    "    while len(line) < 2:\n",
    "        line = [''] + line\n",
    "    data[\"id\"].append(caption.identifier)\n",
    "    data[\"speaker\"].append(line[0].strip())\n",
    "    data[\"content\"].append(line[1].strip())\n",
    "    data[\"start\"].append(caption.start)\n",
    "    data[\"end\"].append(caption.end)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb6763-3eb0-41fb-900f-67778c3d5caf",
   "metadata": {},
   "source": [
    "As a second option, we provide here a code to load the same structure from a plain text document, which only contains the actual content of the speech/conversation, without extra metadata. For the sake of simplicity and reuse of code, we keep the same Data Frame structure as the previous version, by filling the remaining fields with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33cbb-1c2b-404e-ad65-b243c6702308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"data/I_have_a_dream.txt\") as file:\n",
    "    lines = file.read()\n",
    "\n",
    "data = {\n",
    "    \"id\": [],\n",
    "    \"speaker\": [],\n",
    "    \"content\": [],\n",
    "    \"start\": [],\n",
    "    \"end\": []\n",
    "}\n",
    "\n",
    "for line in lines.split(\"\\n\"):\n",
    "    if line.strip() != \"\":\n",
    "        data[\"id\"].append(\"\")\n",
    "        data[\"speaker\"].append(\"\")\n",
    "        data[\"content\"].append(line.strip())\n",
    "        data[\"start\"].append(\"\")\n",
    "        data[\"end\"].append(\"\")        \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e122c-5a54-4285-8788-be12fc86e278",
   "metadata": {},
   "source": [
    "## Step 2: Semantic chunking of the transcript\n",
    "Having the information content loaded according to the transcription format - with the text split into audio blocks, or into paragraphs, we now want to group these small blocks into relevant topics - so we can summarize each topic individually. Here, we are using a very simple approach for that, by using a semantic embedding of each sentence (using an embedding model from Hugging Face Sentence Transformers), and identifying the \"breaks\" among chunks as the ones with higher semantic distance. Notice that this method can be parameterized, to inform the number of topics or the best method to identify the breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c67feb-11f7-47ad-bdec-3ec252e51797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(df.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5538aee-0233-4b58-a574-879dfa64a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSplitter():\n",
    "    def __init__ (self, content, embedding_model, method=\"number\", partition_count = 10, quantile = 0.9):\n",
    "        self.content = content\n",
    "        self.embedding_model = embedding_model\n",
    "        self.partition_count = partition_count\n",
    "        self.quantile = quantile\n",
    "        self.embeddings = embedding_model.encode(content)\n",
    "        self.distances = [cosine(embeddings[i - 1], embeddings[i]) for i in range(1, len(embeddings))]\n",
    "        self.breaks = []\n",
    "        self.centroids = []\n",
    "        self.load_breaks(method=method)\n",
    "\n",
    "    def centroid_distance(self, embedding_id, centroid_id):\n",
    "        return cosine(self.embeddings[embedding], self.centroid[centroid])\n",
    "\n",
    "    def adjust_neighbors(self):\n",
    "        self.breaks = []\n",
    "\n",
    "    def load_breaks(self, method = 'number'):\n",
    "        if method == 'number':\n",
    "            if self.partition_count > len(self.distances):\n",
    "                self.partition_count = len(self.distances)\n",
    "            self.breaks = np.sort(np.argpartition(self.distances, self.partition_count - 1)[0:self.partition_count - 1])\n",
    "        elif method == 'quantiles':\n",
    "            threshold = np.quantile(self.distances, self.quantile)\n",
    "            self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "        else:\n",
    "            self.breaks = []\n",
    "\n",
    "    def get_centroid(self, beginning, end):\n",
    "        return embedding_model.encode('\\n'.join(self.content[beginning : end]))\n",
    "    \n",
    "    def load_centroids(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            self.centroids = [self.get_centroid(0, len(self.content))]\n",
    "        else:\n",
    "            self.centroids = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                self.centroids += [self.get_centroid(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            self.centroids += [self.get_centroid(beginning, len(self.content))]\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        return '\\n'.join(self.content[beginning : end])\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        if len(self.breaks) == 0:\n",
    "            return [self.get_chunk(0, len(self.content))]\n",
    "        else:\n",
    "            chunks = []\n",
    "            beginning = 0\n",
    "            for break_position in self.breaks:\n",
    "                chunks += [self.get_chunk(beginning, break_position + 1)]\n",
    "                beginning = break_position + 1\n",
    "            chunks += [self.get_chunk(beginning, len(self.content))]\n",
    "        return chunks\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f13a0c-9704-462c-ba20-6b267367648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_separator = \"\\n *-* \\n\"\n",
    "\n",
    "splitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\n",
    "chunks = chunk_separator.join(splitter.get_chunks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 3: Using a LLM model to Summarize each chunk\n",
    "In our example, we are going to summarize each individual chunk separately. This solution might be advantageous in different situations:\n",
    " * When the original text is too big , or the loaded model works with a context that is too small. In this scenario, breaking information into chunks are necessary to allow the model to be applied\n",
    " * When the user wants to make sure that all the separate topics of a conversation are covered into the summarized version. An extra step could be added to allow some verification or manual configuration of the chunks to allow the user to customize the output\n",
    "\n",
    "To achieve this goal, we load a LLM model and use a summarization prompt. For the model, we illustrate four different options here:\n",
    "* Calling an cloud API (e.g. openAI API): This would require an API key from the desired service. In our example, we reccomend saving your API keys into a secrets.yaml file, and not shared together with the code, for security issues. An example with empty keys is provided with our code\n",
    "* Connecting to a Hugging Face rest API: This also requires an API key\n",
    "* Loading the model locally using Hugging Face repo: This would require to download the model in the first time you run your code. This might take several minutes (depending on your internet connection), and the model will be saved in local HF cache (set to be persisted in the beginning of this notebook)\n",
    "* Loading the model from a file available as a project asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80dd01-5200-4690-a323-14b5e4acd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate code to connect to Hugging Face models\n",
    "#from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "#import yaml\n",
    "#with open('secrets.yaml') as file:\n",
    "#    secrets = yaml.safe_load(file)\n",
    "#huggingfacehub_api_token = secrets[\"HuggingFace\"]\n",
    "\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#llm = HuggingFaceEndpoint(\n",
    "#   huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "#   repo_id=repo_id,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fb7e0-75db-4166-93f3-bd3d9ff547fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_huggingface import HuggingFacePipeline\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "#model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n",
    "#llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62518d5a-8321-4c17-ba61-7f8960f4545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternate code to load local models. \n",
    "###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\n",
    "\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "            model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            n_gpu_layers=64,\n",
    "            n_batch=512,\n",
    "            n_ctx=4096,\n",
    "            max_tokens=1024,\n",
    "            f16_kv=True,  \n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,\n",
    "            stop=[],\n",
    "            streaming=False,\n",
    "            temperature=0.4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cdb8f-a70a-499a-a2d6-56c14d965169",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''\n",
    "The following text is an excerpt of a transcription:\n",
    "\n",
    "### \n",
    "{context} \n",
    "###\n",
    "\n",
    "Please, produce a single paragraph summarizing the given excerpt.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a586d-fbf5-4551-b022-d50da386e74c",
   "metadata": {},
   "source": [
    "## Step 4: Create parallel chain to summarize the transcript\n",
    "\n",
    "In the following cell, we create a chain that will receive a single string with multiple chunks (separated by the declared separator), than:\n",
    "  * Break the input into separated chains - using the break_chunks function embedded in a RunnableLambda to be used in LangChain\n",
    "  * Run a Parallel Chain with the following elements for each chunk:\n",
    "    * Get an individual element\n",
    "    * Personalize the prompt template to create an individual prompt for each chunk\n",
    "    * Use the LLM inference to summarize the chunk\n",
    "  * Merge the individual summaries into a single one\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5cde3-b064-4280-8ada-8df68820a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def join_summaries(summaries):\n",
    "    return \"\\n\".join(list(summaries.values()))\n",
    "\n",
    "def break_chunks(chunks):\n",
    "    return chunks.split(chunk_separator)\n",
    "\n",
    "lambda_join = RunnableLambda(join_summaries)\n",
    "lambda_break = RunnableLambda(break_chunks)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = lambda_break | {f\"summary_{i}\" : itemgetter(i) | prompt | llm  for (i, _) in enumerate(RunnablePassthrough())} | lambda_join | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 5: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "import yaml\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "os.environ['GALILEO_API_KEY'] = secrets[\"Galileo\"]\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "pq.login(galileo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d47fe-41b9-4f19-9b11-4ef2d3f2740f",
   "metadata": {},
   "source": [
    "## Step 6: Run the chain and connect the metrics to Galileo\n",
    "\n",
    "In this session, we call the created chain and create the mechanisms to ingest the quality metrics into Galileo. For this example, we create a personalized metric (scorer), that will be running locally to measure the quality of the summarization. For this reason, we use HuggingFace implementation of ROUGE (using evaluate library), and implement into a CustomScorer from Galileo (next cell).\n",
    "\n",
    "Below, we illustrate two alternative ways to connect to Galileo:\n",
    "  * Using a customized run, which calculates the scores and logs into Galileo\n",
    "  * Using the langchain callback (currently unavailable due to compatibility issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bb40e-7823-490a-af94-0d8aae5e5886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import time\n",
    "import json\n",
    "import promptquality as pq\n",
    "\n",
    "def rouge_executor(row) -> float:\n",
    "    try:\n",
    "        print(f\"node_input: {row.node_input}\")\n",
    "        print(f\"node_output: {row.node_output}\")\n",
    "\n",
    "        # Try to decode node_input as JSON\n",
    "        try:\n",
    "            node_input = json.loads(row.node_input)\n",
    "            reference = node_input.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_input: {row.node_input}\")\n",
    "            return 0.0\n",
    "\n",
    "        # Try to decode node_output as JSON\n",
    "        try:\n",
    "            node_output = json.loads(row.node_output)\n",
    "            prediction = node_output.get(\"content\", \"\").strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in node_output: {row.node_output}\")\n",
    "            return 0.0\n",
    "\n",
    "        if not reference or not prediction:\n",
    "            print(\"'content' fields are empty in node_input or node_output\")\n",
    "            return 0.0\n",
    "\n",
    "        # Calculates ROUGE-L\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_values = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "        return rouge_values.get(\"rougeL\", 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in rouge_executor: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def rouge_aggregator(scores, indices) -> dict:\n",
    "    if len(scores) == 0:\n",
    "        return {'Average RougeL': 0.0}\n",
    "    else:\n",
    "        return {'Average RougeL': sum(scores) / len(scores)}\n",
    "\n",
    "# Define CustomScorer with corrected functions\n",
    "rouge_scorer = pq.CustomScorer(name='RougeL', executor=rouge_executor, aggregator=rouge_aggregator)\n",
    "\n",
    "# Invoke the chain to get the summaries\n",
    "print(\"Invoking the chain...\")\n",
    "response = chain.invoke(chunks)\n",
    "\n",
    "# Debugging to check the content of response\n",
    "print(f\"Response returned by the chain: {response}\")\n",
    "\n",
    "# Configures the assessment execution\n",
    "partitioned_run = pq.EvaluateRun(\n",
    "    project_name=\"AIStudio_template_code_summarization\",\n",
    "    run_name=\"Test4 partitioned script\",\n",
    "    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    ")\n",
    "\n",
    "# Measures execution time\n",
    "start_time = time.time()\n",
    "response = chain.invoke(chunks)\n",
    "total_time = int((time.time() - start_time) * 1000000)\n",
    "\n",
    "# Adiciona os dados ao workflow\n",
    "partitioned_run.add_workflow(input=chunks, output=response, duration_ns=total_time) \n",
    "partitioned_run.add_llm_step(input=chunks, output=response, duration_ns=total_time, model='local')\n",
    "\n",
    "# Finalizes the execution of the assessment\n",
    "partitioned_run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5bff8-fb62-433e-b1ff-985e610fe4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE IS NOT WORKING YET, AS GALILEO DOES NOT SUPPORT LISTS AS THE OUTPUT OF CHAIN NODES \n",
    "\n",
    "#summarization_callback =  pq.GalileoPromptCallback(\n",
    "#    project_name = \"AIStudio_summarization_template\",\n",
    "#    run_name = \"Partitioned transcript\",\n",
    "#    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\n",
    "#)\n",
    "\n",
    "#summaries = chain.invoke(chunks, config={\"callbacks\": [summarization_callback]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e043a-4d8a-47ba-83b2-d99c0d803575",
   "metadata": {},
   "source": [
    "## Galileo Protect\n",
    "\n",
    "Galileo Protect serves as a powerful tool for safeguarding AI model outputs by detecting and preventing the release of sensitive information like personal addresses or other PII. By integrating Galileo Protect into your AI pipelines, you can ensure that model responses comply with privacy and security guidelines in real-time.\n",
    "\n",
    "Galileo functions as an API that provides support for protection verification of your chain/LLM. To log into the Galileo console, it is necessary to integrate it with another service, such as Galileo Evaluate or Galileo Observe.\n",
    "\n",
    "**Attention**: an integrated API within the Galileo console is required to perform this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a39a4c-6a04-4510-9c10-4b281566d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import galileo_protect as gp\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "\n",
    "with open(\"secrets.yaml\", \"r\") as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "\n",
    "# 2. Configure environment variables with credentials\n",
    "os.environ[\"GALILEO_API_KEY\"] = secrets[\"Galileo\"]\n",
    "os.environ[\"GALILEO_CONSOLE_URL\"] = secrets.get(\"galileo_url\", \"https://console.hp.galileocloud.io/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23d0eb-806a-4a47-b051-f45ba2466801",
   "metadata": {},
   "source": [
    "Galileo Protect works by creating rules that identify conditions such as Personally Identifiable Information (PII) and toxicity. It ensures that the prompt will not receive or respond to sensitive questions. In this example, we create a set of rules (ruleset) and a set of actions that return a pre-programmed response if a rule is triggered. Galileo Protect also offers a variety of other metrics to suit different protection needs. You can learn more about the available metrics here: [Supported Metrics and Operators](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/supported-metrics-and-operators).\n",
    "\n",
    "Additionally, it is possible to import rulesets directly from Galileo through stages. Learn more about this feature here: [Invoking Rulesets](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/invoking-rulesets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23894954-93b0-4c06-9a2c-e3323fcaa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = gp.create_project(\"validate_protect\")\n",
    "print(f\"Project created. Project ID: {project.id}\")\n",
    "\n",
    "stage = gp.create_stage(name=\"validate_chain_stage\", project_id=project.id)\n",
    "print(f\"Stage created. ID do stage: {stage.id}\")\n",
    "\n",
    "# Define the Ruleset for PII Protection\n",
    "ruleset = Ruleset(\n",
    "    rules=[\n",
    "        {\n",
    "            \"metric\": \"pii\",\n",
    "            \"operator\": \"contains\",\n",
    "            \"target_value\": \"ssn\",\n",
    "        },\n",
    "    ],\n",
    "    action={\n",
    "        \"type\": \"OVERRIDE\",\n",
    "        \"choices\": [\"Personal Identifiable Information detected. Sorry, I cannot provide the response.\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize ProtectTool with the configured stage_id and ruleset\n",
    "protect_tool = ProtectTool(stage_id=stage.id, prioritized_rulesets=[ruleset], timeout=10)\n",
    "\n",
    "# Use existing chain and combine with ProtectTool\n",
    "protect_parser = ProtectParser(chain=chain)  # 'chain' has already been defined previously\n",
    "protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "# Example of using the protected chain with input and output\n",
    "input_data = {\n",
    "    \"input\": \"John Doe's social security number is 123-45-6789.\",\n",
    "    \"output\": \"John Doe's social security number is 123-45-6789.\"\n",
    "}\n",
    "\n",
    "# Invoke the protected chain\n",
    "print(\"Invoking the chain with PII protection...\")\n",
    "response = protected_chain.invoke(input_data)\n",
    "print(\"Protected chain response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563e41c-c1a9-423e-a11f-5e7c6ac53770",
   "metadata": {},
   "source": [
    "## Galileo Observe\n",
    "\n",
    "Galileo Observe helps you monitor your generative AI applications in production. With Observe you will understand how your users are using your application and identify where things are going wrong. Keep tabs on your production system, instantly receive alerts when bad things happen, and perform deep root cause analysis though the Observe dashboard.\n",
    "\n",
    "You can connect Galileo Observe to your Langchain chain to monitor metrics such as cost and guardrail indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cbf0e-6c9d-4120-b9e3-17b376b3b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galileo_observe import GalileoObserveCallback\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "example_query = \"\"\"Tell me a story about technology and innovation. \n",
    "Explain how artificial intelligence is shaping the future. \n",
    "Summarize the impact of renewable energy on society.\"\"\"\n",
    "\n",
    "result_break = lambda_break.invoke(example_query)\n",
    "\n",
    "\n",
    "chain = lambda_break | {\n",
    "    f\"summary_{i}\": itemgetter(i) | prompt | llm\n",
    "    for i in range(len(result_break))\n",
    "} | lambda_join | StrOutputParser()\n",
    "\n",
    "monitor_handler = GalileoObserveCallback(project_name=\"validate_galileo_observe\")\n",
    "\n",
    "print(\"Invoking the chain with Galileo Observe...\")\n",
    "try:\n",
    "    output = chain.invoke(\n",
    "        example_query,\n",
    "        config={\"callbacks\": [monitor_handler]}\n",
    "    )\n",
    "    print(\"Observed chain output:\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"Error during chain execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910bddc-a194-4011-b82d-3a8cd6ea762d",
   "metadata": {},
   "source": [
    "## Model service Galileo Protect + Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6721be-3298-4dfc-91dd-4b2bf27007d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.pyfunc import PythonModel, ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from galileo_observe import GalileoObserveCallback\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "import galileo_protect as gp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "class SemanticSplitter:\n",
    "    def __init__(self, content, embedding_model, method=\"number\", partition_count=10, quantile=0.9):\n",
    "        print(\"Starting SemanticSplitter...\")\n",
    "        self.content = [line.strip() for line in content if line.strip()]  \n",
    "        self.embedding_model = embedding_model\n",
    "        self.partition_count = partition_count\n",
    "        self.quantile = quantile\n",
    "\n",
    "        print(\"Calculating embeddings for content...\")\n",
    "        self.embeddings = embedding_model.encode(self.content)\n",
    "        self.distances = [cosine(self.embeddings[i - 1], self.embeddings[i]) for i in range(1, len(self.embeddings))]\n",
    "        self.breaks = []\n",
    "        self.load_breaks(method=method)\n",
    "        print(f\"Loaded breaks: {self.breaks}\")\n",
    "\n",
    "    def load_breaks(self, method='number'):\n",
    "        print(f\"Loading breaks using the method: {method}\")\n",
    "        if len(self.distances) == 0:\n",
    "            print(\"No distances calculated. No breaks loaded.\")\n",
    "            return\n",
    "\n",
    "        if method == 'number':\n",
    "            if self.partition_count > len(self.distances):\n",
    "                self.partition_count = len(self.distances)\n",
    "            self.breaks = np.sort(np.argpartition(self.distances, self.partition_count - 1)[:self.partition_count - 1])\n",
    "        elif method == 'quantiles':\n",
    "            threshold = np.quantile(self.distances, self.quantile)\n",
    "            self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\n",
    "\n",
    "    def get_chunk(self, beginning, end):\n",
    "        return '\\n'.join(self.content[beginning:end]).strip()\n",
    "\n",
    "    def get_chunks(self):\n",
    "        print(\"Dividing text into chunks...\")\n",
    "        if len(self.breaks) == 0:\n",
    "            return [self.get_chunk(0, len(self.content))]\n",
    "\n",
    "        chunks = []\n",
    "        beginning = 0\n",
    "        for break_position in self.breaks:\n",
    "            chunk = self.get_chunk(beginning, break_position + 1)\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            beginning = break_position + 1\n",
    "\n",
    "        chunk = self.get_chunk(beginning, len(self.content))\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        print(f\"Chunks gerados: {chunks}\")\n",
    "        return chunks\n",
    "\n",
    "class TextSummarizationService(PythonModel):\n",
    "    def load_context(self, context):\n",
    "        print(\"Loading model context.\")\n",
    "\n",
    "        \n",
    "        secrets_path = context.artifacts[\"secrets\"]\n",
    "        with open(secrets_path, \"r\") as file:\n",
    "            secrets = yaml.safe_load(file)\n",
    "        \n",
    "        os.environ[\"GALILEO_API_KEY\"] = secrets.get(\"Galileo\", \"\")\n",
    "        os.environ[\"GALILEO_CONSOLE_URL\"] = secrets.get(\"galileo_url\", \"https://console.hp.galileocloud.io/\")\n",
    "        \n",
    "        model_source = secrets.get(\"source\", \"local_molder\") #Use 'HuggingFace', or 'local_model'.\n",
    "\n",
    "        if model_source == \"HuggingFace\":\n",
    "            huggingface_key = secrets.get(\"HuggingFace\", \"\")\n",
    "            hf_model_repo = secrets.get(\"hf_model_repo\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "            self.llm = HuggingFaceEndpoint(huggingfacehub_api_token=huggingface_key, repo_id=hf_model_repo)\n",
    "            print(\"Using the HuggingFace model.\")\n",
    "        elif model_config[\"source\"] == \"local_model\":\n",
    "            print(\"[INFO] Initializing local LlamaCpp model.\")\n",
    "            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "            self.llm = LlamaCpp(\n",
    "                model_path = context.artifacts[\"model_folder\"],\n",
    "                n_gpu_layers=30,\n",
    "                n_batch=512,\n",
    "                n_ctx=4096,\n",
    "                max_tokens=1024,\n",
    "                f16_kv=True,\n",
    "                callback_manager=callback_manager,\n",
    "                verbose=False,\n",
    "                stop=[],\n",
    "                streaming=False,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            print(\"Using the local LlamaCpp model.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model source. Use  'HuggingFace', or 'local_model'.\")\n",
    "\n",
    "        \n",
    "        self.observe_callback = GalileoObserveCallback(project_name=\"summarization_service_observe\")\n",
    "\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        The following text is an excerpt of a transcription:\n",
    "\n",
    "        ###\n",
    "        {context}\n",
    "        ###\n",
    "\n",
    "        Please, produce a single paragraph summarizing the given excerpt.\n",
    "        \"\"\")\n",
    "\n",
    "        project = gp.create_project(\"summarization_service_protect\")\n",
    "        print(f\"Project created. Project ID: {project.id}\")\n",
    "\n",
    "        stage = gp.create_stage(name=\"summarization_stage\", project_id=project.id)\n",
    "        print(f\"Stage created. Stage ID: {stage.id}\")\n",
    "\n",
    "        ruleset = Ruleset(\n",
    "            rules=[\n",
    "                {\n",
    "                    \"metric\": \"pii\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"target_value\": \"ssn\",\n",
    "                },\n",
    "            ],\n",
    "            action={\n",
    "                \"type\": \"OVERRIDE\",\n",
    "                \"choices\": [\"Personal Identifiable Information detected. Sorry, I cannot provide the summary.\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        protect_tool = ProtectTool(stage_id=stage.id, prioritized_rulesets=[ruleset], timeout=10)\n",
    "        self.protect_parser = ProtectParser(chain=self.prompt | self.llm | StrOutputParser())\n",
    "        self.protected_chain = protect_tool | self.protect_parser.parser\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        text = model_input[\"text\"].iloc[0]\n",
    "        print(f\"Input text:\\n{text[:200]}...\")  \n",
    "\n",
    "        splitter = SemanticSplitter(text.split(\"\\n\"), self.embedding_model, method=\"number\", partition_count=6)\n",
    "        chunks = splitter.get_chunks()\n",
    "\n",
    "        print(f\"Number of chunks generated:{len(chunks)}\")\n",
    "\n",
    "        combined_input = \"\\n\\n\".join(chunks)\n",
    "        print(f\"Combined text for batch:\\n{combined_input[:500]}...\")  # Show first 500 characters\n",
    "\n",
    "        print(\"Sending all chunks in batch to the chain...\")\n",
    "        try:\n",
    "            result = self.protected_chain.invoke(\n",
    "                {\"input\": combined_input, \"output\": \"\"},\n",
    "                config={\"callbacks\": [self.observe_callback]}\n",
    "            )\n",
    "            print(\"Batch result processed successfully.\")\n",
    "        except Exception as e:\n",
    "            result = f\"Error processing batch:{e}\"\n",
    "            print(result)\n",
    "\n",
    "        print(f\"Processing completed. Total chunks processed: {len(chunks)}\")\n",
    "\n",
    "        return pd.DataFrame([{\"summary\": result}])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing experiment in MLflow.\")\n",
    "\n",
    "    mlflow.set_experiment(\"Text_Summarization_Service\")\n",
    "\n",
    "    secrets_path = \"secrets.yaml\"\n",
    "    model_path = \"/home/jovyan/datafabric/Llama_7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "\n",
    "    if not os.path.exists(secrets_path):\n",
    "        raise FileNotFoundError(f\"secrets.yaml file not found in path: {os.path.abspath(secrets_path)}\")\n",
    "\n",
    "    input_schema = Schema([ColSpec(\"string\", \"text\")])\n",
    "    output_schema = Schema([ColSpec(\"string\", \"summary\")])\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Text_Summarization_Service_Run\") as run:\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"text_summarization_service\",\n",
    "            python_model=TextSummarizationService(),\n",
    "            artifacts={\"secrets\": secrets_path, \"model_folder\": model_path},\n",
    "            signature=signature,\n",
    "            pip_requirements=[\n",
    "                \"galileo-protect==0.15.1\",\n",
    "                \"galileo-observe==1.13.2\",\n",
    "                \"pyyaml\",\n",
    "                \"pandas\",\n",
    "                \"sentence-transformers\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model_uri = f\"runs:/{run.info.run_id}/text_summarization_service\"\n",
    "        mlflow.register_model(model_uri=model_uri, name=\"Text_Summarization_Service_Model\")\n",
    "\n",
    "        print(f\"Registered model with execution ID: {run.info.run_id}\")\n",
    "        print(f\"Model registered successfully. Run ID: {run.info.run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
