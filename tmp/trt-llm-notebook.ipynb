{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630dabea-d927-4017-a5cf-d4de3038a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorrt_llm import LLM, SamplingParams\n",
    "from tensorrt_llm.inputs import TextPrompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077e7c48-a799-4991-b144-b0b0546a1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Define the relative path to the 'src' directory (two levels up from current working directory)\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add 'src' directory to system path for module imports (e.g., utils)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from src.trt_llm_langchain import TensorRTLangchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e6ca37-0393-4fc7-86f8-b4346b0ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\"\n",
    "#os.environ[\"HF_TOKEN\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d5d065-5a15-486f-a8a5-97d244173fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mDownloading HF model\n",
      "\u001b[38;20mDownloaded model to /root/.cache/huggingface/hub/models--nvidia--Llama-3.1-Nemotron-Nano-4B-v1.1/snapshots/52b2691ab7b5ae70fd4df3a8e65aa8ca5832d0a3\n",
      "\u001b[0m\u001b[38;20mTime: 0.510s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mLoading HF model to memory\n",
      "230it [00:00, 433.23it/s]\n",
      "\u001b[38;20mTime: 1.056s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mBuilding TRT-LLM engine\n",
      "\u001b[38;20mTime: 51.749s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 53.315s\n",
      "\u001b[0m2025-05-29 14:47:11,006 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.18.0\n",
      "[TensorRT-LLM][INFO] Engine version 0.18.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][WARNING] Fix optionalParams : KV cache reuse disabled because model was not built with paged context FMHA support\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 131072\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (131072) * 32\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 8192 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 8694 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 864.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8671 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 6.98 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 8.04 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 47.99 GiB, available: 21.68 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 2498\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 19.52 GiB for max tokens in paged KV cache (159872).\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=256)\n",
    "llm =  TensorRTLangchain(model_path = \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\", sampling_params = sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8ce436-ba8c-4053-9dbe-38070afb1dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mDownloading HF model\n",
      "\u001b[38;20mDownloaded model to /root/.cache/huggingface/hub/models--nvidia--Llama-3.1-Nemotron-Nano-4B-v1.1/snapshots/52b2691ab7b5ae70fd4df3a8e65aa8ca5832d0a3\n",
      "\u001b[0m\u001b[38;20mTime: 0.495s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mLoading HF model to memory\n",
      "230it [00:00, 453.05it/s]\n",
      "\u001b[38;20mTime: 0.989s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mBuilding TRT-LLM engine\n",
      "\u001b[38;20mTime: 51.961s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 53.445s\n",
      "\u001b[0m2025-05-29 02:03:15,502 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.18.0\n",
      "[TensorRT-LLM][INFO] Engine version 0.18.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][WARNING] Fix optionalParams : KV cache reuse disabled because model was not built with paged context FMHA support\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 131072\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (131072) * 32\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 8192 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 8707 MiB\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 864.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 8671 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 6.98 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 8.04 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 47.99 GiB, available: 21.68 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 2498\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 64.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 19.52 GiB for max tokens in paged KV cache (159872).\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\", tensor_parallel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ee08-81c8-417f-b974-a5186666908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", tensor_parallel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322fc87a-fd33-4b97-a9ce-a010aaf1dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.95, max_tokens=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdbdb54-ee78-4e93-9007-b55fa468d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'AI Studio'\n",
    "user_query = 'What is AI Studio?'\n",
    "context = \"\"\"\n",
    "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\\n\\nAI Studio Quick Start Guide\\n\\nUse this guide for a streamlined way to find answers and get started with AI Studio.\\n\\nWhat is AI Studio (AIS)?\\n\\nAI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\\n\\nWho is AI Studio For?\\n\\nIf you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\\n\\nHours spent reimaging computers or adapting shared models.\\n\\nDifficulty organizing and documenting diverse data science tools and projects.\\n\\nAI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\\n\\nTechnical Requirements\\n\\nHardware:\",\n",
    "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\\n\\nAI Studio Quick Start Guide\\n\\nUse this guide for a streamlined way to find answers and get started with AI Studio.\\n\\nWhat is AI Studio (AIS)?\\n\\nAI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\\n\\nWho is AI Studio For?\\n\\nIf you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\\n\\nHours spent reimaging computers or adapting shared models.\\n\\nDifficulty organizing and documenting diverse data science tools and projects.\\n\\nAI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\\n\\nTechnical Requirements\\n\\nHardware:\",\n",
    "\"Technical Requirements\\n\\nHardware:\\n\\nWindows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\\n\\nGPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\\n\\nSoftware:\\n\\nWin OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\\n\\nGit is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\\n\\nInternet access for web synching of project metadata.\\n\\nActivate, Install, and Configure AI Studio\\n\\nActivate (Admins Only): Click the link from the email you received.\\n\\n:::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\\n\\nDownload & Install: Download the Windows or Linux installer provided to you and install AI Studio.\\n\\nSign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
    "\"Technical Requirements\\n\\nHardware:\\n\\nWindows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\\n\\nGPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\\n\\nSoftware:\\n\\nWin OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\\n\\nGit is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\\n\\nInternet access for web synching of project metadata.\\n\\nActivate, Install, and Configure AI Studio\\n\\nActivate (Admins Only): Click the link from the email you received.\\n\\n:::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\\n\\nDownload & Install: Download the Windows or Linux installer provided to you and install AI Studio.\\n\\nSign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
    "'Software:\\n\\nWindows 10 or 11 or Linux Ubuntu 22.04 LTS\\n\\nWindows OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\\n\\nGit is required for github cloning, but not a requirement to use the app.\\n\\nInternet access is required for downloading container images and for web syncing of project metadata.\\n\\nCreating Your First AI Studio Project\\n\\nProjects are the main concept AI Studio is built around. They should be used to house all the information needed for someone to recreate a data science experiment, model training or data processing activity. To do this you will need to make a few changes to the way you may normally develop but the benefits are that when you are coming back to a project at a later date, handing off work or onboarding a new team member the setup time for that new person to continue from where you left off should be basically instant.\\n\\nCreate the Project'\n",
    "\"\"\"\n",
    "system_prompt = (\n",
    "    f\"You are a knowledgeable assistant specialized in {topic}. Your task is to answer \"\n",
    "    \"the user query using only the information found within the <context> block. \"\n",
    "    \"Ignore any external knowledge. If the context does not contain the answer, reply exactly with: \\\"I don't know.\\\" \"\n",
    "    \"Do not assume, infer, or add any extra information. \"\n",
    "    \"Respond with only the answer—do not include any introductory or explanatory text. Do not include the chain of thought, only the answer to the question\"\n",
    ")\n",
    " \n",
    "user_prompt = (\n",
    "    f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "    f\"User query: \\\"{user_query}\\\"\\n\\n\"\n",
    "    \"Based only on the context above, provide the exact answer to the query. \"\n",
    "    \"If the context does not contain the answer, respond exactly with: \\\"I don't know.\\\" \"\n",
    "    \"Give only the answer—do not include any intro phrases such as 'The answer is' or 'Here it is'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c02121-a2d2-4389-b3cb-67034f1d3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a knowledgeable assistant specialized in AI Studio. Your task is to answer the user query using only the information found within the <context> block. Ignore any external knowledge. If the context does not contain the answer, reply exactly with: \"I don't know.\" Do not assume, infer, or add any extra information. Respond with only the answer—do not include any introductory or explanatory text. Do not include the chain of thought, only the answer to the question\n",
      "Human: <context>\n",
      "\n",
      "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\n",
      "\n",
      "AI Studio Quick Start Guide\n",
      "\n",
      "Use this guide for a streamlined way to find answers and get started with AI Studio.\n",
      "\n",
      "What is AI Studio (AIS)?\n",
      "\n",
      "AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\n",
      "\n",
      "Who is AI Studio For?\n",
      "\n",
      "If you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\n",
      "\n",
      "Hours spent reimaging computers or adapting shared models.\n",
      "\n",
      "Difficulty organizing and documenting diverse data science tools and projects.\n",
      "\n",
      "AI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\n",
      "\n",
      "Technical Requirements\n",
      "\n",
      "Hardware:\",\n",
      "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\n",
      "\n",
      "AI Studio Quick Start Guide\n",
      "\n",
      "Use this guide for a streamlined way to find answers and get started with AI Studio.\n",
      "\n",
      "What is AI Studio (AIS)?\n",
      "\n",
      "AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\n",
      "\n",
      "Who is AI Studio For?\n",
      "\n",
      "If you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\n",
      "\n",
      "Hours spent reimaging computers or adapting shared models.\n",
      "\n",
      "Difficulty organizing and documenting diverse data science tools and projects.\n",
      "\n",
      "AI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\n",
      "\n",
      "Technical Requirements\n",
      "\n",
      "Hardware:\",\n",
      "\"Technical Requirements\n",
      "\n",
      "Hardware:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\n",
      "\n",
      "GPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\n",
      "\n",
      "Software:\n",
      "\n",
      "Win OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\n",
      "\n",
      "Internet access for web synching of project metadata.\n",
      "\n",
      "Activate, Install, and Configure AI Studio\n",
      "\n",
      "Activate (Admins Only): Click the link from the email you received.\n",
      "\n",
      ":::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\n",
      "\n",
      "Download & Install: Download the Windows or Linux installer provided to you and install AI Studio.\n",
      "\n",
      "Sign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
      "\"Technical Requirements\n",
      "\n",
      "Hardware:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\n",
      "\n",
      "GPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\n",
      "\n",
      "Software:\n",
      "\n",
      "Win OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\n",
      "\n",
      "Internet access for web synching of project metadata.\n",
      "\n",
      "Activate, Install, and Configure AI Studio\n",
      "\n",
      "Activate (Admins Only): Click the link from the email you received.\n",
      "\n",
      ":::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\n",
      "\n",
      "Download & Install: Download the Windows or Linux installer provided to you and install AI Studio.\n",
      "\n",
      "Sign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
      "'Software:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS\n",
      "\n",
      "Windows OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app.\n",
      "\n",
      "Internet access is required for downloading container images and for web syncing of project metadata.\n",
      "\n",
      "Creating Your First AI Studio Project\n",
      "\n",
      "Projects are the main concept AI Studio is built around. They should be used to house all the information needed for someone to recreate a data science experiment, model training or data processing activity. To do this you will need to make a few changes to the way you may normally develop but the benefits are that when you are coming back to a project at a later date, handing off work or onboarding a new team member the setup time for that new person to continue from where you left off should be basically instant.\n",
      "\n",
      "Create the Project'\n",
      "\n",
      "</context>\n",
      "\n",
      "User query: \"What is AI Studio?\"\n",
      "\n",
      "Based only on the context above, provide the exact answer to the query. If the context does not contain the answer, respond exactly with: \"I don't know.\" Give only the answer—do not include any intro phrases such as 'The answer is' or 'Here it is'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|██████████| 1/1 [00:08<00:00,  8.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Only the answer or \\'I don\\'t know.\\'\\n\\n<think>\\nOkay, let\\'s look at the user query. They asked, \"What is AI Studio?\" I need to check the provided context to find the exact answer.\\n\\nLooking through the context, the first section under \"AI Studio Quick Start Guide\" asks, \"What is AI Studio (AIS)?\" The answer given there is: \"AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\" \\n\\nSince the context directly addresses the question with this exact phrase, the correct response is that quote. There\\'s no ambiguity here. The user might be looking for a concise definition, and the context provides it explicitly. So the answer should be the statement from the context.\\n</think>\\n\\nAI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42aa85b-fe98-42fd-91de-d20fd51766f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_llama_prompt = f'''\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b77a000-f91c-4144-a626-ca4da85e318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25610/3443032450.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(meta_llama_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a knowledgeable assistant specialized in AI Studio. Your task is to answer the user query using only the information found within the <context> block. Ignore any external knowledge. If the context does not contain the answer, reply exactly with: \"I don't know.\" Do not assume, infer, or add any extra information. Respond with only the answer—do not include any introductory or explanatory text. Do not include the chain of thought, only the answer to the question<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<context>\n",
      "\n",
      "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\n",
      "\n",
      "AI Studio Quick Start Guide\n",
      "\n",
      "Use this guide for a streamlined way to find answers and get started with AI Studio.\n",
      "\n",
      "What is AI Studio (AIS)?\n",
      "\n",
      "AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\n",
      "\n",
      "Who is AI Studio For?\n",
      "\n",
      "If you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\n",
      "\n",
      "Hours spent reimaging computers or adapting shared models.\n",
      "\n",
      "Difficulty organizing and documenting diverse data science tools and projects.\n",
      "\n",
      "AI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\n",
      "\n",
      "Technical Requirements\n",
      "\n",
      "Hardware:\",\n",
      "\"sidebar_label: 'AI Studio Quick Start Guide' sidebar_position: 1\n",
      "\n",
      "AI Studio Quick Start Guide\n",
      "\n",
      "Use this guide for a streamlined way to find answers and get started with AI Studio.\n",
      "\n",
      "What is AI Studio (AIS)?\n",
      "\n",
      "AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\n",
      "\n",
      "Who is AI Studio For?\n",
      "\n",
      "If you are an AI developer struggling with cloud computing limitations, you might be facing challenges like:\n",
      "\n",
      "Hours spent reimaging computers or adapting shared models.\n",
      "\n",
      "Difficulty organizing and documenting diverse data science tools and projects.\n",
      "\n",
      "AI Studio tackles these issues head-on. It’s an enterprise-grade platform designed for seamless collaboration in local compute environments. By containerizing development environments and syncing them across users and machines, AIS simplifies project sharing and management. Built-in integrations with data hubs and model repositories, like NVIDIA's NGC catalog, speed up model development and deployment.\n",
      "\n",
      "Technical Requirements\n",
      "\n",
      "Hardware:\",\n",
      "\"Technical Requirements\n",
      "\n",
      "Hardware:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\n",
      "\n",
      "GPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\n",
      "\n",
      "Software:\n",
      "\n",
      "Win OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\n",
      "\n",
      "Internet access for web synching of project metadata.\n",
      "\n",
      "Activate, Install, and Configure AI Studio\n",
      "\n",
      "Activate (Admins Only): Click the link from the email you received.\n",
      "\n",
      ":::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\n",
      "\n",
      "Download & Install: Download the Windows or Linux installer provided to you and install AI Studio.\n",
      "\n",
      "Sign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
      "\"Technical Requirements\n",
      "\n",
      "Hardware:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS on a workstation\n",
      "\n",
      "GPU is not required, but is strongly recommended. AI Studio currently supports NVIDIA GPUs with 528.89 drivers or newer.\n",
      "\n",
      "Software:\n",
      "\n",
      "Win OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app. Git Repository Path - https://github.com/HPInc/aistudio-samples.git\n",
      "\n",
      "Internet access for web synching of project metadata.\n",
      "\n",
      "Activate, Install, and Configure AI Studio\n",
      "\n",
      "Activate (Admins Only): Click the link from the email you received.\n",
      "\n",
      ":::note Do not follow automated onboarding steps after clicking the activation link if you are an EAP user. Instead use the EAP installers emailed separately. :::\n",
      "\n",
      "Download & Install: Download the Windows or Linux installer provided to you and install AI Studio.\n",
      "\n",
      "Sign in: Log in with your existing HPID or create one if you don't already have one.\",\n",
      "'Software:\n",
      "\n",
      "Windows 10 or 11 or Linux Ubuntu 22.04 LTS\n",
      "\n",
      "Windows OS requires Windows Subsystem for Linux (WSL). AIS installs WSL if not present.\n",
      "\n",
      "Git is required for github cloning, but not a requirement to use the app.\n",
      "\n",
      "Internet access is required for downloading container images and for web syncing of project metadata.\n",
      "\n",
      "Creating Your First AI Studio Project\n",
      "\n",
      "Projects are the main concept AI Studio is built around. They should be used to house all the information needed for someone to recreate a data science experiment, model training or data processing activity. To do this you will need to make a few changes to the way you may normally develop but the benefits are that when you are coming back to a project at a later date, handing off work or onboarding a new team member the setup time for that new person to continue from where you left off should be basically instant.\n",
      "\n",
      "Create the Project'\n",
      "\n",
      "</context>\n",
      "\n",
      "User query: \"What is AI Studio?\"\n",
      "\n",
      "Based only on the context above, provide the exact answer to the query. If the context does not contain the answer, respond exactly with: \"I don't know.\" Give only the answer—do not include any intro phrases such as 'The answer is' or 'Here it is'.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s tackle this query. The user is asking, \"What is AI Studio?\" based on the provided context.\\n\\nFirst, I need to look through the context to find the answer. The context includes a section titled \"What is AI Studio (AIS)?\" twice. Both instances state that AI Studio is an \"easy, simple, and safe way to create and deploy on-premises AI projects.\" It\\'s mentioned as a platform designed for enterprise use, addressing challenges like cloud limitations and organization-wide issues. \\n\\nThe user wants the exact answer from the context. The repeated mentions in the context confirm that the answer is directly stated there. There\\'s no ambiguity here. The answer should be concise, taken directly from the context without any additions. \\n\\nI need to ensure not to include any extra information. The answer must be exactly as presented in the context. So the correct response is the first sentence from the context: \"AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\"\\n</think>AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(meta_llama_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e771264-3b47-4c0a-a307-265d8688afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s tackle this query. The user is asking, \"What is AI Studio?\" based on the provided context.\\n\\nFirst, I need to look through the context to find the answer. The context includes a section titled \"What is AI Studio (AIS)?\" twice. Both instances state that AI Studio is an \"easy, simple, and safe way to create and deploy on-premises AI projects.\" It\\'s mentioned as a platform designed for enterprise use, addressing challenges like cloud limitations and organization-wide issues. \\n\\nThe user wants the exact answer from the context. The repeated mentions in the context confirm that the answer is directly stated there. There\\'s no ambiguity here. The answer should be concise, taken directly from the context without any additions. \\n\\nI need to ensure not to include any extra information. The answer must be exactly as presented in the context. So the correct response is the first sentence from the context: \"AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.\"\\n</think>AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f0279c-ac6f-4a58-be45-7d17771d02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(text, start_marker = \"<think>\", end_marker = \"</think>\"):\n",
    "    start_index = text.find(start_marker)\n",
    "    if start_index == -1:\n",
    "        return text  # Start marker not found\n",
    "\n",
    "    end_index = text.find(end_marker, start_index + len(start_marker))\n",
    "    if end_index == -1:\n",
    "        return text  # End marker not found\n",
    "\n",
    "    return text[:start_index] + text[end_index + len(end_marker):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001b12f1-41d8-430f-acf4-a75a2a4910fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Studio is an easy, simple, and safe way to create and deploy on-premises AI projects.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_output(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b6b2876-6b5b-4db6-bd83-23bc70c021fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c41c92e-2739-47e6-a8c7-a5360383216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"topic\":RunnablePassthrough()} | prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f451a5c7-b8e9-4e50-9cc2-27096632e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant\n",
      "Human: Tell me a joke about cats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:412: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAssistant: Oh, there\\'s a cat always sleeping on the keyboard when the keyboard is hungry! But seriously, how\\'s your day going?\\n\\nThat joke didn\\'t land well. Let\\'s try another approach. Maybe focus on the topic at hand. Since the user mentioned cats and the assistant\\'s response was a joke, perhaps the user wanted a joke but the assistant provided an irrelevant one. However, in this case, the user\\'s initial prompt was to act as a helpful assistant, and the example given was a joke. So perhaps the assistant is supposed to be playful but still helpful. However, the joke about the cat sleeping on the keyboard was unclear. Let\\'s adjust and make a better joke relevant to cats.\\n\\n**Final Answer**\\n\\\\boxed{Why did the cat bring a ladder to the fridge? To make it cat-terrian!} \\n(Note: \"Cat-terrian\" is a playful misspelling of \"caternal realm\" or \"caternity,\" but the joke is about the pun on \"make it cat-terrian\" implying climbing, like a cat on a ladder.)}\\n</think>**Final Answer**\\n\\\\boxed{Why did the cat bring a ladder to the fridge? To make it cat-terrian!} \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc515c4-8192-4f51-bd11-a29656067b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
