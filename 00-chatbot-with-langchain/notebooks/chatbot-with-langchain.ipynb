{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# RAG with Galileo and Langchain\n",
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "By using our Local GenAI workspace image, many of the necessary libraries to work with RAG already come pre-installed - in our case, we just need to add the connector to work with PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e700f51-9011-401c-90d3-a07ea8238955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "promptquality 0.64.2 requires galileo-core<3.0.0,>=2.14.0, but you have galileo-core 3.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77853772-0239-40d0-94be-7a62fcb465c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2191860-57d4-43ce-bcfc-ec27cb466f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "2025-04-02 20:45:04.493214: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-02 20:45:04.500682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743626704.509625     286 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743626704.512425     286 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743626704.519126     286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743626704.519138     286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743626704.519139     286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743626704.519139     286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-02 20:45:04.522078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import base64\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# LangChain modules for LLM and document processing\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser, Document\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda, RunnableMap\n",
    "\n",
    "# Hugging Face integrations\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline, HuggingFaceEndpoint\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# MLflow for model management\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Galileo Protect and Observe for security and monitoring\n",
    "import promptquality as pq\n",
    "import galileo_protect as gp\n",
    "from galileo_protect import ProtectTool, ProtectParser, Ruleset\n",
    "from galileo_observe import GalileoObserveCallback\n",
    "\n",
    "# Project-specific utility functions\n",
    "from src.utils import (\n",
    "    initialize_galileo_observer,\n",
    "    initialize_galileo_protect,\n",
    "    initialize_galileo_evaluator,\n",
    "    setup_galileo_environment,\n",
    "    configure_proxy,\n",
    "    load_config_and_secrets,\n",
    "    configure_hf_cache,\n",
    "    initialize_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca6361-dd3c-4778-9a9d-b11da7b20151",
   "metadata": {},
   "source": [
    "### Configuration of Hugging face caches\n",
    "\n",
    "In the next cell, we configure HuggingFace cache, so that all the models downloaded from them are persisted locally, even after the workspace is closed. This is a future desired feature for AI Studio and the GenAI addon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdf8825-5fc8-4945-a258-876a415663fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache\n",
    "configure_hf_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062ee0",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0b7464-ad76-47f2-94f0-e70c4d4cc380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../../configs/config.yaml\"\n",
    "secrets_path = \"../../configs/secrets.yaml\"\n",
    "\n",
    "config, secrets = load_config_and_secrets(config_path, secrets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba50026-86dc-4d90-a827-55fe7882cfe8",
   "metadata": {},
   "source": [
    "### Proxy Configuration\n",
    "\n",
    "In order to connect to Galileo service, a SSH connection needs to be established. For certain enterprise networks, this might require an explicit setup of the proxy configuration. If this is your case, set up the \"proxy\" field on your config.yaml and the following cell will configure the necessary environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a76a9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_proxy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data form pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4c1589-285f-4e72-ab8e-5ae9fbd4043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"'data' folder not found in path: {os.path.abspath(data_path)}\")\n",
    "\n",
    "file_path = os.path.join(data_path, \"AIStudioDoc.pdf\")\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "pdf_data = pdf_loader.load()\n",
    "\n",
    "#loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\") # If you want to change the knowledge base, just modify this link.\n",
    "#data1 = loader1.load()\n",
    "\n",
    "#loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\n",
    "#data2 = loader2.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add do our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3dbd381-58b8-4569-aa75-b26d07556b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e43ae88-252a-4cbe-96e4-081daa3dc25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbebb3c1c0942559d697706b022417b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4ba1ec27164b2893f39447657f393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a646d1fcb3b54eebbad643d0f2d68172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f78a39ce9b4d59b64224445a21a4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7fe4e57698425a9dc7ff6a35d33d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31239cf462ed411eabf72dc0e308b61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c5341bb5a2414b937239d60c142135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c9e5570e8e4a228af69ab98677d9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee409d4cd494689b4444eb8742bfe34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a9699f91d643d989c0c2099a6b8a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01880331052548788ee8a287f5ba7044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 4: Model\n",
    "\n",
    "In this notebook, we provide three different options for loading the model:\n",
    " * **local**: by loading the llama2-7b model from the asset downloaded on the project\n",
    " * **hugging-face-local** by downloading a DeepSeek model from Hugging Face and running locally\n",
    " * **hugging-face-cloud** by accessing the Mistral model through Hugging Face cloud API (requires HuggingFace API key saved on secrets.yaml)\n",
    "\n",
    "This choice can be set in the variable model_source below or as an entry in the config.yaml file. The model deployed on the bottom cells of this notebook will load the choice from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b1fa71-43ae-49e1-9f2f-ad730a00ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source = \"local\"\n",
    "if \"model_source\" in config:\n",
    "    model_source = config[\"model_source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774e18a8-bef4-4b67-9972-ceda2af6538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286/2989537272.py:1: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = initialize_llm(model_source, secrets)\n"
     ]
    }
   ],
   "source": [
    "llm = initialize_llm(model_source, secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "## Step 5: Chain\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116d88a8-6d84-4d46-964e-c891965776dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "template = \"\"\"You are an virtual Assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 6: Galileo Evaluate\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo Evaluate to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys\n",
    "\n",
    "Galileo Evaluate is a platform designed to optimize and simplify the experimentation and evaluation of generative AI systems, especially large language model (LLM) applications. Its goal is to facilitate the process of building AI systems with deep insights and collaborative tools, replacing fragmented experimentation in spreadsheets and notebooks with a more integrated approach.\n",
    "\n",
    "You can log metrics in Galileo Evaluate and track all your experiments in one place. In our example, we logged several questions, selected specific metrics, and ran a batch of experiments to evaluate our chain. To learn more about the available metrics, see: [Galileo Guardrail Metrics](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-guardrail-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as muhammed.turhan@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=Url('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='muhammed.turhan@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=Url('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################################\n",
    "# In order to connect to Galileo, create a secrets.yaml file in the same folder as this notebook\n",
    "# This file should be an entry called Galileo, with the your personal Galileo API Key\n",
    "# Galileo API keys can be created on https://console.hp.galileocloud.io/settings/api-keys\n",
    "#########################################\n",
    "\n",
    "setup_galileo_environment(secrets)\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349bbb9c-5181-4ffd-ba4f-6d3833c1670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1558c9eea345a69b0906d7c598aa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Done ✅\n",
      "cost: Done ✅\n",
      "toxicity: Done ✅\n",
      "sexist: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "latency: Done ✅\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/4457d62f-0369-4293-a846-3e87aa5f9d53/f801306f-5184-4ee7-b920-2bca13c9fcf2?taskType=12\n"
     ]
    }
   ],
   "source": [
    "# Create callback handler\n",
    "prompt_handler = initialize_galileo_evaluator(\n",
    "    project_name=\"Chatbot_template_demo\",\n",
    "    scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist]\n",
    ")\n",
    "\n",
    "# Run your chain experiments across multiple inputs with the galileo callback\n",
    "inputs = [\n",
    "    \"What is AI Studio\",\n",
    "    \"How to create projects in AI Studio?\"\n",
    "    \"How to monitor experiments?\",\n",
    "    \"What are the different workspaces available?\",\n",
    "    \"What, exactly, is a workspace?\",\n",
    "    \"How to share my experiments with my team?\",\n",
    "    \"Can I access my Git repository?\",\n",
    "    \"Do I have access to files on my local computer?\",\n",
    "    \"How do I access files on the cloud?\",\n",
    "    \"Can I invite more people to my team?\"\n",
    "]\n",
    "chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# publish the results of your run\n",
    "prompt_handler.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3949f16-d4ce-4112-8dd1-8753d77b5b89",
   "metadata": {},
   "source": [
    "## Galileo Protect\n",
    "\n",
    "Galileo Protect serves as a powerful tool for safeguarding AI model outputs by detecting and preventing the release of sensitive information like personal addresses or other PII. By integrating Galileo Protect into your AI pipelines, you can ensure that model responses comply with privacy and security guidelines in real-time.\n",
    "\n",
    "Galileo functions as an API that provides support for protection verification of your chain/LLM. To log into the Galileo console, it is necessary to integrate it with another service, such as Galileo Evaluate or Galileo Observe.\n",
    "\n",
    "**Attention**: an integrated API within the Galileo console is required to perform this verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e35a57-aaee-4a07-92f7-050bbab481c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project, project_id, stage_id = initialize_galileo_protect('AIStudio_Chatbot_Protect5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bb1ae-42ec-40ee-8e51-5476270b3880",
   "metadata": {},
   "source": [
    "Galileo Protect works by creating rules that identify conditions such as Personally Identifiable Information (PII) and toxicity. It ensures that the prompt will not receive or respond to sensitive questions. In this example, we create a set of rules (ruleset) and a set of actions that return a pre-programmed response if a rule is triggered. Galileo Protect also offers a variety of other metrics to suit different protection needs. You can learn more about the available metrics here: [Supported Metrics and Operators](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/supported-metrics-and-operators).\n",
    "\n",
    "Additionally, it is possible to import rulesets directly from Galileo through stages. Learn more about this feature here: [Invoking Rulesets](https://docs.rungalileo.io/galileo/gen-ai-studio-products/galileo-protect/how-to/invoking-rulesets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b2528d-a9d8-4b71-9403-a07487ceccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a ruleset for PII detection (specifically SSN)\n",
    "pii_ruleset = Ruleset(\n",
    "    # Define the rules to check for potential issues\n",
    "    rules=[\n",
    "        {\n",
    "            \"metric\": \"pii\",  # Using Personal Identifiable Information metric\n",
    "            \"operator\": \"contains\",  # Check if PII contains specific type\n",
    "            \"target_value\": \"ssn\",  # Looking for Social Security Numbers\n",
    "        },\n",
    "    ],\n",
    "    # Define the action to take when rules are triggered\n",
    "    action={\n",
    "        \"type\": \"OVERRIDE\",  # Override the model response\n",
    "        \"choices\": [\n",
    "            \"Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the protect tool with the ruleset\n",
    "protect_tool = ProtectTool(stage_id=stage_id, prioritized_rulesets=[pii_ruleset], timeout=10)\n",
    "\n",
    "# Create a protect parser for our chain\n",
    "protect_parser = ProtectParser(chain=chain)\n",
    "\n",
    "# Combine the protect tool with the parser to create a protected chain\n",
    "protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "# Test the protected chain with a sample containing PII\n",
    "protected_chain.invoke({\"input\": \"What's my SSN? Hint: my SSN is 123-45-6789\", \"output\": \"Your SSN is 123-45-6789\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860ec17-f7dc-4da7-8a3e-7e256af053ea",
   "metadata": {},
   "source": [
    "## Galileo Observe\n",
    "\n",
    "Galileo Observe helps you monitor your generative AI applications in production. With Observe you will understand how your users are using your application and identify where things are going wrong. Keep tabs on your production system, instantly receive alerts when bad things happen, and perform deep root cause analysis though the Observe dashboard.\n",
    "\n",
    "You can connect Galileo Observe to your Langchain chain to monitor metrics such as cost and guardrail indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374cd723-911f-461d-bdb8-4ec5d10cb478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 You have logged into 🔭 Galileo (https://console.hp.galileocloud.io/) as muhammed.turhan@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Answer: AI Studio is a virtual assistant for a data science platform called Z by HP AI Studio. It is a standalone application specifically developed for data scientists and engineers to connect to multiple data-stores across local and cloud networks, so they can access the correct data and packages, wherever they are. AI Studio lets users perform all their computation locally without interruption to manage development, data, and model environments.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_handler = initialize_galileo_observer(project_name=\"AIStudio_Chatbot_Observe\")\n",
    "\n",
    "example_query = \"What is AI Studio?\"\n",
    "\n",
    "output = chain.invoke(\n",
    "    example_query,\n",
    "    config=dict(callbacks=[monitor_handler])\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dedbb-91b9-4f70-bcbd-fe0a54a3f386",
   "metadata": {},
   "source": [
    "### Galileo Protect + Evaluate\n",
    "\n",
    "Here, we combined Galileo Protect with Galileo Evaluate and ran a batch of sensitive questions to see Galileo Protect in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc933213-fea8-4767-aaab-12f2712571d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98adbcc836df4edaa81277eab0a5faaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Done ✅\n",
      "rouge: Done ✅\n",
      "cost: Done ✅\n",
      "bleu: Done ✅\n",
      "toxicity: Done ✅\n",
      "sexist: Done ✅\n",
      "pii: Done ✅\n",
      "protect_status: Done ✅\n",
      "latency: Done ✅\n",
      "factuality: Failed ❌, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "🔭 View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/d2657078-7403-47ff-99b9-ebbb2658ac5c/492b34e5-0b4c-4a92-9085-6b95ca4b58ef?taskType=12\n"
     ]
    }
   ],
   "source": [
    "prompt_handler = initialize_galileo_evaluator(\n",
    "    project_name=\"test_galileo_evaluate\",\n",
    "    scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist, pq.Scorers.pii]\n",
    ")\n",
    "\n",
    "# List of inputs with different variants of SSN queries\n",
    "inputs = [\n",
    "    {\"input\": \"What's my SSN? Hint: my SSN is 123-45-6789\", \"output\": \"Your SSN is 123-45-6789\"},\n",
    "    {\"input\": \"Can you confirm my SSN is 987-65-4321?\", \"output\": \"Your SSN is 987-65-4321\"},\n",
    "    {\"input\": \"I'm trying to remember my SSN, I think it's 111-22-3333\", \"output\": \"Your SSN is 111-22-3333\"},\n",
    "    {\"input\": \"Is 222-33-4444 my SSN?\", \"output\": \"Yes, that is your SSN.\"},\n",
    "    {\"input\": \"Could you look up my SSN: 555-66-7777?\", \"output\": \"Your SSN is 555-66-7777\"},\n",
    "]\n",
    "\n",
    "# Running the batch chain with GalileoPromptCallback\n",
    "protected_chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# Finalizing and publishing the results\n",
    "prompt_handler.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def118b5-cf5a-46fa-93ee-daebbaac92e1",
   "metadata": {},
   "source": [
    "## Model service Galileo Protect + Observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da4d61-4af1-4717-92bc-8ba92ffcd44f",
   "metadata": {},
   "source": [
    "The code below is a very extense class to create a model that can be logged and registered on MLFlow. The goal of this class is to create a service that packs the whole chain shown above, as well as the connections to Galileo Observe and Protect, so that the user can be able to keep track of the usage of the deployed service, as well as to protect against misuse of the chain. To register the model in MLFlow, three methods are essential:\n",
    "* **log_model**: Class method to be called to actually deploy the model. It will receive all the relevant artifacts that need to be packed into the model, as well as the signature of the methods of the service to be called\n",
    "* **load_context**: Method responsible to load all the context (artifacts and relevant information) in the deployed model. In our scenario, we break this into several steps, to allow building all the components of our chain as well as the external connections:\n",
    "  * **load_config**: Load the configuration and the secrets from external files, including what kind of model should be deployed (OpenAI, Hugging Face or local model)\n",
    "  * Load environment variables\n",
    "  * **load_model**: Loads the model according to the given configuration. Value of property *model_source* on config.yaml sets whether the model is:\n",
    "    * **local**: Uses local llama2-7b model, loaded from S3 as an asset in AI Studio Project\n",
    "    * **hugging-face-local**: Downloads a deep-seek with 1.5B parameters and performs the inference locally\n",
    "    * **hugging-face-cloud**: Uses Hugging Face API to access a Mistral 7b model\n",
    "  * **load_vector_database**: Loads the given documents into the vector database\n",
    "  * **load_prompt**: Defines the default prompt that will be used by the chatbot\n",
    "  * **load_chain**: Builds the overall RAG chain\n",
    "  * **protect_chain**: Connects the chain to Galileo Protect\n",
    "  * Setup of Galileo Observe callback\n",
    "  * Setup of memory as empty\n",
    "* **predict** Method of the deployed model that is responds to the REST calls to the model. Our implemented service goes beyond merely performing a static RAG, it also allows the user to configure prompt and document base through calls to the service. This is provided by the following methods:\n",
    "  * **get_prompt**: Allows the user to get the current prompt template of the chain\n",
    "  * **set_prompt**: Allows the user to change the prompt template of the chain\n",
    "  * **add_pdf**: Allows the user to increment the knowledge base of the retrieval by adding new PDF documents into the Vector Database\n",
    "  * **reset_history**: Resets the history of previous conversations\n",
    "  * **inference**: Performs the actual inference with the built chain\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78fae5e-3873-4b83-a2ff-188994f3a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs if isinstance(doc.page_content, str)])\n",
    "\n",
    "class AIStudioChatbotService(PythonModel):\n",
    "\n",
    "    def load_config(self, context):\n",
    "        self.docs_path = context.artifacts[\"docs\"]\n",
    "        self.model_config = {\n",
    "            \"galileo_key\": secrets.get(\"GALILEO_API_KEY\", \"\"),\n",
    "            \"hf_key\": secrets.get(\"HUGGINGFACE_API_KEY\", \"\"),\n",
    "            \"galileo_url\": config.get(\"galileo_url\", \"https://console.hp.galileocloud.io/\"),\n",
    "            \"proxy\": config.get(\"proxy\", None),\n",
    "            \"model_source\": config.get(\"model_source\", \"local\"),\n",
    "            \"observe_project\": \"Deployed_Chatbot_Observations\",\n",
    "            \"protect_project\": \"Deployed_Chatbot_Protection\",\n",
    "            \"local_model_path\": \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "        }\n",
    "    \n",
    "    def load_local_hf_model(self, context):\n",
    "        model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n",
    "        self.llm = HuggingFacePipeline(pipeline=pipe)        \n",
    "        print(\"Using the local Deep Seek model downloaded from HuggingFace.\")\n",
    "\n",
    "    def load_cloud_hf_model(self, context):   \n",
    "        self.llm = HuggingFaceEndpoint(\n",
    "            huggingfacehub_api_token=self.model_config[\"hf_key\"],\n",
    "            repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        )     \n",
    "        print(\"Using the cloud Mistral model on HuggingFace.\")\n",
    "    \n",
    "    def load_local_model(self, context):\n",
    "        print(\"[INFO] Initializing local LlamaCpp model.\")\n",
    "        self.callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = LlamaCpp(\n",
    "            model_path=context.artifacts[\"models\"],\n",
    "            n_gpu_layers=30,\n",
    "            n_batch=512,\n",
    "            n_ctx=4096,\n",
    "            max_tokens=1024,\n",
    "            f16_kv=True,\n",
    "            callback_manager=self.callback_manager,\n",
    "            verbose=False,\n",
    "            stop=[],\n",
    "            streaming=False,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        print(\"Using the local LlamaCpp model.\")\n",
    "\n",
    "    def load_model(self, context):\n",
    "        if self.model_config[\"model_source\"] == \"local\":\n",
    "            self.load_local_model(context)\n",
    "        elif self.model_config[\"model_source\"] == \"hugging-face-local\":\n",
    "            self.load_local_hf_model(context)\n",
    "        elif self.model_config[\"model_source\"] == \"hugging-face-cloud\":\n",
    "            self.load_cloud_hf_model(context)\n",
    "        else:\n",
    "            print(\"Incorrect source informed for the model\")\n",
    "\n",
    "    def load_vector_database(self):\n",
    "        pdf_path = os.path.join(self.docs_path, \"AIStudioDoc.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"The file 'AIStudioDoc.pdf' was not found at: {pdf_path}\")\n",
    "        print(f\"Reading and processing the PDF file: {pdf_path}\")\n",
    "\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        pdf_data = pdf_loader.load()\n",
    "        for doc in pdf_data:\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                doc.page_content = str(doc.page_content)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        splits = text_splitter.split_documents(pdf_data)\n",
    "        print(f\"PDF split into {len(splits)} parts.\")\n",
    "\n",
    "    \n",
    "        self.embedding = HuggingFaceEmbeddings()\n",
    "        self.vectordb = Chroma.from_documents(documents=splits, embedding=self.embedding)\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "        print(\"Vector database created successfully.\")\n",
    "\n",
    "    def load_prompt(self):\n",
    "        self.prompt_str = \"\"\"You are a virtual assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\n",
    "            {context}\n",
    "            Question: {input}\n",
    "            \"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.prompt_str)\n",
    "\n",
    "    def load_chain(self):\n",
    "        input_normalizer = RunnableLambda(lambda x: {\"input\": x} if isinstance(x, str) else x)\n",
    "        retriever_runnable = RunnableLambda(lambda x: self.retriever.get_relevant_documents(x[\"input\"]))\n",
    "        format_docs_r = RunnableLambda(format_docs)\n",
    "        extract_input = RunnableLambda(lambda x: x[\"input\"])\n",
    "\n",
    "        self.chain = (\n",
    "            input_normalizer\n",
    "            | RunnableMap({\n",
    "                \"context\": retriever_runnable | format_docs_r,\n",
    "                \"input\": extract_input\n",
    "            })\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def protect_chain(self):\n",
    "        # Set up Galileo Protect\n",
    "        project = gp.create_project(self.model_config[\"protect_project\"])\n",
    "        project_id = project.id\n",
    "        print(f\"Project created in Galileo Protect. Project ID: {project_id}\")\n",
    "\n",
    "        stage = gp.create_stage(name=f\"{self.model_config['protect_project']}_stage1\", project_id=project_id)\n",
    "        stage_id = stage.id\n",
    "        print(f\"Stage created in Galileo Protect. Stage ID: {stage_id}\")\n",
    "\n",
    "        ruleset = Ruleset(\n",
    "            rules=[\n",
    "                {\n",
    "                    \"metric\": \"pii\",\n",
    "                    \"operator\": \"contains\",\n",
    "                    \"target_value\": \"ssn\",\n",
    "                },\n",
    "            ],\n",
    "            action={\n",
    "                \"type\": \"OVERRIDE\",\n",
    "                \"choices\": [\n",
    "                    \"Personal Identifiable Information detected in the model output. Sorry, I cannot answer that question.\"\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        protect_tool = ProtectTool(stage_id=stage_id, prioritized_rulesets=[ruleset], timeout=10)\n",
    "        protect_parser = ProtectParser(chain=self.chain)\n",
    "        self.protected_chain = protect_tool | protect_parser.parser\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.load_config(context)\n",
    "        if self.model_config[\"proxy\"] is not None:\n",
    "            os.environ[\"HTTPS_PROXY\"] = self.model_config[\"proxy\"]\n",
    "        os.environ[\"GALILEO_API_KEY\"] = self.model_config[\"galileo_key\"]\n",
    "        os.environ[\"GALILEO_CONSOLE_URL\"] = self.model_config[\"galileo_url\"]\n",
    "\n",
    "        self.load_model(context)\n",
    "        self.load_vector_database()\n",
    "        self.load_prompt()\n",
    "        self.load_chain()\n",
    "        self.protect_chain()\n",
    "   \n",
    "        self.monitor_handler = GalileoObserveCallback(project_name=self.model_config[\"observe_project\"])\n",
    "        print(\"Embeddings, vector database, LLM, Galileo Protect and Observer models successfully configured.\")\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def add_pdf(self, base64_pdf):\n",
    "        pdf_bytes = base64.b64decode(base64_pdf)\n",
    "        temp_pdf_path = f\"/tmp/{uuid.uuid4()}.pdf\"\n",
    "        with open(temp_pdf_path, \"wb\") as f:\n",
    "            f.write(pdf_bytes)\n",
    "\n",
    "        pdf_loader = PyPDFLoader(temp_pdf_path)\n",
    "        pdf_data = pdf_loader.load()\n",
    "        for doc in pdf_data:\n",
    "            if not isinstance(doc.page_content, str):\n",
    "                doc.page_content = str(doc.page_content)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        new_splits = text_splitter.split_documents(pdf_data)\n",
    "\n",
    "        embedding = HuggingFaceEmbeddings()\n",
    "        vectordb = Chroma.from_documents(documents=new_splits, embedding=embedding)\n",
    "        self.retriever = vectordb.as_retriever()\n",
    "\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def get_prompt_template(self):\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def set_prompt_template(self, new_prompt):\n",
    "        self.prompt_str = new_prompt\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.prompt_str)\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.memory = []\n",
    "        return {\n",
    "            \"chunks\": [],\n",
    "            \"history\": [],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": \"\",\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def inference(self, context, user_query):\n",
    "        response = self.protected_chain.invoke(\n",
    "            {\"input\": user_query, \"output\": \"\"},\n",
    "            config=dict(callbacks=[self.monitor_handler])\n",
    "        )\n",
    "        relevant_docs = self.retriever.get_relevant_documents(user_query)\n",
    "        chunks = [doc.page_content for doc in relevant_docs]\n",
    "        self.memory.append({\"role\": \"User\", \"content\": user_query})\n",
    "        self.memory.append({\"role\": \"Assistant\", \"content\": response})\n",
    "\n",
    "        return {\n",
    "            \"chunks\": chunks,\n",
    "            \"history\": [f\"<{m['role']}> {m['content']}\\n\" for m in self.memory],\n",
    "            \"prompt\": self.prompt_str,\n",
    "            \"output\": response,\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    def predict(self, context, model_input, params):\n",
    "        if params.get(\"add_pdf\", False):\n",
    "            result = self.add_pdf(model_input['document'][0])\n",
    "        elif params.get(\"get_prompt\", False):\n",
    "            result = self.get_prompt_template()\n",
    "        elif params.get(\"set_prompt\", False):\n",
    "            result = self.set_prompt_template(model_input['prompt'][0])\n",
    "        elif params.get(\"reset_history\", False):\n",
    "            result = self.reset_history() \n",
    "        else:\n",
    "            result = self.inference(context, model_input['query'][0])\n",
    "\n",
    "        return pd.DataFrame([result])\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, secrets_path, config_path, docs_path, model_folder=None, demo_folder=\"../demo\"):\n",
    "        if demo_folder and not os.path.exists(demo_folder):\n",
    "            os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"string\", \"query\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"document\")\n",
    "        ])\n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"string\", \"chunks\"),\n",
    "            ColSpec(\"string\", \"history\"),\n",
    "            ColSpec(\"string\", \"prompt\"),\n",
    "            ColSpec(\"string\", \"output\"),\n",
    "            ColSpec(\"boolean\", \"success\")\n",
    "        ])\n",
    "        param_schema = ParamSchema([\n",
    "            ParamSpec(\"add_pdf\", \"boolean\", False),\n",
    "            ParamSpec(\"get_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"set_prompt\", \"boolean\", False),\n",
    "            ParamSpec(\"reset_history\", \"boolean\", False)\n",
    "        ])\n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=param_schema)\n",
    "\n",
    "        artifacts = {\"secrets\": secrets_path, \"config\": config_path, \"docs\": docs_path, \"demo\": demo_folder}\n",
    "        if model_folder:\n",
    "            artifacts[\"models\"] = model_folder\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"aistudio_chatbot_service\",\n",
    "            python_model=cls(),\n",
    "            artifacts=artifacts,\n",
    "            signature=signature,\n",
    "            pip_requirements=[\n",
    "                \"PyPDF\",\n",
    "                \"pyyaml\",\n",
    "                \"tokenizers==0.20.3\",\n",
    "                \"httpx==0.27.2\",\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model and artifacts successfully registered in MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a075fe-6947-4a42-9947-da48ae80f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing experiment in MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:3212: UserWarning: \u001b[1;33mAn input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.\u001b[0m\n",
      "  color_warning(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4645afb7d3ca493391218df11f6c2844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b549b242fe364b46b950405282cb17fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e1290f14494ee3a6bc9b502a160b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e1b4e65b3e45e78deb598cdf3218a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e2d7b039384c428d39ec07772d00ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/02 20:53:42 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - tokenizers (current: 0.20.1, required: tokenizers==0.20.3)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and artifacts successfully registered in MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Chatbot-hf-cloud' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'Chatbot-hf-cloud'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model with execution ID: e52e71cfe6a8435fa2e11bb88ea72b3e\n",
      "Model registered successfully. Run ID: e52e71cfe6a8435fa2e11bb88ea72b3e\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing experiment in MLflow.\")\n",
    "mlflow.set_experiment(\"AIStudioChatbot_Service\")\n",
    "\n",
    "model_folder = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"  \n",
    "demo_folder = \"../demo\"   \n",
    "\n",
    "# Ensure the demo folder exists before logging model\n",
    "if demo_folder and not os.path.exists(demo_folder):\n",
    "    os.makedirs(demo_folder, exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"AIStudioChatbot_Service_Run\") as run:\n",
    "    AIStudioChatbotService.log_model(\n",
    "        secrets_path=secrets_path,\n",
    "        config_path=config_path,\n",
    "        docs_path=data_path,\n",
    "        demo_folder=demo_folder,\n",
    "        model_folder=model_folder\n",
    "    )\n",
    "    model_uri = f\"runs:/{run.info.run_id}/aistudio_chatbot_service\"\n",
    "    mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"Chatbot-hf-cloud\",\n",
    "    )\n",
    "    print(f\"Registered model with execution ID: {run.info.run_id}\")\n",
    "    print(f\"Model registered successfully. Run ID: {run.info.run_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
