{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# RAG with Galileo, LangChain and GPT\n",
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "This step install the necessary libraries for connecting with Galileo and the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8fcd-58ec-4eb9-a9bc-8c93d632f284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community\n",
    "!pip install langchain\n",
    "!pip install langchain_openai\n",
    "!pip install promptquality #Galileo\n",
    "!pip install chromadb\n",
    "!pip install sentence-transformers\n",
    "!pip install openai\n",
    "!pip install PyPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data form pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b054f8-6bca-4f82-9f51-04cf05024896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"docs/AIStudioDoc.pdf\"\n",
    ")\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "pdf_data = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 2: Creation of Chunks\n",
    "Here, we split the loaded documents into chunks, so we have smaller and more specific texts to add do our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01272fc8-9902-4153-8a9a-212b4d7820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "splits = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999ff1f-1f6a-452c-848e-e3c67adf7766",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3: Retrieval\n",
    "\n",
    "We transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d128b8b-db9f-4f3b-a592-dc4d45272703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/zcddtdt53nz010hp1w2b86w40000gn/T/ipykernel_35305/1793829886.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "/var/folders/4z/zcddtdt53nz010hp1w2b86w40000gn/T/ipykernel_35305/1793829886.py:4: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "/opt/anaconda3/envs/galileo/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/galileo/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58329084-640e-4b25-be70-ac1ec90467e9",
   "metadata": {},
   "source": [
    "## Step 4: Model\n",
    "\n",
    "In this example, we will use OpenAI API to connect to GPT-3.5 model. A broader range of models could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d289f610-c5a2-49ad-860d-4ec2068fcaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-instruct\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "### Code to connect to Hugging Face models\n",
    "#import yaml\n",
    "#with open('config.yaml') as file:\n",
    "    #config = yaml.safe_load(file)\n",
    "#huggingfacehub_api_token = config[\"hf_key\"]\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#llm = HuggingFaceEndpoint(\n",
    "   #huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "   #repo_id=repo_id,\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f5d66-d99d-4851-a48e-248b2e81e2c9",
   "metadata": {},
   "source": [
    "## Step 5: Chain\n",
    "In this part, we define a pipeline that receives a question and context, formats the context documents, and uses a Hugging Face (Mistral) chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116d88a8-6d84-4d46-964e-c891965776dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "template = \"\"\"You are an virtual Assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d2a5d-35e3-46ed-a53e-ef49fc1c11a4",
   "metadata": {},
   "source": [
    "## Step 6: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e666a9-311c-42d4-bc34-260333184ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ You have logged into ðŸ”­ Galileo (https://console.hp.galileocloud.io/) as minh@rungalileo.io.\n"
     ]
    }
   ],
   "source": [
    "import promptquality as pq\n",
    "import os\n",
    "\n",
    "os.environ['GALILEO_API_KEY'] = os.environ['GALILEO_API_KEY_HP']\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "config = pq.login(galileo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713cc62",
   "metadata": {},
   "source": [
    "Through callbacks, we choose the metrics we want to monitor via the Galileo console. We pass a list of queries to run our created chain and log in to Galileo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349bbb9c-5181-4ffd-ba4f-6d3833c1670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.28it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Done âœ…\n",
      "latency: Done âœ…\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/da316d6a-d39d-4537-8aaa-b9ba9efd93a5/baa628f9-4543-471c-bc5f-28387784f321?taskType=12\n"
     ]
    }
   ],
   "source": [
    "# Create callback handler\n",
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    project_name=\"AIStudio_RAG_Evaluate\",\n",
    "    #scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist, pq.Scorers.chunk_attribution_utilization_luna,\n",
    "    scorers=[pq.Scorers.context_adherence_plus, pq.Scorers.correctness, pq.Scorers.chunk_attribution_utilization_luna, pq.Scorers.toxicity, pq.Scorers.sexist, pq.Scorers.pii,\n",
    " ]\n",
    ")\n",
    "\n",
    "# Run your chain experiments across multiple inputs with the galileo callback\n",
    "inputs = [\n",
    "    \"What is AI Studio\",\n",
    "    \"How to create projects in AI Studio?\",\n",
    "    \"How to monitor experiments?\",\n",
    "    \"What are the different workspaces available?\",\n",
    "    \"What, exactly, is a workspace?\",\n",
    "    \"How to share my experiments with my team?\",\n",
    "    \"Can I access my Git repository?\",\n",
    "    \"Do I have access to files on my local computer?\",\n",
    "    \"How do I access files on the cloud?\",\n",
    "    \"Can I invite more people to my team?\",\n",
    "    \"How do I install dependencies in AI Studio?\",\n",
    "    \"How to set up version control in AI Studio?\",\n",
    "    \"What kind of data formats can be imported?\",\n",
    "    \"Can I schedule experiments to run automatically?\",\n",
    "    \"How do I manage different environments?\",\n",
    "    \"What are the options for running experiments?\",\n",
    "    \"Can I collaborate with external users on projects?\",\n",
    "    \"How can I track changes in the project?\",\n",
    "    \"Is there a way to automate reporting?\",\n",
    "    \"What integrations are supported?\",\n",
    "    \"How do I update existing projects?\",\n",
    "    \"What is the best way to organize my projects?\",\n",
    "    \"How do I manage roles and permissions?\",\n",
    "    \"Can I customize my workspace layout?\",\n",
    "    \"What security features are in place?\",\n",
    "    \"Is there a limit to the number of experiments I can run?\",\n",
    "    \"How do I visualize experiment results?\",\n",
    "    \"Can I deploy models directly from AI Studio?\",\n",
    "    \"How do I back up my data?\",\n",
    "    \"Can I use third-party libraries?\",\n",
    "    \"Is there a way to optimize my experiments for performance?\",\n",
    "    \"What are the best practices for data handling in AI Studio?\",\n",
    "    \"How do I create pipelines for workflows?\",\n",
    "    \"Can I set up notifications for completed tasks?\",\n",
    "    \"What are the hardware requirements for running AI Studio?\",\n",
    "    \"How do I manage resource usage?\",\n",
    "    \"Can I import projects from other platforms?\",\n",
    "    \"Is there support for real-time collaboration?\",\n",
    "    \"How do I perform hyperparameter tuning?\",\n",
    "    \"What are the limitations of the free tier?\",\n",
    "    \"How can I extend AI Studio with custom tools?\"\n",
    "]\n",
    "\n",
    "chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# publish the results of your run\n",
    "prompt_handler.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galileo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
