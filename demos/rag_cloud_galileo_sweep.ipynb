{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9ff4be-1fee-47eb-8d09-611c29a7a83f",
   "metadata": {},
   "source": [
    "# RAG with Galileo, LangChain and GPT\n",
    "Retrieval-Augmented Generation (RAG) is an architectural approach that can enhance the effectiveness of large language model (LLM) applications using customized data. In this example, we use LangChain, an orchestrator for language pipelines, to build an assistant capable of loading information from a web page and use it for answering user questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d10f-d930-4be7-a9e8-15606d466460",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "This step install the necessary libraries for connecting with Galileo and the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8fcd-58ec-4eb9-a9bc-8c93d632f284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community\n",
    "!pip install langchain\n",
    "!pip install langchain_openai\n",
    "!pip install promptquality #Galileo\n",
    "!pip install chromadb\n",
    "!pip install sentence-transformers\n",
    "!pip install openai\n",
    "!pip install PyPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92659e-23ea-4965-bb27-58cd1dd0f3ec",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "In this step, we will use the Langchain framework to  extract the content from a local PDF file with the product documentation. Also, we have commented some example on how to use Web Loaders to load data form pages on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b054f8-6bca-4f82-9f51-04cf05024896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "    \"docs/AIStudioDoc.pdf\"\n",
    ")\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "pdf_data = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a731a4",
   "metadata": {},
   "source": [
    "## Step 2: Connect to Galileo\n",
    "Through the Galileo library called Prompt Quality, we connect our API generated in the Galileo console to log in. To get your ApiKey, use this link: https://console.hp.galileocloud.io/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39faa467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/galileo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ You have logged into ðŸ”­ Galileo (https://console.hp.galileocloud.io/) as minh@rungalileo.io.\n"
     ]
    }
   ],
   "source": [
    "import promptquality as pq\n",
    "import os\n",
    "\n",
    "os.environ['GALILEO_API_KEY'] = os.environ['GALILEO_API_KEY_HP']\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "config = pq.login(galileo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4725de",
   "metadata": {},
   "source": [
    "## Step 3: Model Selection\n",
    "\n",
    "In this example, we will define our LLM as GPT-3.5 model hosted by OpenAI. A broader range of models could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1a588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "### Code to connect to Hugging Face models\n",
    "#import yaml\n",
    "#with open('config.yaml') as file:\n",
    "    #config = yaml.safe_load(file)\n",
    "#huggingfacehub_api_token = config[\"hf_key\"]\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#llm = HuggingFaceEndpoint(\n",
    "   #huggingfacehub_api_token=huggingfacehub_api_token,\n",
    "   #repo_id=repo_id,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ed05-192e-4f59-856c-cf9bff160fc8",
   "metadata": {},
   "source": [
    "## Step 4: Embed, Chunk, Construct Chain\n",
    "First, we split the loaded documents into chunks, so we have smaller and more specific texts to add do our vector database.\n",
    "\n",
    "Then, we transform the texts into embeddings and store them in a vector database. This allows us to perform similarity search, and proper retrieval of documents.\n",
    "\n",
    "Next, we define a pipeline that receives a question and context, formats the context documents, and uses a chat model to answer the question based on the provided context. The output is then formatted as a string for easy reading.\n",
    "\n",
    "Finally, through callbacks, we choose the metrics we want to monitor via the Galileo console. We pass a list of queries to run our created chain and log in to Galileo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01272fc8-9902-4153-8a9a-212b4d7820d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/zcddtdt53nz010hp1w2b86w40000gn/T/ipykernel_43257/152776688.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "/var/folders/4z/zcddtdt53nz010hp1w2b86w40000gn/T/ipykernel_43257/152776688.py:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "/opt/anaconda3/envs/galileo/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.79it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Done âœ…\n",
      "latency: Done âœ…\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/9c846e65-abcf-40e6-a484-e4bcad859252?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.26it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/cc94e416-7581-4363-8cfd-25d428992730?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.63it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/cca1973e-cf5c-4b8f-af04-9956ed07cade?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.11it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/fd57b31d-3941-48fe-aaac-bdff9d35c99b?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.31it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/ffb72885-292f-4fce-b2a0-76933ab3843c?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.94it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/c055e3fe-648a-4e9c-8384-91b63b52e3fe?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.30it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/1ab729c6-4c74-4180-bf2a-c489144b5c0a?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.80it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/4af64296-a3d9-4cbf-a484-f5d2407a6de4?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.04s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/0de92901-10c0-49dc-aacd-a9e5d548258b?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.17it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/4f0bc36a-3897-45d5-b979-1749e43f056c?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.14it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/0761c12b-6e8f-4f5f-b35e-8b75e5f2ff4b?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.70it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/d67446e8-86a6-442a-8d73-beea5730ee90?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.18it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/1c366c0e-085f-46f0-8c92-bbef5f15464d?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.44it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/36c0df1b-cd1f-46ca-b7e6-833e28386c99?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.75it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/6a0d4c1f-6627-4459-abad-17726a059947?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.56it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/ca61001e-7044-4c01-b394-40b504a6fb27?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.66it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/931d6251-4b69-4bc8-a925-1e26f3f84d2f?taskType=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing complete!: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.68it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "rag_nli: Computing ðŸš§\n",
      "cost: Computing ðŸš§\n",
      "toxicity: Computing ðŸš§\n",
      "sexist: Computing ðŸš§\n",
      "pii: Computing ðŸš§\n",
      "protect_status: Computing ðŸš§\n",
      "latency: Computing ðŸš§\n",
      "groundedness: Computing ðŸš§\n",
      "factuality: Computing ðŸš§\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/3562c0af-f907-48a5-bf96-cf0e351c90fa/ed5f0d25-a15b-4508-b3ab-5953322ac44c?taskType=12\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "CHUNK_SIZES = [i for i in range(500, 800, 100)]\n",
    "CHUNK_OVERLAPS = [i for i in range(0, 300, 100)]\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "SYS_INSTRUCTIONS = [\n",
    "    \"\"\"You are an virtual Assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\"\"\",\n",
    "    \"\"\"You are a specialized virtual assistant for AI Studio, a comprehensive Data Science platform. Your role is to assist users with tasks related to data analysis, machine learning, and model building. Always base your answers on the provided context and ensure responses are directly relevant to the platform's functionality, tools, and workflows, prioritizing precision and clarity in your explanations.\n",
    "    When answering user queries, ensure that responses are concise yet informative. If the context provided is insufficient or the information is unavailable, acknowledge this clearly, and offer alternative suggestions or next steps the user can take within the AI Studio platform. Avoid making unsupported assumptions and focus on providing actionable insights related to the user's Data Science tasks.\n",
    "    In situations where users inquire about advanced data science topics (such as hyperparameter tuning, feature engineering, or algorithm selection), provide detailed, clear, and example-driven explanations. Make sure your guidance is digestible for users at various experience levels. Avoid unnecessary jargon, and when advanced technical terms are necessary, include brief definitions or explanations to ensure clarity.\n",
    "    If the userâ€™s query seems ambiguous or lacks enough information, prompt them for clarification to refine the scope of your response. This will help you provide more accurate and relevant information within the context of AI Studioâ€™s features, such as model evaluation, data preprocessing, or code execution. Always aim for accuracy and ensure your response aligns with the platformâ€™s capabilities and tools.\"\"\",\n",
    "\n",
    "]\n",
    "\n",
    "# Run your chain experiments across multiple inputs with the galileo callback\n",
    "inputs = [\n",
    "    \"What is AI Studio\",\n",
    "    \"How to create projects in AI Studio?\",\n",
    "    \"How to monitor experiments?\",\n",
    "    \"What are the different workspaces available?\",\n",
    "    \"What, exactly, is a workspace?\",\n",
    "    \"How to share my experiments with my team?\",\n",
    "    \"Can I access my Git repository?\",\n",
    "    \"Do I have access to files on my local computer?\",\n",
    "    \"How do I access files on the cloud?\",\n",
    "    \"Can I invite more people to my team?\",\n",
    "    \"How do I install dependencies in AI Studio?\",\n",
    "    \"How to set up version control in AI Studio?\",\n",
    "    \"What kind of data formats can be imported?\",\n",
    "    \"Can I schedule experiments to run automatically?\",\n",
    "    \"How do I manage different environments?\",\n",
    "    \"What are the options for running experiments?\",\n",
    "    \"Can I collaborate with external users on projects?\",\n",
    "    \"How can I track changes in the project?\",\n",
    "    \"Is there a way to automate reporting?\",\n",
    "    \"What integrations are supported?\",\n",
    "    \"How do I update existing projects?\",\n",
    "    \"What is the best way to organize my projects?\",\n",
    "    \"How do I manage roles and permissions?\",\n",
    "    \"Can I customize my workspace layout?\",\n",
    "    \"What security features are in place?\",\n",
    "    \"Is there a limit to the number of experiments I can run?\",\n",
    "    \"How do I visualize experiment results?\",\n",
    "    \"Can I deploy models directly from AI Studio?\",\n",
    "    \"How do I back up my data?\",\n",
    "    \"Can I use third-party libraries?\",\n",
    "    \"Is there a way to optimize my experiments for performance?\",\n",
    "    \"What are the best practices for data handling in AI Studio?\",\n",
    "    \"How do I create pipelines for workflows?\",\n",
    "    \"Can I set up notifications for completed tasks?\",\n",
    "    \"What are the hardware requirements for running AI Studio?\",\n",
    "    \"How do I manage resource usage?\",\n",
    "    \"Can I import projects from other platforms?\",\n",
    "    \"Is there support for real-time collaboration?\",\n",
    "    \"How do I perform hyperparameter tuning?\",\n",
    "    \"What are the limitations of the free tier?\",\n",
    "    \"How can I extend AI Studio with custom tools?\"\n",
    "]\n",
    "\n",
    "for CHUNK_SIZE in CHUNK_SIZES:\n",
    "    for CHUNK_OVERLAP in CHUNK_OVERLAPS:\n",
    "        for idx, SYS_INSTRUCTION in enumerate(SYS_INSTRUCTIONS):\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "            splits = text_splitter.split_documents(pdf_data)\n",
    "            vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "            retriever = vectordb.as_retriever()\n",
    "\n",
    "            template = \"\"\"[SYS_INSTRUCTION]\n",
    "\n",
    "                {context}\n",
    "\n",
    "                Question: {query}\n",
    "                \"\"\".replace(\"[SYS_INSTRUCTION]\", SYS_INSTRUCTION)\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "            chain = {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "            \n",
    "            # Metadata tagging for our Galileo Evaluate run\n",
    "            metadata_tag_vectordb = pq.RunTag(key=\"Vector Database\", value=vectordb.__class__.__name__, tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_retriever = pq.RunTag(key=\"Retriever\", value=retriever.get_name(), tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_embeddings = pq.RunTag(key=\"Embeddings\", value=embedding.model_name, tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_orchestration = pq.RunTag(key=\"Orchestration\", value=\"LangChain\", tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_model = pq.RunTag(key=\"Model\", value=llm.model_name, tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_CHUNK_SIZE = pq.RunTag(key=\"CHUNK_SIZE\", value=str(CHUNK_SIZE), tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_CHUNK_OVERLAP = pq.RunTag(key=\"CHUNK_OVERLAP\", value=str(CHUNK_OVERLAP), tag_type=pq.TagType.GENERIC)\n",
    "            metadata_tag_SYS_INSTRUCTION = pq.RunTag(key=\"SYS_INSTRUCTION_INDEX\", value=str(idx), tag_type=pq.TagType.GENERIC)\n",
    "            run_tags = [metadata_tag_vectordb, metadata_tag_retriever, metadata_tag_embeddings, metadata_tag_orchestration, metadata_tag_model, metadata_tag_CHUNK_SIZE, metadata_tag_CHUNK_OVERLAP, metadata_tag_SYS_INSTRUCTION]\n",
    "\n",
    "            # Create callback handler\n",
    "            prompt_handler = pq.GalileoPromptCallback(\n",
    "                project_name=\"AIStudio_RAG_Evaluate_35\",\n",
    "                scorers=[pq.Scorers.context_adherence_plus, pq.Scorers.correctness, pq.Scorers.chunk_attribution_utilization_luna, pq.Scorers.toxicity, pq.Scorers.sexist, pq.Scorers.pii,],\n",
    "                run_tags=run_tags\n",
    "            )\n",
    "\n",
    "            # run the chain\n",
    "            chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "            \n",
    "            # publish the results of your run\n",
    "            prompt_handler.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galileo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
